{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree:\n",
    ".\n",
    "├── data/\n",
    "│   └── Research Data Project/\n",
    "│       └── exit_velo_project_data.csv\n",
    "├── src/\n",
    "│   ├── data/\n",
    "│   │   ├── load_data.py\n",
    "│   │   └── ColumnSchema.py\n",
    "│   ├── features/\n",
    "│   │   ├── data_prep.py\n",
    "│   │   ├── eda.py\n",
    "│   │   ├── feature_engineering.py\n",
    "│   │   ├── feature_selection.py\n",
    "│   │   ├── preprocess.py\n",
    "│   │   └── __init__.py\n",
    "│   ├── models/\n",
    "│   │   ├── bayesian_alternatives.py\n",
    "│   │   ├── gbm.py\n",
    "│   │   ├── hierarchical.py\n",
    "│   │   ├── linear.py\n",
    "│   │   ├── mixed.py\n",
    "│   │   └── hierarchical_predict.py\n",
    "│   ├── utils/\n",
    "│   │   ├── bayesian_explainability.py\n",
    "│   │   ├── bayesian_metrics.py\n",
    "│   │   ├── gbm_utils.py\n",
    "│   │   ├── hierarchical_utils.py\n",
    "│   │   ├── jax_gpu_utils.py\n",
    "│   │   ├── jax_memory_monitor.py\n",
    "│   │   ├── posterior.py\n",
    "│   │   ├── validation.py\n",
    "│   │   └── __init__.py\n",
    "│   └── train.py\n",
    "└── requirements.txt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "src/data\n",
    "\n",
    "    load_data.py\n",
    "    • load_data: Reads the raw CSV of exit-velo observations.\n",
    "    • clean_raw: Drops rows missing the target (exit_velo) or key columns, reports remaining nulls.\n",
    "    • load_and_clean_data: Convenience wrapper that runs both in sequence.\n",
    "    Purpose: Centralize raw-data ingestion and initial cleanup so downstream code always sees a non-mutated, null-filtered DataFrame.\n",
    "\n",
    "    ColumnSchema.py\n",
    "    • Defines _ColumnSchema, with lists of ID, ordinal, nominal, numerical, and target columns.\n",
    "    • Exposes methods like .numerical(), .categorical(), .model_features() for type-safe feature lists.\n",
    "    Purpose: Avoid hard-coding column names everywhere—everything downstream (preprocessing, modeling) refers to this single source of truth.\n",
    "\n",
    "src/features\n",
    "\n",
    "    feature_engineering.py\n",
    "    • Pure functions that add derived columns:\n",
    "    – binned features (age_bin, la_bin, spray_bin)\n",
    "    – rolling, lagged statistics (player_ev_mean50, etc.)\n",
    "    – categorical flags (same_hand, hitter_type)\n",
    "    Purpose: Enrich raw data with predictive covariates while avoiding leakage.\n",
    "\n",
    "    eda.py\n",
    "    • Quick checks and diagnostics: null summaries, correlation tables, ANOVA for league-level effects, LOWESS age trends, Durbin–Watson tests, distribution plots, etc.\n",
    "    Purpose: Exploratory-data-analysis helpers to understand data shape, spot red flags (small samples, outliers), and guide model design (e.g. hierarchical effects).\n",
    "\n",
    "    data_prep.py\n",
    "    • Low-level utilities for domain-specific filtering & clipping:\n",
    "    – compute_clip_bounds\n",
    "    – clip_extreme_ev (trim 1st/99th-pct EVs)\n",
    "    – filter_bunts_and_popups, filter_low_event_batters, filter_physical_implausibles\n",
    "    Purpose: Apply baseball-specific rules to remove implausible or uninformative observations before modeling.\n",
    "\n",
    "    preprocess.py\n",
    "    • Builds full scikit-learn pipelines:\n",
    "    – filter_and_clip for domain cleaning\n",
    "    – fit_preprocessor/transform_preprocessor to impute, encode, scale, and preserve clipping bounds\n",
    "    – inverse_transform_preprocessor to map transformed arrays back to original features\n",
    "    – prepare_for_mixed_and_hierarchical adds category codes for mixed-effects and Bayesian models\n",
    "    Purpose: Turn your feature-engineered DataFrame into numeric arrays ready for any downstream estimator, with consistent handling of missingness and train/test separation.\n",
    "\n",
    "    feature_selection.py\n",
    "    • Baseline RandomForest (train_baseline_model)\n",
    "    • Permutation importance (compute_permutation_importance) and SHAP importance (compute_shap_importance)\n",
    "    • Helpers to threshold and intersect feature lists, plus I/O for saving/loading your final feature list\n",
    "    Purpose: Automate ranking and selection of the most predictive features before you commit to a final modeling dataset.\n",
    "\n",
    "src/models\n",
    "\n",
    "    linear.py\n",
    "    • OLS/Ridge regression wrappers with simple fit_ridge interface.\n",
    "    Purpose: Quick, interpretable baselines for exit-velo prediction.\n",
    "\n",
    "    gbm.py\n",
    "    • XGBoost regressor pipeline with GPU detection, Optuna-based tuning (tune_gbm), early stopping and save/load via gbm_utils.\n",
    "    Purpose: High-performing gradient-boosting baseline that scales to larger datasets.\n",
    "\n",
    "    mixed.py\n",
    "    • Frequentist mixed-effects model (statsmodels.MixedLM) capturing random intercepts by batter.\n",
    "    Purpose: Model batter-level variability explicitly, bridging simple regressions and full Bayesian hierarchies.\n",
    "\n",
    "    hierarchical.py\n",
    "    • Full Bayesian hierarchical model in PyMC (optionally accelerated via JAX).\n",
    "    • Priors on global mean, level effects, batter/season/pitcher random effects, plus memory-monitoring hooks.\n",
    "    Purpose: Capture multi-level structure (league, batter, season, pitcher) and quantify uncertainty via full posterior inference.\n",
    "\n",
    "    bayesian_alternatives.py\n",
    "    • Plug-and-play interfaces for other Bayesian engines: CmdStanPy, PyJAGS, NumPyro, TensorFlow-Probability, PyMC-ADVI.\n",
    "    Purpose: Compare different probabilistic back-ends under the same InferenceData API.\n",
    "\n",
    "    hierarchical_predict.py\n",
    "    • Post-hoc prediction utilities that merge saved global effects JSON and posterior summaries with a new roster to produce point forecasts and intervals for 2024.\n",
    "    Purpose: Turn your trained hierarchical model into actionable predictions on fresh data.\n",
    "\n",
    "    model_shap_reports.py\n",
    "    • Wrappers around ExplainerDashboard (Dash), Shapash, and Shapiq for interactive and static explainability reports.\n",
    "    Purpose: Provide end-to-end explainability for your chosen model.\n",
    "\n",
    "src/utils\n",
    "\n",
    "    gbm_utils.py: save/load combined GBM + preprocessor pipelines via joblib.\n",
    "\n",
    "    bayesian_explainability.py: Arviz forest plots, posterior-predictive checks, plus kernel SHAP on posterior means.\n",
    "\n",
    "    bayesian_metrics.py: Compute classical (RMSE, MAE, R²) and Bayesian model‐comparison metrics (LOO, WAIC).\n",
    "\n",
    "    hierarchical_utils.py: Save/load ArviZ InferenceData to NetCDF and preprocessor joblib.\n",
    "\n",
    "    posterior.py: Extract per-batter random‐effect summaries into a DataFrame, and write global effects out to JSON.\n",
    "\n",
    "    validation.py: K-fold CV for both sklearn estimators and PyMC models, plus prediction‐interval helpers (analytic and bootstrap).\n",
    "\n",
    "    jax_gpu_utils.py: GPU diagnostics for JAX backends (checks nvidia-smi, device list, LD_LIBRARY_PATH).\n",
    "\n",
    "    (plus memory-monitoring and JAX memory-fix modules)\n",
    "\n",
    "Purpose: Miscellaneous building blocks for model persistence, diagnostics, metrics, and workflow orchestration.\n",
    "Top-Level Scripts\n",
    "\n",
    "    train.py\n",
    "    Orchestrates a 70/30 split to fit and compare Ridge, GBM, mixed-LM, and quick Bayesian HMC, printing RMSEs side by side.\n",
    "    Purpose: Unified entry point to benchmark all four modeling paradigms on exit-velo data.\n",
    "\n",
    "Overall Pipeline Flow\n",
    "\n",
    "    Load & clean raw CSV → 2. Feature-engineer (new covariates) → 3. Preprocess (impute, encode, scale, clip) → 4. Select top features → 5. Fit model(s) → 6. Evaluate & compare metrics → 7. Explain results → 8. Predict on 2024 roster with saved summaries.\n",
    "\n",
    "Each module slots neatly into one of those stages, making your codebase both modular and extensible for future seasons or new model families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/data/load_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/data/load_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path='data/Research Data Project/Research Data Project/exit_velo_project_data.csv'):\n",
    "    \"\"\"\n",
    "    Load raw exit velocity data from the data directory.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Raw exit velocity data\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "# ──  utility ─────────────────────────────────────────────────────────\n",
    "def clean_raw(df: pd.DataFrame, \n",
    "              target: str = \"exit_velo\"\n",
    "              ,debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Central place to:\n",
    "      1. Drop rows with NaN in *target*.\n",
    "      2. Print a concise null‑summary afterwards (one line per column).\n",
    "    \n",
    "    Returns a *fresh copy* (never mutates in‑place).\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Dropping rows with NaN in {target}\")\n",
    "        print(df.isna().sum())\n",
    "        \n",
    "    drop_columns = [target, \"hit_type\", \"launch_angle\"]\n",
    "    out = df.dropna(subset=drop_columns).copy()   \n",
    "\n",
    "    if debug:\n",
    "        print(f\"after rows dropped with NaN in {target}\")\n",
    "        print(out.isna().sum())\n",
    "    # quick dashboard\n",
    "    nulls = out.isna().sum()\n",
    "    non_zero = nulls[nulls > 0]\n",
    "    if non_zero.empty:\n",
    "        print(f\"✅  After target‑filter, no other nulls (n={len(out):,}).\")\n",
    "    else:\n",
    "        print(\"⚠️  Nulls after target‑filter:\")\n",
    "        for col, cnt in non_zero.items():\n",
    "            pct = cnt / len(out)\n",
    "            print(f\"  • {col:<15} {cnt:>7,} ({pct:5.2%})\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_and_clean_data(path='data/Research Data Project/Research Data Project/exit_velo_project_data.csv'\n",
    "                        ,debug: bool = False):\n",
    "    df = load_data(path)\n",
    "    df = clean_raw(df, debug = True) \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = 'data/Research Data Project/Research Data Project/exit_velo_project_data.csv'\n",
    "\n",
    "    df = load_and_clean_data(path, debug = True)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/features/feature_engineering.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/features/feature_engineering.py\n",
    "\"\"\"Feature engineering utilities for the exit‑velo project.\n",
    "\n",
    "All helpers are pure functions that take a pandas DataFrame and return a *copy*\n",
    "with additional engineered columns so we avoid side effects.\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "# Helper functions\n",
    "###############################################################################\n",
    "def _classify_hitters_by_season(\n",
    "    df: pd.DataFrame,\n",
    "    batter_col: str = \"batter_id\",\n",
    "    season_col: str = \"season\",\n",
    "    outcome_col: str = \"outcome\",\n",
    "    power_pct: float = 0.8,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Season-by-season classification: top `power_pct` hitters by triple+HR rate\n",
    "    are 'POWER', all others 'CONTACT'. Batters with no hits default to CONTACT.\n",
    "    \"\"\"\n",
    "    # 1) restrict to actual base hits\n",
    "    hits = df[df[outcome_col].isin([\"SINGLE\",\"DOUBLE\",\"TRIPLE\",\"HOME RUN\"])].copy()\n",
    "\n",
    "    # 2) count per (season, batter)\n",
    "    counts = (\n",
    "        hits\n",
    "        .assign(\n",
    "            contact = lambda d: d[outcome_col].isin([\"SINGLE\",\"DOUBLE\"]).astype(int),\n",
    "            power   = lambda d: d[outcome_col].isin([\"TRIPLE\",\"HOME RUN\"]).astype(int),\n",
    "        )\n",
    "        .groupby([season_col, batter_col])\n",
    "        .agg(\n",
    "            total_hits   = (outcome_col, \"size\"),\n",
    "            contact_hits = (\"contact\",    \"sum\"),\n",
    "            power_hits   = (\"power\",      \"sum\"),\n",
    "        )\n",
    "    )\n",
    "    counts[\"power_rate\"]   = counts[\"power_hits\"]   / counts[\"total_hits\"]\n",
    "    counts[\"contact_rate\"] = counts[\"contact_hits\"] / counts[\"total_hits\"]\n",
    "\n",
    "    # 3) find the season‐level 80th percentile of power_rate\n",
    "    pct80 = counts.groupby(level=0)[\"power_rate\"].quantile(power_pct)\n",
    "    # map it back onto each row\n",
    "    counts[\"season_power_80\"] = counts.index.get_level_values(season_col).map(pct80)\n",
    "\n",
    "    # 4) label\n",
    "    def _label(row):\n",
    "        return \"POWER\" if row.power_rate >= row.season_power_80 else \"CONTACT\"\n",
    "\n",
    "    labels = counts.apply(_label, axis=1)\n",
    "    labels.name = \"hitter_type\"\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _rolling_stat_lagged(\n",
    "    df: pd.DataFrame,\n",
    "    group_cols: list[str],\n",
    "    target: str,\n",
    "    stat: str = \"mean\",\n",
    "    window: int = 50,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Group-wise rolling statistic using only *previous* rows.\n",
    "    For each group defined by group_cols, shift the target by 1 row \n",
    "    then compute a rolling(window) agg(stat) with min_periods=10.\n",
    "    \"\"\"\n",
    "    # 1) Within each group, shift the target by one so we only use past data\n",
    "    def shifted_rolling(x: pd.Series) -> pd.Series:\n",
    "        return x.shift(1).rolling(window=window, min_periods=10).agg(stat)\n",
    "\n",
    "    rolled = (\n",
    "        df\n",
    "        .groupby(group_cols)[target]     # group by batter_id or pitcher_id\n",
    "        .apply(shifted_rolling)          # shift & roll inside each group\n",
    "        .reset_index(level=group_cols, drop=True)  # get back a plain Series\n",
    "    )\n",
    "    return rolled\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Public API\n",
    "###############################################################################\n",
    "\n",
    "def feature_engineer(df: pd.DataFrame, copy: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Return a DataFrame enriched with engineered features (no leakage).\"\"\"\n",
    "\n",
    "    if copy:\n",
    "        df = df.copy()\n",
    "\n",
    "    # 1) Uppercase strings\n",
    "    str_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    df[str_cols] = df[str_cols].apply(lambda col: col.str.upper())\n",
    "\n",
    "    # 2) Age\n",
    "    df[\"age_sq\"]   = df[\"age\"] ** 2\n",
    "    df[\"age_bin\"]  = pd.qcut(df[\"age\"], q=4, duplicates='drop')\n",
    "\n",
    "    # 3) Height\n",
    "    avg_height    = df[\"batter_height\"].mean()\n",
    "    df[\"height_diff\"] = df[\"batter_height\"] - avg_height\n",
    "\n",
    "    # 4) Launch & spray bins\n",
    "    df[\"la_bin\"]    = pd.qcut(df[\"launch_angle\"], q=5, duplicates='drop')\n",
    "    df[\"spray_bin\"] = pd.qcut(df[\"spray_angle\"], q=3, duplicates='drop')\n",
    "\n",
    "    # 5) Handedness & matchups\n",
    "    df[\"same_hand\"]        = (df[\"batter_hand\"] == df[\"pitcher_hand\"])\n",
    "    df[\"hand_match\"]       = df[\"batter_hand\"] + \"_VS_\" + df[\"pitcher_hand\"]\n",
    "    df[\"pitch_hand_match\"] = df[\"pitch_group\"] + \"_\" + df[\"hand_match\"]\n",
    "\n",
    "    # 6) Batter lagged rolling EV stats\n",
    "    df[\"player_ev_mean50\"] = _rolling_stat_lagged(df, [\"batter_id\"], \"exit_velo\", \"mean\", 50)\n",
    "    df[\"player_ev_std50\"]  = _rolling_stat_lagged(df, [\"batter_id\"], \"exit_velo\",  \"std\", 50)\n",
    "    gm = df[\"exit_velo\"].mean(); gs = df[\"exit_velo\"].std()\n",
    "    df[\"player_ev_mean50\"].fillna(gm)\n",
    "    df[\"player_ev_std50\"].fillna(gs)\n",
    "\n",
    "    # 7) Pitcher lagged mean\n",
    "    df[\"pitcher_ev_mean50\"] = _rolling_stat_lagged(df, [\"pitcher_id\"], \"exit_velo\", \"mean\", 50)\n",
    "    df[\"pitcher_ev_mean50\"].fillna(gm)\n",
    "\n",
    "    # 8) Center covariates\n",
    "    df[\"age_centered\"]    = df[\"age\"]    - df[\"age\"].median()\n",
    "    df[\"season_centered\"] = df[\"season\"] - df[\"season\"].median()\n",
    "    df[\"level_idx\"]       = df[\"level_abbr\"].map({\"AA\":0, \"AAA\":1, \"MLB\":2})\n",
    "\n",
    "    # --- 9) New season‐by‐season batter classification ---\n",
    "    labels = _classify_hitters_by_season(df)\n",
    "    # bring labels into df by season & batter_id\n",
    "    labels_df = labels.reset_index()  # columns: [season, batter_id, hitter_type]\n",
    "    df = df.merge(labels_df, on=[\"season\",\"batter_id\"], how=\"left\")\n",
    "\n",
    "    # 10) fill anyone still NaN → they had no base hits\n",
    "    df[\"hitter_type\"] = df[\"hitter_type\"].fillna(\"CONTACT\")\n",
    "    return df\n",
    "\n",
    "###############################################################################\n",
    "# CLI entry‑point (quick smoke test)\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path, debug = True)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    print(df_fe.columns)\n",
    "    \n",
    "            \n",
    "    # --- DEBUG: batters with no classification ---\n",
    "    missing_mask = df_fe[\"hitter_type\"].isna()\n",
    "    missing_df   = df_fe[missing_mask].copy()\n",
    "    unique_b     = missing_df[\"batter_id\"].nunique()\n",
    "    print(f\"[DEBUG] {unique_b} unique batters have no label (NaN in hitter_type).\")\n",
    "\n",
    "    # --- Fix sort_values key to map each array's length ---\n",
    "    outcome_by_batter = (\n",
    "        missing_df\n",
    "        .groupby(\"batter_id\")[\"outcome\"]\n",
    "        .unique()\n",
    "        .sort_values(\n",
    "            key=lambda s: s.map(len),      # for each entry, use len(array)\n",
    "            ascending=False\n",
    "        )\n",
    "    )\n",
    "    print(\"[DEBUG] Sample of missing batters → their unique outcomes:\")\n",
    "    print(outcome_by_batter.head(10).to_dict())\n",
    "\n",
    "    print(\"[DEBUG] Outcome value counts among missing batters:\")\n",
    "    print(missing_df[\"outcome\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColumnSchema: separates raw and engineered columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/data/ColumnSchema.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/data/ColumnSchema.py\n",
    "\"\"\" \n",
    "Column schema helper for exit‑velo project.\n",
    "\n",
    "Centralises every raw and engineered column name in one place and exposes\n",
    " type‑safe accessors so downstream code never hard‑codes strings.\n",
    "\n",
    "Usage\n",
    "-----\n",
    ">>> from src.features.columns import cols\n",
    ">>> num_cols = cols.numerical()\n",
    ">>> ord_cols = cols.ordinal()\n",
    ">>> all_for_model = cols.model_features()\n",
    "\"\"\"\n",
    "\n",
    "from functools import lru_cache\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "class _ColumnSchema:\n",
    "    \"\"\"Container for canonical column lists.\n",
    "\n",
    "    Keeping everything behind methods avoids accidental mutation and lets\n",
    "    IDEs offer autocompletion (because the return type is always `List[str]`).\n",
    "    \"\"\"\n",
    "\n",
    "    _ID_COLS: List[str] = [\n",
    "        \"season\", \"batter_id\", \"pitcher_id\",\n",
    "    ]\n",
    "\n",
    "    _ORDINAL_CAT_COLS: List[str] = [\n",
    "        \"level_abbr\",   # AA < AAA < MLB\n",
    "        \"age_bin\",      # 4 quantile bins of age\n",
    "        \"la_bin\",       # 5 quantile bins of launch angle\n",
    "        \"spray_bin\",    # 3 quantile bins of spray angle\n",
    "        # \"outcome\",      # categorical outcome (out, single, double, triple, home run, sacrifice fly, sacrifice bunt, fielders choice)\n",
    "    ]\n",
    "\n",
    "\n",
    "    _NOMINAL_CAT_COLS = [\n",
    "        \"hit_type\",\n",
    "        \"pitch_group\",\n",
    "        \"batter_hand\",\n",
    "        \"pitcher_hand\",\n",
    "        \"hand_match\",\n",
    "        \"pitch_hand_match\",\n",
    "        \"same_hand\",\n",
    "        \"hitter_type\", \n",
    "    ]\n",
    "\n",
    "    _NUMERICAL_COLS = [\n",
    "        \"launch_angle\",\n",
    "        \"spray_angle\",\n",
    "        \"hangtime\",\n",
    "        \"height_diff\",\n",
    "        \"age_sq\",\n",
    "        \"age_centered\",\n",
    "        \"season_centered\",\n",
    "        \"level_idx\",\n",
    "        \"player_ev_mean50\",\n",
    "        \"player_ev_std50\",\n",
    "        \"pitcher_ev_mean50\",\n",
    "        # \"power_rate\",\n",
    "        # \"season_power_80\", \n",
    "    ]\n",
    "\n",
    "\n",
    "    _TARGET_COL: str = \"exit_velo\"\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    # Public helpers\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    def id(self) -> List[str]:\n",
    "        return self._ID_COLS.copy()\n",
    "\n",
    "    def ordinal(self) -> List[str]:\n",
    "        return self._ORDINAL_CAT_COLS.copy()\n",
    "\n",
    "    def nominal(self) -> List[str]:\n",
    "        return self._NOMINAL_CAT_COLS.copy()\n",
    "\n",
    "    def categorical(self) -> List[str]:\n",
    "        \"\"\"All cat cols (ordinal + nominal).\"\"\"\n",
    "        return self._ORDINAL_CAT_COLS + self._NOMINAL_CAT_COLS\n",
    "\n",
    "    def numerical(self) -> List[str]:\n",
    "        return self._NUMERICAL_COLS.copy()\n",
    "\n",
    "\n",
    "    def target(self) -> str:\n",
    "        return self._TARGET_COL\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    @lru_cache(maxsize=1)\n",
    "    def model_features(self) -> List[str]:\n",
    "        \"\"\"Columns fed into the ML pipeline *after* preprocess.\n",
    "\n",
    "        Excludes the target but includes derived cols.\n",
    "        \"\"\"\n",
    "        phys_minus_target = [\n",
    "            c for c in self._NUMERICAL_COLS if c != self._TARGET_COL\n",
    "        ]\n",
    "\n",
    "        return (\n",
    "            phys_minus_target\n",
    "            + self._NOMINAL_CAT_COLS\n",
    "            + self._ORDINAL_CAT_COLS  # some algos want raw string order\n",
    "        )\n",
    "\n",
    "    def all_raw(self) -> List[str]:\n",
    "        \"\"\"Returns every column expected in raw input CSV (incl. engineered).\"\"\"\n",
    "        return (\n",
    "            self._ID_COLS\n",
    "            + self._ORDINAL_CAT_COLS\n",
    "            + self._NOMINAL_CAT_COLS\n",
    "            + self._NUMERICAL_COLS\n",
    "        )\n",
    "\n",
    "    def as_dict(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Dictionary form – handy for YAML/JSON dumps.\"\"\"\n",
    "        return {\n",
    "            \"id\": self.id(),\n",
    "            \"ordinal\": self.ordinal(),\n",
    "            \"nominal\": self.nominal(),\n",
    "            \"numerical\": self.numerical(),\n",
    "            \"target\": [self.target()],\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Target column:      \", cols.target())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    print(\"\\nAs dict (JSON):\")\n",
    "    print(json.dumps(cols.as_dict(), indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/features/eda.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/features/eda.py\n",
    "\n",
    "import pandas as pd  # type: ignore\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from scipy.stats import f_oneway, ttest_ind\n",
    "\n",
    "# Optional imports with fallbacks for advanced statistics\n",
    "try:\n",
    "    import scipy.stats as stats  # type: ignore\n",
    "    from statsmodels.nonparametric.smoothers_lowess import (\n",
    "        lowess  # type: ignore\n",
    "    )\n",
    "    from statsmodels.stats.stattools import (\n",
    "        durbin_watson  # type: ignore\n",
    "    )\n",
    "    _HAS_STATS_LIBS = True\n",
    "except ImportError:\n",
    "    _HAS_STATS_LIBS = False\n",
    "    print(\"Warning: scipy or statsmodels not available. \"\n",
    "          \"Some diagnostics will be limited.\")\n",
    "\n",
    "\n",
    "\n",
    "def get_column_groups() -> dict:\n",
    "    \"\"\"\n",
    "    Return a mapping of column-type → list of columns,\n",
    "    based on the canonical schema in src.features.feature_selection.cols.\n",
    "    \"\"\"\n",
    "    return cols.as_dict()\n",
    "\n",
    "def check_nulls(df: pd.DataFrame):\n",
    "    # Identify columns with null values\n",
    "    null_columns = df.columns[df.isnull().any()].tolist()\n",
    "    \n",
    "    # Output the columns with null values\n",
    "    if null_columns:\n",
    "        print(\"Columns with null values:\", null_columns)\n",
    "    else:\n",
    "        print(\"No columns with null values.\")\n",
    "\n",
    "\n",
    "def quick_pulse_check(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    level_col: str = \"level_abbr\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Print a quick summary table:\n",
    "      - total rows\n",
    "      - unique batters\n",
    "      - overall median exit_velo\n",
    "      - median exit_velo by level\n",
    "      - distribution of events per batter (median, 25th pct)\n",
    "      - distribution of seasons per batter\n",
    "      - pearson correlations of velo with launch_angle & hangtime\n",
    "    Returns a pd.DataFrame with those metrics.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    total_rows = len(df)\n",
    "    n_batters = df[group_col].nunique()\n",
    "    overall_med = df[velo_col].median()\n",
    "\n",
    "    # median by level\n",
    "    med_by_level = df.groupby(level_col)[velo_col].median()\n",
    "\n",
    "    # events per batter\n",
    "    ev_per = df[group_col].value_counts()\n",
    "    ev_stats = ev_per.quantile([0.25, 0.5]).to_dict()\n",
    "\n",
    "    # seasons per batter\n",
    "    seasons_per = df.groupby(group_col)[\"season\"].nunique()\n",
    "    seasons_stats = seasons_per.value_counts().sort_index().to_dict()\n",
    "\n",
    "    # basic correlations\n",
    "    corr = df[[velo_col, \"launch_angle\", \"hangtime\"]].corr()[velo_col].drop(velo_col)\n",
    "\n",
    "    # Build a summary table\n",
    "    metrics = [\n",
    "        \"Total rows\",\n",
    "        \"Unique batters\",\n",
    "        \"Overall median EV\",\n",
    "    ]\n",
    "    values = [\n",
    "        total_rows,\n",
    "        n_batters,\n",
    "        overall_med,\n",
    "    ]\n",
    "    \n",
    "    # Add level-specific metrics\n",
    "    for lvl in med_by_level.index:\n",
    "        metrics.append(f\"Median EV @ {lvl}\")\n",
    "        values.append(med_by_level[lvl])\n",
    "    \n",
    "    # Add batter event metrics\n",
    "    metrics.extend([\n",
    "        \"Events per batter (25th pct)\",\n",
    "        \"Events per batter (median)\",\n",
    "    ])\n",
    "    values.extend([\n",
    "        ev_stats.get(0.25, \"N/A\"),\n",
    "        ev_stats.get(0.5, \"N/A\"),\n",
    "    ])\n",
    "    \n",
    "    # Add season distribution\n",
    "    for season_count, count in seasons_stats.items():\n",
    "        metrics.append(f\"Batters with {season_count} season(s)\")\n",
    "        values.append(count)\n",
    "    \n",
    "    # Add correlations\n",
    "    metrics.extend([\n",
    "        \"ρ(exit_velo, launch_angle)\",\n",
    "        \"ρ(exit_velo, hangtime)\",\n",
    "    ])\n",
    "    values.extend([\n",
    "        corr.get(\"launch_angle\", \"N/A\"),\n",
    "        corr.get(\"hangtime\", \"N/A\"),\n",
    "    ])\n",
    "\n",
    "    table = pd.DataFrame({\n",
    "        \"Metric\": metrics,\n",
    "        \"Value\": values\n",
    "    })\n",
    "    \n",
    "    print(table.to_string(index=False))\n",
    "    return table\n",
    "\n",
    "\n",
    "def red_flag_small_samples(df: pd.DataFrame,\n",
    "                           group_col: str = \"batter_id\",\n",
    "                           threshold: int = 15) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Identify batters with fewer than `threshold` events.\n",
    "    Returns a Series of counts indexed by batter_id.\n",
    "    \"\"\"\n",
    "    counts = df[group_col].value_counts()\n",
    "    small = counts[counts < threshold]\n",
    "    print(f\"> Batters with fewer than {threshold} events: {len(small)}\")\n",
    "    if len(small) > 0:\n",
    "        print(f\"  First few: {', '.join(map(str, small.index[:5]))}\")\n",
    "    return small\n",
    "\n",
    "\n",
    "def red_flag_level_effect(df: pd.DataFrame,\n",
    "                          level_col: str = \"level_abbr\",\n",
    "                          velo_col: str = \"exit_velo\") -> tuple:\n",
    "    \"\"\"\n",
    "    One-way ANOVA of exit_velo across levels.\n",
    "    Returns (F-statistic, p-value) or (None, None) if scipy is not available.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> ANOVA on exit_velo by level: scipy not available\")\n",
    "        print(\"> Basic level summary instead:\")\n",
    "        summary = df.groupby(level_col)[velo_col].agg(['mean', 'std', 'count'])\n",
    "        print(summary)\n",
    "        return None, None\n",
    "    \n",
    "    groups = [\n",
    "        df[df[level_col] == lvl][velo_col].dropna()\n",
    "        for lvl in df[level_col].unique()\n",
    "    ]\n",
    "    F, p = stats.f_oneway(*groups)\n",
    "    print(f\"> ANOVA on {velo_col} by {level_col}: F={F:.3f}, p={p:.3e}\")\n",
    "    return F, p\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  REPLACE old red_flag_level_effect  → clearer name & doc\n",
    "# ------------------------------------------------------------------\n",
    "def league_level_effect(\n",
    "    df: pd.DataFrame,\n",
    "    level_col: str = \"level_abbr\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    ") -> tuple[float | None, float | None]:\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – confirms MLB vs Triple‑A (etc.) differences to\n",
    "      justify hierarchical level effects in the model.\n",
    "\n",
    "    One‑way ANOVA of `exit_velo` across `level_col`.\n",
    "    Returns (F, p) or (None, None) if SciPy unavailable.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> SciPy unavailable – falling back to group summary\")\n",
    "        print(df.groupby(level_col)[velo_col].describe())\n",
    "        return None, None\n",
    "\n",
    "    groups = [df[df[level_col] == lv][velo_col].dropna()\n",
    "              for lv in df[level_col].unique()]\n",
    "    f_val, p_val = stats.f_oneway(*groups)\n",
    "    print(f\"> Level effect ANOVA: F={f_val:.3f}, p={p_val:.3e}\")\n",
    "    return f_val, p_val\n",
    "\n",
    "\n",
    "\n",
    "def diag_age_effect(df: pd.DataFrame,\n",
    "                    age_col: str = \"age_centered\",\n",
    "                    velo_col: str = \"exit_velo\") -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    LOWESS smoothing of exit_velo vs. age_centered.\n",
    "    Returns the smoothed array or None if statsmodels is not available.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> Age effect analysis: statsmodels not available\")\n",
    "        print(\"> Basic correlation instead:\")\n",
    "        corr = df[[age_col, velo_col]].corr().iloc[0, 1]\n",
    "        print(f\"Correlation between {age_col} and {velo_col}: {corr:.3f}\")\n",
    "        return None\n",
    "    \n",
    "    # Run LOWESS smoothing\n",
    "    smooth_result = lowess(df[velo_col], df[age_col])\n",
    "    \n",
    "    # Plot the result\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.scatter(df[age_col], df[velo_col], alpha=0.1, s=1, color='gray')\n",
    "    plt.plot(\n",
    "        smooth_result[:, 0], \n",
    "        smooth_result[:, 1], \n",
    "        'r-', \n",
    "        linewidth=2, \n",
    "        label=\"LOWESS fit\"\n",
    "    )\n",
    "    plt.xlabel(age_col)\n",
    "    plt.ylabel(velo_col)\n",
    "    plt.title(\"Age effect (LOWESS)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return smooth_result\n",
    "\n",
    "\n",
    "def diag_time_series_dw(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"season\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    velo_col: str = \"exit_velo\"\n",
    ") -> pd.Series | None:\n",
    "    \"\"\"\n",
    "    Compute Durbin–Watson on each batter's time series of mean exit_velo.\n",
    "    Returns a Series of DW statistics or None if statsmodels is not available.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> Time series analysis: statsmodels not available\")\n",
    "        return None\n",
    "    \n",
    "    # Create pivot table of seasons (columns) by batters (rows)\n",
    "    pivot = (\n",
    "        df\n",
    "        .groupby([group_col, time_col])[velo_col]\n",
    "        .mean()\n",
    "        .unstack(fill_value=np.nan)\n",
    "    )\n",
    "    \n",
    "    # Only process batters with at least 3 seasons\n",
    "    valid_batters = pivot.dropna(thresh=3).index\n",
    "    if len(valid_batters) == 0:\n",
    "        print(\"> No batters with sufficient seasons for Durbin-Watson test\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate DW statistic for each valid batter\n",
    "    dw_stats = {}\n",
    "    for batter in valid_batters:\n",
    "        series = pivot.loc[batter].dropna()\n",
    "        if len(series) >= 3:  # Recheck after dropna\n",
    "            dw = durbin_watson(series)\n",
    "            dw_stats[batter] = dw\n",
    "    \n",
    "    dw_series = pd.Series(dw_stats)\n",
    "    print(\n",
    "        f\"> Mean Durbin–Watson across {len(dw_series)} batters: \"\n",
    "        f\"{dw_series.mean():.3f}\"\n",
    "    )\n",
    "    print(\"> DW < 1.5 suggests positive autocorrelation\")\n",
    "    print(\"> DW > 2.5 suggests negative autocorrelation\")\n",
    "    print(\"> DW ≈ 2.0 suggests no autocorrelation\")\n",
    "    \n",
    "    return dw_series\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  REPLACE old diag_time_series_dw WITH optional helper\n",
    "# ------------------------------------------------------------------\n",
    "def _optional_dw_check(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"season\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    ") -> pd.Series | None:\n",
    "    \"\"\"\n",
    "    (OPTIONAL) Durbin–Watson residual autocorrelation **per batter**.\n",
    "    Mostly irrelevant for cross‑sectional EV analysis but retained\n",
    "    behind a private name for power users.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        return None\n",
    "    pivot = (\n",
    "        df.groupby([group_col, time_col])[velo_col]\n",
    "          .mean().unstack()\n",
    "    )\n",
    "    stats_out = {}\n",
    "    for idx, row in pivot.dropna(thresh=3).iterrows():\n",
    "        if row.count() >= 3:\n",
    "            stats_out[idx] = durbin_watson(row.dropna())\n",
    "    if not stats_out:\n",
    "        print(\"> DW check: no eligible batters\")\n",
    "        return None\n",
    "    s = pd.Series(stats_out)\n",
    "    print(f\"DW mean={s.mean():.2f} (1.5<→pos autocorr, >2.5→neg)\")\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_red_flags(df: pd.DataFrame, \n",
    "                    sample_threshold: int = 15) -> dict:\n",
    "    \"\"\"\n",
    "    Run all red flag checks and return the results in a dictionary.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Check for small sample sizes\n",
    "    small_samples = red_flag_small_samples(df, threshold=sample_threshold)\n",
    "    results['small_samples'] = small_samples\n",
    "    \n",
    "    # Check for level effects\n",
    "    f_stat, p_val = red_flag_level_effect(df)\n",
    "    results['level_effect'] = {\n",
    "        'f_statistic': f_stat,\n",
    "        'p_value': p_val\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_distributions(df: pd.DataFrame,\n",
    "                       velo_col: str = \"exit_velo\",\n",
    "                       by: str = \"level_abbr\"):\n",
    "    \"\"\"\n",
    "    Histogram of `velo_col` faceted by `by`.\n",
    "    Returns the Matplotlib figure so callers can save or show it.\n",
    "    \"\"\"\n",
    "    groups = df[by].unique()\n",
    "    fig, axes = plt.subplots(len(groups), 1,\n",
    "                             figsize=(6, 2.8 * len(groups)),\n",
    "                             sharex=True)\n",
    "    for ax, grp in zip(axes, groups):\n",
    "        ax.hist(df[df[by] == grp][velo_col], bins=30, alpha=0.75)\n",
    "        ax.set_title(f\"{by} = {grp} (n={len(df[df[by] == grp])})\")\n",
    "        ax.set_xlabel(velo_col)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_correlations(df: pd.DataFrame, cols: list[str]):\n",
    "    \"\"\"\n",
    "    Heat-map of Pearson correlations for `cols`.\n",
    "    \"\"\"\n",
    "    corr = df[cols].corr()\n",
    "    fig, ax = plt.subplots(figsize=(0.6 * len(cols) + 2,\n",
    "                                    0.6 * len(cols) + 2))\n",
    "    im = ax.imshow(corr, vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "    ax.set_xticks(range(len(cols)), cols, rotation=90)\n",
    "    ax.set_yticks(range(len(cols)), cols)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_time_trends(df: pd.DataFrame,\n",
    "                     time_col: str = \"season\",\n",
    "                     group_col: str = \"batter_id\",\n",
    "                     velo_col: str = \"exit_velo\",\n",
    "                     sample: int = 50):\n",
    "    \"\"\"\n",
    "    Plot mean exit-velo over time for a random sample of batters.\n",
    "    \"\"\"\n",
    "    batters = df[group_col].unique()\n",
    "    chosen = np.random.choice(batters,\n",
    "                              min(sample, len(batters)),\n",
    "                              replace=False)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    for b in chosen:\n",
    "        series = (\n",
    "            df[df[group_col] == b]\n",
    "            .groupby(time_col)[velo_col]\n",
    "            .mean()\n",
    "        )\n",
    "        ax.plot(series.index, series.values, alpha=0.3)\n",
    "    ax.set_xlabel(time_col)\n",
    "    ax.set_ylabel(velo_col)\n",
    "    ax.set_title(\"Sample batter exit-velo over time\")\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def summarize_numeric_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    numeric_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarise each numeric predictor against the target.\n",
    "\n",
    "    Returns a DataFrame indexed by feature with:\n",
    "      n          – number of non‑null pairs\n",
    "      pearson_r  – Pearson correlation coefficient\n",
    "    \"\"\"\n",
    "    # --- Pull fresh lists from the schema every time -----------------\n",
    "    groups = cols.as_dict()\n",
    "\n",
    "    if numeric_cols is None:\n",
    "        numeric_cols = groups.get(\"numerical\", [])\n",
    "\n",
    "    # --- Clean the list ---------------------------------------------\n",
    "    numeric_cols = [\n",
    "        c for c in numeric_cols\n",
    "        if c != target_col and c in df.columns      # ❶ exclude target, ❷ guard\n",
    "    ]\n",
    "\n",
    "    records = []\n",
    "    for col in numeric_cols:\n",
    "        sub = df[[col, target_col]].dropna()\n",
    "        if sub.empty:               # skip columns that are all‑NA\n",
    "            continue\n",
    "        r = sub[col].corr(sub[target_col])\n",
    "        records.append({\"feature\": col, \"n\": len(sub), \"pearson_r\": r})\n",
    "\n",
    "    result = (\n",
    "        pd.DataFrame.from_records(records)\n",
    "        .set_index(\"feature\")\n",
    "        .sort_values(\"pearson_r\", ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Numeric vs target correlations ===\")\n",
    "    print(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_numeric_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    numeric_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter plots of each numeric predictor vs the target with r‑value in title.\n",
    "    \"\"\"\n",
    "    summary = summarize_numeric_vs_target(df, numeric_cols, target_col)\n",
    "    for feature, row in summary.iterrows():\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.scatter(\n",
    "            df[feature], df[target_col],\n",
    "            alpha=0.3, s=5, edgecolors=\"none\"\n",
    "        )\n",
    "        plt.title(f\"{feature} vs {target_col}  (r = {row['pearson_r']:.2f})\")\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel(target_col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def summarize_categorical_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    cat_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\"\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each categorical feature, returns a DataFrame of:\n",
    "      count, mean, median, std of the target by category.\n",
    "    \"\"\"\n",
    "    groups = get_column_groups()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = groups.get(\"categorical\", [])\n",
    "\n",
    "    summaries: dict[str, pd.DataFrame] = {}\n",
    "    for col in cat_cols:\n",
    "        stats = (\n",
    "            df\n",
    "            .groupby(col)[target_col]\n",
    "            .agg(count=\"count\", mean=\"mean\", median=\"median\", std=\"std\")\n",
    "            .sort_values(\"count\", ascending=False)\n",
    "        )\n",
    "        print(f\"\\n=== {col} vs {target_col} summary ===\")\n",
    "        print(stats)\n",
    "        summaries[col] = stats\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def plot_categorical_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    cat_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\"\n",
    "):\n",
    "    \"\"\"\n",
    "    For each categorical feature, draw a box‑plot of the target by category.\n",
    "    \"\"\"\n",
    "    groups = get_column_groups()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = groups.get(\"categorical\", [])\n",
    "\n",
    "    for col in cat_cols:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        df.boxplot(column=target_col, by=col, vert=False,\n",
    "                   grid=False, patch_artist=True)\n",
    "        plt.title(f\"{target_col} by {col}\")\n",
    "        plt.suptitle(\"\")           # remove pandas' automatic suptitle\n",
    "        plt.xlabel(target_col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def examine_and_filter_by_sample_size(\n",
    "    df: pd.DataFrame,\n",
    "    count_col: str = \"exit_velo\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    season_col: str = \"season\",\n",
    "    percentile: float = 0.05,\n",
    "    min_count: int | None = None,\n",
    "    filter_df: bool = False,\n",
    ") -> tuple[dict[int, pd.DataFrame], pd.DataFrame | None]:\n",
    "    \"\"\"\n",
    "    For each season:\n",
    "      - compute per-batter count, mean, std of `count_col`\n",
    "      - pick cutoff: min_count if provided, else the `percentile` quantile\n",
    "      - print diagnostics\n",
    "      - plot histograms *safely* (drops NaNs first)\n",
    "    Returns:\n",
    "      - summaries: dict season → per-batter summary DataFrame\n",
    "      - filtered_df: if filter_df, the original df filtered to batters ≥ cutoff\n",
    "    \"\"\"\n",
    "    summaries: dict[int, pd.DataFrame] = {}\n",
    "    mask_keep: list[pd.Series] = []\n",
    "\n",
    "    for season, sub in df.groupby(season_col):\n",
    "        # 1) per-batter summary (count *non-NA* exit_velo)\n",
    "        summary = (\n",
    "            sub\n",
    "            .groupby(group_col)[count_col]\n",
    "            .agg(count=\"count\", mean=\"mean\", std=\"std\")\n",
    "            .sort_values(\"count\")\n",
    "        )\n",
    "        summaries[season] = summary\n",
    "\n",
    "        # 2) determine cutoff\n",
    "        cutoff = min_count if min_count is not None else int(summary[\"count\"].quantile(percentile))\n",
    "        small = summary[summary[\"count\"] < cutoff]\n",
    "        large = summary[summary[\"count\"] >= cutoff]\n",
    "\n",
    "        # 3) diagnostics\n",
    "        print(f\"\\n=== Season {season} (cutoff = {cutoff}) ===\")\n",
    "        print(f\"  small (<{cutoff} events): {len(small)} batters\")\n",
    "        print(small[[\"count\",\"mean\",\"std\"]].describe(), \"\\n\")\n",
    "        print(f\"  large (≥{cutoff} events): {len(large)} batters\")\n",
    "        print(large[[\"count\",\"mean\",\"std\"]].describe())\n",
    "\n",
    "        # 4) **safe plotting**: drop NaNs, skip if nothing to plot\n",
    "        small_means = small[\"mean\"].dropna()\n",
    "        large_means = large[\"mean\"].dropna()\n",
    "\n",
    "        if small_means.empty and large_means.empty:\n",
    "            print(f\"  ⚠️  Season {season}: no valid per-batter means to plot\")\n",
    "        else:\n",
    "            plt.figure(figsize=(8, 3))\n",
    "            if not small_means.empty:\n",
    "                plt.hist(small_means, bins=30, alpha=0.6, label=f\"n<{cutoff}\")\n",
    "            if not large_means.empty:\n",
    "                plt.hist(large_means, bins=30, alpha=0.6, label=f\"n≥{cutoff}\")\n",
    "            plt.title(f\"Season {season}: per-batter EV means\")\n",
    "            plt.xlabel(\"Mean exit_velo\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # 5) build mask to keep only large-sample batters\n",
    "        if filter_df:\n",
    "            keep_ids = large.index\n",
    "            mask_keep.append(\n",
    "                (df[season_col] == season) &\n",
    "                (df[group_col].isin(keep_ids))\n",
    "            )\n",
    "\n",
    "    # 6) combine masks and filter\n",
    "    filtered_df = None\n",
    "    if filter_df and mask_keep:\n",
    "        combined = pd.concat(mask_keep, axis=1).any(axis=1)\n",
    "        filtered_df = df[combined].copy()\n",
    "\n",
    "    return summaries, filtered_df\n",
    "\n",
    "\n",
    "\n",
    "def hypothesis_test(df, feature, target=\"exit_velo\", test_type=\"anova\"):\n",
    "    \"\"\"\n",
    "    Perform hypothesis tests for feature significance.\n",
    "    \"\"\"\n",
    "    if test_type == \"anova\":\n",
    "        groups = [df[df[feature] == cat][target] for cat in df[feature].unique()]\n",
    "        F, p = f_oneway(*groups)\n",
    "        print(f\"ANOVA: F={F:.3f}, p={p:.3e}\")\n",
    "        return F, p\n",
    "    elif test_type == \"ttest\":\n",
    "        group1 = df[df[feature] == 0][target]\n",
    "        group2 = df[df[feature] == 1][target]\n",
    "        t, p = ttest_ind(group1, group2)\n",
    "        print(f\"T-test: t={t:.3f}, p={p:.3e}\")\n",
    "        return t, p\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  NEW: robust outlier flagging\n",
    "# ------------------------------------------------------------------\n",
    "def flag_outliers_iqr(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    iqr_mult: float = 1.5,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – extreme EVs (>120 mph or <40 mph) can distort\n",
    "      skew / variance estimates used in hierarchical priors.\n",
    "\n",
    "    Returns a boolean Series (True = *suspect* outlier) using the\n",
    "    classic IQR rule: value < Q1 − k·IQR  or  > Q3 + k·IQR.\n",
    "    \"\"\"\n",
    "    q1, q3 = df[velo_col].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - iqr_mult * iqr, q3 + iqr_mult * iqr\n",
    "    mask = (df[velo_col] < lower) | (df[velo_col] > upper)\n",
    "    n = int(mask.sum())\n",
    "    print(f\"> Outlier flag ({velo_col}): {n} rows outside [{lower:.1f}, {upper:.1f}]\")\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  NEW: EV distribution summary + QQ plot\n",
    "# ------------------------------------------------------------------\n",
    "def ev_distribution_summary(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    bins: int = 40,\n",
    "):\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – confirms right‑skew & heavy‑tail nature of EV\n",
    "      so you can choose a skew‑normal or Student‑t likelihood.\n",
    "\n",
    "    Prints skew/kurtosis, shows histogram, KDE, CDF & QQ (if scipy).\n",
    "    \"\"\"\n",
    "    data = df[velo_col].dropna()\n",
    "    print(\n",
    "        f\"Skewness = {stats.skew(data):.2f},  \"\n",
    "        f\"Kurtosis = {stats.kurtosis(data, fisher=False):.2f}\"\n",
    "    )\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "    ax[0].hist(data, bins=bins, density=True, alpha=0.7)\n",
    "    data.plot(kind=\"kde\", ax=ax[0], linewidth=2)\n",
    "    ax[0].set_title(\"Histogram & KDE\")\n",
    "\n",
    "    # empirical CDF\n",
    "    ecdf_x = np.sort(data)\n",
    "    ecdf_y = np.arange(1, len(ecdf_x) + 1) / len(ecdf_x)\n",
    "    ax[1].plot(ecdf_x, ecdf_y)\n",
    "    ax[1].set_title(\"Empirical CDF\")\n",
    "\n",
    "    # QQ vs normal\n",
    "    from scipy import stats as _st\n",
    "    _st.probplot(data, dist=\"norm\", plot=ax[2])\n",
    "    ax[2].set_title(\"QQ‑plot vs Normal\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  NEW: Year/era trend diagnostic\n",
    "# ------------------------------------------------------------------\n",
    "def year_trend_ev(\n",
    "    df: pd.DataFrame,\n",
    "    season_col: str = \"season\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    ci: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – detects ball‑era shifts (e.g. 2019 “juiced”,\n",
    "      2021 “deadened”) so forecasts for 2024 use correct baseline.\n",
    "\n",
    "    Produces a table & line plot of mean/median EV per season.\n",
    "    \"\"\"\n",
    "    g = df.groupby(season_col)[velo_col]\n",
    "    stats_df = g.agg(mean=\"mean\", median=\"median\", n=\"count\")\n",
    "    print(\"\\n=== Exit‑velo by season ===\")\n",
    "    print(stats_df)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 3))\n",
    "    stats_df[\"mean\"].plot(ax=ax, marker=\"o\", label=\"Mean EV\")\n",
    "    stats_df[\"median\"].plot(ax=ax, marker=\"s\", label=\"Median EV\")\n",
    "    if ci:\n",
    "        sem = g.sem()\n",
    "        ax.fill_between(\n",
    "            stats_df.index,\n",
    "            stats_df[\"mean\"] - 1.96 * sem,\n",
    "            stats_df[\"mean\"] + 1.96 * sem,\n",
    "            alpha=0.2,\n",
    "            label=\"95% CI (mean)\"\n",
    "        )\n",
    "    ax.set_ylabel(velo_col)\n",
    "    ax.set_title(\"Seasonal trend in exit velocity\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    return stats_df, fig\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  REPLACE old red_flag_level_effect  → clearer name & doc\n",
    "# ------------------------------------------------------------------\n",
    "def league_level_effect(\n",
    "    df: pd.DataFrame,\n",
    "    level_col: str = \"level_abbr\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    ") -> tuple[float | None, float | None]:\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – confirms MLB vs Triple‑A (etc.) differences to\n",
    "      justify hierarchical level effects in the model.\n",
    "\n",
    "    One‑way ANOVA of `exit_velo` across `level_col`.\n",
    "    Returns (F, p) or (None, None) if SciPy unavailable.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> SciPy unavailable – falling back to group summary\")\n",
    "        print(df.groupby(level_col)[velo_col].describe())\n",
    "        return None, None\n",
    "\n",
    "    groups = [df[df[level_col] == lv][velo_col].dropna()\n",
    "              for lv in df[level_col].unique()]\n",
    "    f_val, p_val = stats.f_oneway(*groups)\n",
    "    print(f\"> Level effect ANOVA: F={f_val:.3f}, p={p_val:.3e}\")\n",
    "    return f_val, p_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# debugs:\n",
    "def summarize_categorical_missingness(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each categorical column (ordinal + nominal), compute:\n",
    "      - original_null_count / pct\n",
    "      - imputed_missing_count / pct\n",
    "    Safely handles pandas.Categorical by first adding 'MISSING' to its categories.\n",
    "    \"\"\"\n",
    "    cols    = _ColumnSchema()\n",
    "    cat_cols = cols.ordinal() + cols.nominal()\n",
    "    summary = []\n",
    "    n = len(df)\n",
    "\n",
    "    for col in cat_cols:\n",
    "        ser = df[col]\n",
    "        orig_null = ser.isna().sum()\n",
    "\n",
    "        # If it's a Categorical, add 'MISSING' as a valid category\n",
    "        if is_categorical_dtype(ser):\n",
    "            ser = ser.cat.add_categories(['MISSING'])\n",
    "\n",
    "        # Count rows that would become 'MISSING'\n",
    "        imputed_missing = ser.fillna('MISSING').eq('MISSING').sum()\n",
    "\n",
    "        summary.append({\n",
    "            'column': col,\n",
    "            'original_null_count':   orig_null,\n",
    "            'original_null_pct':     orig_null / n,\n",
    "            'imputed_missing_count': imputed_missing,\n",
    "            'imputed_missing_pct':   imputed_missing / n,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "\n",
    "    # --- inspect nulls in the raw data ---\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values in raw data.\")\n",
    "    else:\n",
    "        print(\"=== Raw data null counts ===\")\n",
    "        for col, cnt in null_counts.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "\n",
    "    \n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:     \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    # use list‐comprehension to drop target(s) from numerical features\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "\n",
    "    # Filter out rows with nulls in the target column(s)\n",
    "    target_cols = cols.target()\n",
    "    df_filtered = df.dropna(subset=target_cols)\n",
    "\n",
    "    # Check for nulls in the filtered data\n",
    "    null_counts_filtered = df_filtered.isnull().sum()\n",
    "    null_counts_filtered = null_counts_filtered[null_counts_filtered > 0]\n",
    "    if null_counts_filtered.empty:\n",
    "        print(\"✅  No missing values in filtered data.\")\n",
    "    else:\n",
    "        print(\"=== Filtered data null counts ===\")\n",
    "        for col, cnt in null_counts_filtered.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "\n",
    "    print(\"\\n===== check on small samples =====\")\n",
    "    summaries, _ = examine_and_filter_by_sample_size(df_filtered, percentile=0.05)\n",
    "    summaries, df_filtered = examine_and_filter_by_sample_size(\n",
    "        df_filtered, percentile=0.05, min_count=15, filter_df=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Example usage\n",
    "    print(\"\\n===== NULLS CHECK =====\")\n",
    "    check_nulls(df_fe)\n",
    "    \n",
    "    print(\"\\n===== QUICK PULSE CHECK =====\")\n",
    "    quick_pulse_check(df_fe)\n",
    "    \n",
    "    print(\"\\n===== RED FLAGS CHECK =====\")\n",
    "    check_red_flags(df_fe)\n",
    "    \n",
    "    print(\"\\n===== AGE EFFECT ANALYSIS =====\")\n",
    "    diag_age_effect(df_fe, age_col=\"age\")\n",
    "    \n",
    "    print(\"\\n===== TIME SERIES ANALYSIS =====\")\n",
    "    diag_time_series_dw(df_fe)\n",
    "    \n",
    "    print(\"\\n===== PLOTTING =====\")\n",
    "    fig1 = plot_distributions(df_fe, by=\"hit_type\")\n",
    "    fig2 = plot_correlations(df_fe, numericals)  # Using cols schema\n",
    "    fig3 = plot_time_trends(df_fe, sample=20)\n",
    "\n",
    "\n",
    "    # — Numeric features —\n",
    "    num_summary = summarize_numeric_vs_target(df_fe)\n",
    "    plot_numeric_vs_target(df_fe)\n",
    "\n",
    "    # — Categorical features —\n",
    "    cat_summary = summarize_categorical_vs_target(df_fe)\n",
    "    plot_categorical_vs_target(df_fe)\n",
    "\n",
    "    # Example: Test if age has significant effect\n",
    "    hypothesis_test(df_fe, feature=\"age_bin\", test_type=\"anova\")\n",
    "    \n",
    "    \n",
    "    league_level_effect(df_fe)\n",
    "    year_trend_ev(df_fe)\n",
    "    flag_outliers_iqr(df_fe)\n",
    "    ev_distribution_summary(df_fe)\n",
    "    # _optional_dw_check(df_fe)   # only if you still care\n",
    "\n",
    "\n",
    "    summary_df = summarize_categorical_missingness(df_fe)\n",
    "    print(summary_df.to_markdown(index=False))\n",
    "    \n",
    "    # uniques of outcome\n",
    "    print(\"outcome uniques:=========================\")\n",
    "    print(df_fe[\"outcome\"].unique())\n",
    "    # uniques of hit_type\n",
    "    print(\"hit_type uniques:=========================\")\n",
    "    print(df_fe[\"hit_type\"].unique())\n",
    "\n",
    "\n",
    "    # assume df is your feature-engineered DataFrame\n",
    "    df = df.copy()\n",
    "    # 1) Flag missing hangtime\n",
    "    df['hangtime_missing'] = df['hangtime'].isna()\n",
    "\n",
    "    # 2) Contingency of hit_type vs missingness\n",
    "    hit_type_ct = pd.crosstab(\n",
    "        df['hit_type'].fillna('UNKNOWN'),\n",
    "        df['hangtime_missing'],\n",
    "        margins=True,\n",
    "        normalize='columns'\n",
    "    )\n",
    "    print(\"\\nProportion of hit_type when hangtime is missing:\")\n",
    "    print(hit_type_ct)\n",
    "\n",
    "    # 3) Contingency of outcome vs missingness\n",
    "    outcome_ct = pd.crosstab(\n",
    "        df['outcome'].fillna('UNKNOWN'),\n",
    "        df['hangtime_missing'],\n",
    "        margins=True,\n",
    "        normalize='columns'\n",
    "    )\n",
    "    print(\"\\nProportion of outcome when hangtime is missing:\")\n",
    "    print(outcome_ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/features/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/features/data_prep.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def compute_clip_bounds(\n",
    "    series: pd.Series,\n",
    "    *,\n",
    "    method: str = \"quantile\",\n",
    "    quantiles: tuple[float,float] = (0.01,0.99),\n",
    "    std_multiplier: float = 3.0,\n",
    ") -> tuple[float,float]:\n",
    "    \"\"\"\n",
    "    Compute (lower, upper) but do not apply them.\n",
    "    \"\"\"\n",
    "    s = series.dropna()\n",
    "    if method == \"quantile\":\n",
    "        return tuple(s.quantile(list(quantiles)).to_list())\n",
    "    if method == \"mean_std\":\n",
    "        mu, sigma = s.mean(), s.std()\n",
    "        return mu - std_multiplier*sigma, mu + std_multiplier*sigma\n",
    "    if method == \"iqr\":\n",
    "        q1, q3 = s.quantile([0.25,0.75])\n",
    "        iqr = q3 - q1\n",
    "        return q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    raise ValueError(f\"Unknown method {method}\")\n",
    "\n",
    "\n",
    "def clip_extreme_ev(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    lower: float | None = None,\n",
    "    upper: float | None = None,\n",
    "    *,\n",
    "    method: str = \"quantile\",\n",
    "    quantiles: tuple[float, float] = (0.01, 0.99),\n",
    "    std_multiplier: float = 3.0,\n",
    "    debug: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clip exit velocities to [lower, upper].\n",
    "\n",
    "    If lower/upper are None, compute them from the data using one of:\n",
    "      - method=\"quantile\": use df[velo_col].quantile(quantiles)\n",
    "      - method=\"mean_std\":  use mean ± std_multiplier * std\n",
    "      - method=\"iqr\":       use [Q1 - 1.5*IQR, Q3 + 1.5*IQR]\n",
    "\n",
    "    When debug=True, prints counts & percentages of values that will be clipped.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    series = df[velo_col].dropna()\n",
    "\n",
    "    # 1) infer bounds if not given\n",
    "    if lower is None or upper is None:\n",
    "        if method == \"quantile\":\n",
    "            low_q, high_q = quantiles\n",
    "            lower_, upper_ = series.quantile([low_q, high_q]).to_list()\n",
    "        elif method == \"mean_std\":\n",
    "            mu, sigma = series.mean(), series.std()\n",
    "            lower_, upper_ = mu - std_multiplier * sigma, mu + std_multiplier * sigma\n",
    "        elif method == \"iqr\":\n",
    "            q1, q3 = series.quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            lower_, upper_ = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method '{method}' for clip_extreme_ev\")\n",
    "        lower  = lower  if lower  is not None else lower_\n",
    "        upper  = upper  if upper  is not None else upper_\n",
    "\n",
    "    # 2) debug: count how many will be clipped\n",
    "    if debug:\n",
    "        total   = len(series)\n",
    "        n_low   = (series < lower).sum()\n",
    "        n_high  = (series > upper).sum()\n",
    "        print(f\"[clip_extreme_ev] lower={lower:.2f}, upper={upper:.2f}\")\n",
    "        print(f\"  → {n_low:,} / {total:,} ({n_low/total:.2%}) below lower\")\n",
    "        print(f\"  → {n_high:,} / {total:,} ({n_high/total:.2%}) above upper\")\n",
    "\n",
    "    # 3) actually clip\n",
    "    df[velo_col] = df[velo_col].clip(lower, upper)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def filter_bunts_and_popups(\n",
    "    df: pd.DataFrame,\n",
    "    hit_col: str = \"hit_type\",\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop bunts and pop-ups from the DataFrame.\n",
    "    If debug=True, prints how many rows were removed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    initial = len(df)\n",
    "    mask = ~df[hit_col].str.upper().isin([\"BUNT\", \"POP_UP\"])\n",
    "    filtered = df[mask]\n",
    "    if debug:\n",
    "        removed = initial - len(filtered)\n",
    "        print(f\"[filter_bunts_and_popups] dropped {removed:,} rows \"\n",
    "              f\"({removed/initial:.2%}) bunts/pop-ups\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def filter_low_event_batters(\n",
    "    df: pd.DataFrame,\n",
    "    batter_col: str = \"batter_id\",\n",
    "    min_events: int = 15,\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop all rows for batters with fewer than min_events total events.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    counts = df[batter_col].value_counts()\n",
    "    keep_batters = counts[counts >= min_events].index\n",
    "    initial = len(df)\n",
    "    filtered = df[df[batter_col].isin(keep_batters)]\n",
    "    if debug:\n",
    "        removed = initial - len(filtered)\n",
    "        print(f\"[filter_low_event_batters] dropped {removed:,} rows ({removed/initial:.2%}) for batters with <{min_events} events\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def filter_physical_implausibles(\n",
    "    df: pd.DataFrame,\n",
    "    hang_col: str = \"hangtime\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    min_hang: float = 0.5,\n",
    "    max_velo: float = 115.0,\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop rows where hangtime < min_hang AND exit_velo > max_velo,\n",
    "    as these are likely sensor glitches (e.g., foul tips).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    initial = len(df)\n",
    "    mask = ~((df[hang_col] < min_hang) & (df[velo_col] > max_velo))\n",
    "    filtered = df[mask]\n",
    "    if debug:\n",
    "        removed = initial - len(filtered)\n",
    "        print(f\"[filter_physical_implausibles] dropped {removed:,} rows ({removed/initial:.2%}) for hangtime<{min_hang} & exit_velo>{max_velo}\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Smoke test / CLI entry\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.data.ColumnSchema import _ColumnSchema\n",
    "    from src.features.eda import summarize_categorical_missingness\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    # from src.features.data_prep import filter_and_clip, compute_clip_bounds, clip_extreme_ev, filter_bunts_and_popups, filter_low_event_batters, filter_physical_implausibles\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    \n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    # use list‐comprehension to drop target(s) from numerical features\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "    \n",
    "\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    summary_df = summarize_categorical_missingness(df_fe)\n",
    "    print(summary_df.to_markdown(index=False))\n",
    "    \n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    \n",
    "    # filter out bunts and popups\n",
    "    df_fe = filter_bunts_and_popups(df_fe)\n",
    "    # chekc on bunts and popups\n",
    "    print(df_fe[\"hit_type\"].unique())\n",
    "    print(df_fe[\"outcome\"].unique())\n",
    "\n",
    "    debug = True\n",
    "    TARGET = cols.target()\n",
    "    lower, upper = compute_clip_bounds(\n",
    "        df[TARGET],\n",
    "        method=\"quantile\",            # default: 1st/99th percentile\n",
    "        quantiles=(0.01, 0.99),\n",
    "    )\n",
    "    if debug:\n",
    "        total = len(df)\n",
    "        n_low = (df[TARGET] < lower).sum()\n",
    "        n_high= (df[TARGET] > upper).sum()\n",
    "        print(f\"[fit_preprocessor] clipping train EV to [{lower:.2f}, {upper:.2f}]\")\n",
    "        print(f\"  → {n_low:,}/{total:,} ({n_low/total:.2%}) below\")\n",
    "        print(f\"  → {n_high:,}/{total:,} ({n_high/total:.2%}) above\")\n",
    "    df_clipped = clip_extreme_ev(df, lower=lower, upper=upper)\n",
    "\n",
    "    print(\"Final rows after filter & clip:\", len(df_clipped))\n",
    "\n",
    "\n",
    "    # 1) drop batters with too few events\n",
    "    df_fe = filter_low_event_batters(df_fe, batter_col=\"batter_id\", min_events=15, debug=debug)\n",
    "\n",
    "    # 2) drop physical implausibles\n",
    "    df_fe = filter_physical_implausibles(\n",
    "        df_fe,\n",
    "        hang_col=\"hangtime\",\n",
    "        velo_col=\"exit_velo\",\n",
    "        debug=debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/features/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/features/preprocess.py\n",
    "\"\"\"\n",
    "Preprocessing module for exit velocity pipeline.\n",
    "Supports multiple model types (linear, XGBoost, PyMC, etc.) with\n",
    "automatic ordinal-category detection from the data.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, MissingIndicator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.features.data_prep import (filter_bunts_and_popups\n",
    "                                    , compute_clip_bounds\n",
    "                                    , clip_extreme_ev\n",
    "                                    , filter_low_event_batters\n",
    "                                    , filter_physical_implausibles)\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# Numeric & nominal pipelines (unchanged)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "numeric_linear = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median', add_indicator=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "numeric_iterative = Pipeline([\n",
    "    ('impute', IterativeImputer(random_state=0, add_indicator=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "nominal_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='MISSING')),\n",
    "    ('encode', OneHotEncoder(drop='first', handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# Helper function for domain cleaning to reduce duplication\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def filter_and_clip(\n",
    "    df: pd.DataFrame,\n",
    "    lower: float = None,\n",
    "    upper: float = None,\n",
    "    quantiles: tuple[float, float] = (0.01, 0.99),\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the dataset by:\n",
    "    1. Filtering out bunts and popups\n",
    "    2. Clipping extreme exit velocity values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame to clean\n",
    "    lower : float, optional\n",
    "        Lower bound for clipping, computed from data if None\n",
    "    upper : float, optional\n",
    "        Upper bound for clipping, computed from data if None\n",
    "    quantiles : tuple[float, float], optional\n",
    "        Quantiles to use if computing bounds from data\n",
    "    debug : bool, optional\n",
    "        Whether to print diagnostic information\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cleaned DataFrame (copy of input)\n",
    "    tuple[float, float]\n",
    "        The (lower, upper) bounds used for clipping\n",
    "    \"\"\"\n",
    "    cols = _ColumnSchema()\n",
    "    TARGET = cols.target()\n",
    "    \n",
    "    # 1. Filter out bunts and popups\n",
    "    df = filter_bunts_and_popups(df, debug=debug)\n",
    "    \n",
    "    # 2. Compute bounds if not provided\n",
    "    if lower is None or upper is None:\n",
    "        lower_computed, upper_computed = compute_clip_bounds(\n",
    "            df[TARGET], method=\"quantile\", quantiles=quantiles\n",
    "        )\n",
    "        lower = lower if lower is not None else lower_computed\n",
    "        upper = upper if upper is not None else upper_computed\n",
    "    \n",
    "    # 3. Clip extreme values\n",
    "    df = clip_extreme_ev(df, lower=lower, upper=upper, debug=debug)\n",
    "\n",
    "    # 1) drop batters with too few events\n",
    "    df = filter_low_event_batters(df, batter_col=\"batter_id\", min_events=15, debug=debug)\n",
    "\n",
    "    # 2) drop physical implausibles\n",
    "    df = filter_physical_implausibles(\n",
    "        df,\n",
    "        hang_col=\"hangtime\",\n",
    "        velo_col=\"exit_velo\",\n",
    "        debug=debug)\n",
    "    \n",
    "    return df, (lower, upper)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# Dynamic preprocess functions\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) fit_preprocessor (with train‐set‐only bound computation + clip)  \n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def fit_preprocessor(\n",
    "    df: pd.DataFrame,\n",
    "    model_type: str = \"linear\",\n",
    "    debug: bool = False,\n",
    "    quantiles: tuple[float, float] = (0.01, 0.99),\n",
    "    max_safe_rows: int = 200000\n",
    ") -> tuple[np.ndarray, pd.Series, ColumnTransformer]:\n",
    "    \"\"\"\n",
    "    Fit a preprocessing pipeline to transform raw data into model-ready features,\n",
    "    adding a MissingIndicator for ordinal features.\n",
    "\n",
    "    IMPORTANT: Call this on TRAINING data only.\n",
    "    \"\"\"\n",
    "    if len(df) > max_safe_rows:\n",
    "        warnings.warn(\n",
    "            f\"Dataset has {len(df)} rows (>{max_safe_rows}); \"\n",
    "            \"ensure you’re only fitting on TRAINING data.\",\n",
    "            UserWarning\n",
    "        )\n",
    "\n",
    "    # 0) Domain cleaning and clipping\n",
    "    df, (lower, upper) = filter_and_clip(df, quantiles=quantiles, debug=debug)\n",
    "\n",
    "    cols = _ColumnSchema()\n",
    "    TARGET = cols.target()\n",
    "\n",
    "    # 1) Split out features + coerce numerics\n",
    "    num_feats = [c for c in cols.numerical() if c != TARGET]\n",
    "    ord_feats = cols.ordinal()\n",
    "    nom_feats = cols.nominal()\n",
    "\n",
    "    df[num_feats] = df[num_feats].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X = df[num_feats + ord_feats + nom_feats].copy()\n",
    "    y = df[TARGET]\n",
    "\n",
    "    # 2) Prepare ordinal columns\n",
    "    X[ord_feats] = X[ord_feats].astype(\"string\")\n",
    "    X.loc[:, ord_feats] = X.loc[:, ord_feats].mask(X[ord_feats].isna(), np.nan)\n",
    "    ordinal_categories = [[*X[c].dropna().unique(), \"MISSING\"] for c in ord_feats]\n",
    "    ordinal_pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"MISSING\")),\n",
    "        (\"encode\", OrdinalEncoder(\n",
    "            categories=ordinal_categories,\n",
    "            handle_unknown=\"use_encoded_value\",\n",
    "            unknown_value=-1,\n",
    "            dtype=\"int32\"\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    # 3) Numeric pipeline selection\n",
    "    if model_type == \"linear\":\n",
    "        num_imputer = SimpleImputer(strategy=\"median\", add_indicator=True)\n",
    "    else:\n",
    "        num_imputer = IterativeImputer(\n",
    "            random_state=0,\n",
    "            add_indicator=True\n",
    "        )\n",
    "\n",
    "    numeric_pipe = Pipeline([\n",
    "        (\"impute\", num_imputer),\n",
    "        (\"scale\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    # 4) ColumnTransformer with an extra MissingIndicator for ordinals\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"num\", numeric_pipe, num_feats),\n",
    "            (\"ord_ind\", MissingIndicator(missing_values=np.nan), ord_feats),\n",
    "            (\"ord\", ordinal_pipe, ord_feats),\n",
    "            (\"nom\", nominal_pipe, nom_feats),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    # preserve clipping bounds for transform\n",
    "    ct.lower_, ct.upper_ = lower, upper\n",
    "\n",
    "    X_mat = ct.fit_transform(X, y)\n",
    "    return X_mat, y, ct\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def transform_preprocessor(\n",
    "    df: pd.DataFrame,\n",
    "    transformer: ColumnTransformer,\n",
    ") -> tuple[np.ndarray, pd.Series]:\n",
    "    \"\"\"\n",
    "    Transform new data using a fitted preprocessor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        New data to transform\n",
    "    transformer : ColumnTransformer\n",
    "        Fitted transformer from fit_preprocessor\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_mat : np.ndarray\n",
    "        Transformed feature matrix\n",
    "    y : pd.Series\n",
    "        Target values\n",
    "    \"\"\"\n",
    "    cols, TARGET = _ColumnSchema(), _ColumnSchema().target()\n",
    "\n",
    "    # Domain filter & clip using the bounds from the fitted transformer\n",
    "    df, _ = filter_and_clip(df, lower=transformer.lower_, upper=transformer.upper_)\n",
    "\n",
    "    # rebuild lists & coerce numerics\n",
    "    num_feats = [c for c in cols.numerical() if c != TARGET]\n",
    "    ord_feats = cols.ordinal()\n",
    "    nom_feats = cols.nominal()\n",
    "    df[num_feats] = df[num_feats].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    X = df[num_feats + ord_feats + nom_feats].copy()\n",
    "\n",
    "    # **NEW** – ensure ordinals are strings, then fill unseen→\"MISSING\"\n",
    "    X[ord_feats] = X[ord_feats].astype(\"string\")\n",
    "    X.loc[:, ord_feats] = X.loc[:, ord_feats].where(X.loc[:, ord_feats].notna(), \"MISSING\")\n",
    "\n",
    "    y = df[TARGET]\n",
    "    X_mat = transformer.transform(X)\n",
    "    return X_mat, y\n",
    "\n",
    "\n",
    "def inverse_transform_preprocessor(\n",
    "    X_trans: np.ndarray,\n",
    "    transformer: ColumnTransformer\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Invert each block of a ColumnTransformer back to its original features,\n",
    "    skipping transformers that lack inverse_transform (e.g., MissingIndicator).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import warnings\n",
    "    from sklearn.impute import MissingIndicator  # for isinstance check\n",
    "\n",
    "    # 1) Recover original feature order\n",
    "    orig_features: list[str] = []\n",
    "    for name, _, cols in transformer.transformers_:\n",
    "        if cols == 'drop': continue\n",
    "        orig_features.extend(cols)\n",
    "\n",
    "    parts = []\n",
    "    start = 0\n",
    "    n_rows = X_trans.shape[0]\n",
    "\n",
    "    # 2) Loop through each transformer block\n",
    "    for name, trans, cols in transformer.transformers_:\n",
    "        if cols == 'drop':\n",
    "            continue\n",
    "\n",
    "        fitted = transformer.named_transformers_[name]\n",
    "\n",
    "        # 2a) Determine number of output columns\n",
    "        dummy = np.zeros((1, len(cols)))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "            try:\n",
    "                out = fitted.transform(dummy)\n",
    "            except Exception:\n",
    "                out = dummy\n",
    "        n_out = out.shape[1]\n",
    "\n",
    "        # 2b) Slice the real transformed data block\n",
    "        block = X_trans[:, start : start + n_out]\n",
    "        start += n_out\n",
    "\n",
    "        # 2c) Handle each transformer type\n",
    "        if isinstance(fitted, MissingIndicator):\n",
    "            # Skip MissingIndicator: it has no inverse_transform\n",
    "            continue  # :contentReference[oaicite:2]{index=2}\n",
    "        elif trans == 'passthrough':\n",
    "            inv = block\n",
    "        elif name == 'num':\n",
    "            scaler = fitted.named_steps['scale']\n",
    "            inv_full = scaler.inverse_transform(block)\n",
    "            inv = inv_full[:, :len(cols)]\n",
    "        else:\n",
    "            # Ordinal or nominal pipelines\n",
    "            if isinstance(fitted, Pipeline):\n",
    "                last = list(fitted.named_steps.values())[-1]\n",
    "                inv = last.inverse_transform(block)\n",
    "            else:\n",
    "                inv = fitted.inverse_transform(block)\n",
    "\n",
    "        parts.append(pd.DataFrame(inv, columns=cols, index=range(n_rows)))\n",
    "\n",
    "    # 3) Concatenate & reorder to match original features\n",
    "    df_orig = pd.concat(parts, axis=1)\n",
    "    return df_orig[orig_features]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_for_mixed_and_hierarchical(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the rows *and* adds convenience covariates expected by the\n",
    "    hierarchical and mixed-effects models.\n",
    "    \"\"\"\n",
    "    cols = _ColumnSchema()\n",
    "    TARGET = cols.target()\n",
    "\n",
    "    # Apply domain cleaning with default quantiles\n",
    "    df, _ = filter_and_clip(df.copy())\n",
    "\n",
    "    # Category coding for batter\n",
    "    df[\"batter_id\"] = df[\"batter_id\"].astype(\"category\")\n",
    "\n",
    "    # New: Category coding for season\n",
    "    df[\"season_cat\"] = df[\"season\"].astype(\"category\")\n",
    "    df[\"season_idx\"] = df[\"season_cat\"].cat.codes\n",
    "\n",
    "    # New: Category coding for pitcher\n",
    "    df[\"pitcher_cat\"] = df[\"pitcher_id\"].astype(\"category\")\n",
    "    df[\"pitcher_idx\"] = df[\"pitcher_cat\"].cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    df_fe = feature_engineer(df)\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "\n",
    "\n",
    "    train_df, test_df = train_test_split(df_fe, test_size=0.2, random_state=42)\n",
    "    # run with debug prints\n",
    "    X_train, y_train, tf = fit_preprocessor(train_df, model_type='linear', debug=True)\n",
    "    X_test,  y_test      = transform_preprocessor(test_df, tf)\n",
    "\n",
    "    print(\"Processed shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "    # Example of inverse transform: \n",
    "    print(\"==========Example of inverse transform:==========\")\n",
    "    df_back = inverse_transform_preprocessor(X_train, tf)\n",
    "    print(\"\\n✅ Inverse‐transformed head (should mirror your original X_train):\")\n",
    "    print(df_back.head())\n",
    "    print(\"Shape:\", df_back.shape, \"→ original X_train shape before transform:\", X_train.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/features/feature_selection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/features/feature_selection.py\n",
    "import pandas as pd\n",
    "\n",
    "# ── NEW: model and importance imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "\n",
    "from pathlib import Path\n",
    "from src.data.load_data import load_and_clean_data\n",
    "from src.features.feature_engineering import feature_engineer\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "# ── NEW: shapash and shapiq imports\n",
    "from shapash import SmartExplainer\n",
    "import shapiq\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def train_baseline_model(X, y):\n",
    "    \"\"\"\n",
    "    Fit a RandomForestRegressor on X, y.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    # You can adjust hyperparameters as needed\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def compute_permutation_importance(\n",
    "    model,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    n_repeats: int = 10,\n",
    "    n_jobs: int = 1,\n",
    "    max_samples: float | int = None,\n",
    "    random_state: int = 42,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute permutation importances with controlled resource usage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : estimator\n",
    "        Fitted model implementing .predict and .score.\n",
    "    X : pd.DataFrame\n",
    "        Features.\n",
    "    y : pd.Series or array\n",
    "        Target.\n",
    "    n_repeats : int\n",
    "        Number of shuffles per feature.\n",
    "    n_jobs : int\n",
    "        Number of parallel jobs (avoid -1 on Windows).\n",
    "    max_samples : float or int, optional\n",
    "        If float in (0,1], fraction of rows to sample.\n",
    "        If int, absolute number of rows to sample.\n",
    "    random_state : int\n",
    "        Seed for reproducibility.\n",
    "    verbose : bool\n",
    "        Print debug info if True.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: feature, importance_mean, importance_std.\n",
    "        Sorted descending by importance_mean.\n",
    "    \"\"\"\n",
    "    # Debug info\n",
    "    if verbose:\n",
    "        print(f\"⏳ Computing permutation importances on {X.shape[0]} rows × {X.shape[1]} features\")\n",
    "        print(f\"   n_repeats={n_repeats}, n_jobs={n_jobs}, max_samples={max_samples}\")\n",
    "\n",
    "    # Subsample if requested\n",
    "    X_sel, y_sel = X, y\n",
    "    if max_samples is not None:\n",
    "        if isinstance(max_samples, float):\n",
    "            nsamp = int(len(X) * max_samples)\n",
    "        else:\n",
    "            nsamp = int(max_samples)\n",
    "        if verbose:\n",
    "            print(f\"   Subsampling to {nsamp} rows for speed\")\n",
    "        X_sel, y_sel = resample(X, y, replace=False, n_samples=nsamp, random_state=random_state)\n",
    "\n",
    "    try:\n",
    "        result = permutation_importance(\n",
    "            model,\n",
    "            X_sel, y_sel,\n",
    "            n_repeats=n_repeats,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "    except OSError as e:\n",
    "        # Graceful fallback to single job\n",
    "        if verbose:\n",
    "            print(f\"⚠️  OSError ({e}). Retrying with n_jobs=1\")\n",
    "        result = permutation_importance(\n",
    "            model,\n",
    "            X_sel, y_sel,\n",
    "            n_repeats=n_repeats,\n",
    "            random_state=random_state,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "    # Build and sort DataFrame\n",
    "    importance_df = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": X.columns,\n",
    "            \"importance_mean\": result.importances_mean,\n",
    "            \"importance_std\": result.importances_std,\n",
    "        })\n",
    "        .sort_values(\"importance_mean\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"✅ Permutation importances computed.\")\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def compute_shap_importance(model, X, nsamples=100):\n",
    "    \"\"\"\n",
    "    Compute mean absolute SHAP values per feature.\n",
    "    Returns a DataFrame sorted by importance.\n",
    "    \"\"\"\n",
    "    # DeepExplainer or TreeExplainer for tree-based models\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    # sample for speed\n",
    "    X_sample = X.sample(n=min(nsamples, len(X)), random_state=42)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    # For regression, shap_values is a 2D array\n",
    "    mean_abs_shap = pd.DataFrame({\n",
    "        \"feature\": X_sample.columns,\n",
    "        \"shap_importance\": np.abs(shap_values).mean(axis=0),\n",
    "    })\n",
    "    mean_abs_shap = mean_abs_shap.sort_values(\"shap_importance\", ascending=False).reset_index(drop=True)\n",
    "    return mean_abs_shap\n",
    "\n",
    "\n",
    "\n",
    "def filter_permutation_features(\n",
    "    importance_df: pd.DataFrame,\n",
    "    threshold: float\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Return features whose permutation importance_mean exceeds threshold.\n",
    "    \"\"\"\n",
    "    kept = importance_df.loc[\n",
    "        importance_df[\"importance_mean\"] > threshold, \"feature\"\n",
    "    ]\n",
    "    return kept.tolist()\n",
    "\n",
    "\n",
    "def filter_shap_features(\n",
    "    importance_df: pd.DataFrame,\n",
    "    threshold: float\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Return features whose shap_importance exceeds threshold.\n",
    "    \"\"\"\n",
    "    kept = importance_df.loc[\n",
    "        importance_df[\"shap_importance\"] > threshold, \"feature\"\n",
    "    ]\n",
    "    return kept.tolist()\n",
    "\n",
    "\n",
    "def select_final_features(\n",
    "    perm_feats: list[str],\n",
    "    shap_feats: list[str],\n",
    "    mode: str = \"intersection\"\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Combine permutation and SHAP feature lists.\n",
    "    mode=\"intersection\" for features in both lists,\n",
    "    mode=\"union\" for features in either list.\n",
    "    \"\"\"\n",
    "    set_perm = set(perm_feats)\n",
    "    set_shap = set(shap_feats)\n",
    "    if mode == \"union\":\n",
    "        final = set_perm | set_shap\n",
    "    else:\n",
    "        final = set_perm & set_shap\n",
    "    # return sorted for reproducibility\n",
    "    return sorted(final)\n",
    "\n",
    "\n",
    "\n",
    "def load_final_features(\n",
    "    file_path: str = \"data/models/features/final_features.txt\"\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Read the newline-delimited feature names file and return as a list.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as fp:\n",
    "        return [line.strip() for line in fp if line.strip()]\n",
    "\n",
    "\n",
    "def filter_to_final_features(\n",
    "    df: pd.DataFrame,\n",
    "    file_path: str = \"data/models/features/final_features.txt\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a feature-engineered DataFrame, load the final feature list,\n",
    "    then return df[ ID_cols + final_features + [target] ].\n",
    "    \"\"\"\n",
    "    # load the feature names\n",
    "    final_feats = load_final_features(file_path)\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    keep = cols.id() + final_feats + [cols.target()]\n",
    "    missing = set(keep) - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Cannot filter: missing columns {missing}\")\n",
    "    return df[keep].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- existing loading & schema logic ---\n",
    "    raw_path = Path(\"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\")\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values in raw data.\")\n",
    "    else:\n",
    "        print(\"=== Raw data null counts ===\")\n",
    "        for col, cnt in null_counts.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "    df_fe = feature_engineer(df)\n",
    "    print(\"Raw →\", df.shape, \"//  Feature-engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "\n",
    "    cols = _ColumnSchema()\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:     \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "\n",
    "    # ── STEP 1: fully preprocess the engineered DataFrame ──\n",
    "    from src.features.preprocess import fit_preprocessor, inverse_transform_preprocessor\n",
    "\n",
    "    # fit_preprocessor returns (X_matrix, y, fitted_transformer)\n",
    "    X_np, y, preproc = fit_preprocessor(df_fe, model_type='linear', debug=False)\n",
    "\n",
    "    # Use the same index that y carries (only non-bunt, non-NA rows)\n",
    "    idx = y.index\n",
    "    \n",
    "    # turn that into a DataFrame with the same column names:\n",
    "    feat_names = preproc.get_feature_names_out()\n",
    "    X = pd.DataFrame(X_np, columns=feat_names, index=idx)\n",
    "    print(f\"✅ Preprocessed feature matrix: {X.shape[0]} rows × {X.shape[1]} cols\")\n",
    "\n",
    "    # (optional) confirm inverse transform lines up:\n",
    "    df_back = inverse_transform_preprocessor(X_np, preproc)\n",
    "    df_back.index = idx\n",
    "    print(\"✅ inverse_transform round-trip (head):\")\n",
    "    print(df_back.head())\n",
    "\n",
    "    # ── STEP 2: train & compute importances on *that* X ──\n",
    "    print(\"\\nTraining baseline model…\")\n",
    "    model = train_baseline_model(X, y)\n",
    "\n",
    "    print(\"\\n🔍 Permutation Importances:\")\n",
    "    perm_imp = compute_permutation_importance(\n",
    "        model, X, y,\n",
    "        n_repeats=10,\n",
    "        n_jobs=2,            # test small parallelism\n",
    "        max_samples=0.5,     # test subsampling\n",
    "        verbose=True\n",
    "    )\n",
    "    print(perm_imp)\n",
    "\n",
    "\n",
    "    print(\"\\n🔍 SHAP Importances:\")\n",
    "    shap_imp = compute_shap_importance(model, X)\n",
    "    print(shap_imp)\n",
    "\n",
    "    # ── STEP 3: threshold & select your final features ──\n",
    "    perm_thresh = 0.01\n",
    "    shap_thresh = 0.01\n",
    "    perm_feats = filter_permutation_features(perm_imp, perm_thresh)\n",
    "    shap_feats = filter_shap_features(shap_imp, shap_thresh)\n",
    "    final_feats = select_final_features(perm_feats, shap_feats, mode=\"intersection\")\n",
    "    print(f\"\\nFinal preprocessed feature list ({len(final_feats)}):\")\n",
    "    print(final_feats)\n",
    "\n",
    "    # ── STEP 4: build & save a dataset with just those features + target + IDs ──\n",
    "    df_final = pd.concat([\n",
    "        df_fe[cols.id()],\n",
    "        df_fe[[cols.target()]],\n",
    "        X[final_feats]\n",
    "    ], axis=1)\n",
    "    print(\"Final dataset shape:\", df_final.shape)\n",
    "\n",
    "    Path(\"data/models/features/final_features.txt\").write_text(\"\\n\".join(final_feats))\n",
    "    print(\"✔️ Saved feature list to final_features.txt\")\n",
    "\n",
    "\n",
    "    # Demo: filter the full df_fe back to just those features\n",
    "    df_filtered = filter_to_final_features(df_fe)\n",
    "    print(\"Filtered to final features shape:\", df_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model choices\n",
    "\n",
    "see modelling_choices.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows with NaN in exit_velo\n",
      "season               0\n",
      "level_abbr           0\n",
      "batter_id            0\n",
      "pitcher_id           0\n",
      "exit_velo        97979\n",
      "launch_angle      9130\n",
      "spray_angle       6227\n",
      "hangtime         75968\n",
      "hit_type           201\n",
      "batter_hand          0\n",
      "pitcher_hand         0\n",
      "batter_height        0\n",
      "pitch_group          0\n",
      "outcome              0\n",
      "age                  0\n",
      "dtype: int64\n",
      "after rows dropped with NaN in exit_velo\n",
      "season               0\n",
      "level_abbr           0\n",
      "batter_id            0\n",
      "pitcher_id           0\n",
      "exit_velo            0\n",
      "launch_angle         0\n",
      "spray_angle          0\n",
      "hangtime         12429\n",
      "hit_type             0\n",
      "batter_hand          0\n",
      "pitcher_hand         0\n",
      "batter_height        0\n",
      "pitch_group          0\n",
      "outcome              0\n",
      "age                  0\n",
      "dtype: int64\n",
      "⚠️  Nulls after target‑filter:\n",
      "  • hangtime         12,429 (1.00%)\n",
      "   season level_abbr  batter_id  pitcher_id  exit_velo  launch_angle  \\\n",
      "0    2023        mlb        235        1335    95.7352       47.2362   \n",
      "1    2023        mlb       3182        1335    95.9380        4.7291   \n",
      "2    2023        mlb       3856        1988    89.1404      -16.2251   \n",
      "3    2023        mlb       2017        1988    88.7278       -6.8385   \n",
      "4    2023        mlb       1594        1988    89.2888        0.5079   \n",
      "\n",
      "   spray_angle  hangtime     hit_type batter_hand pitcher_hand  batter_height  \\\n",
      "0      -6.4422    6.4960     fly_ball           R            R             72   \n",
      "1      -4.8052    0.7806  ground_ball           L            R             75   \n",
      "2      15.2382    0.0311  ground_ball           S            R             72   \n",
      "3     -11.5988    0.1215  ground_ball           R            R             69   \n",
      "4     -22.1899    0.3802  ground_ball           R            R             73   \n",
      "\n",
      "  pitch_group outcome   age  \n",
      "0          FB     out  32.8  \n",
      "1          OS     out  29.2  \n",
      "2          OS     out  29.7  \n",
      "3          BB     out  23.4  \n",
      "4          FB     out  35.3  \n",
      "Index(['season', 'level_abbr', 'batter_id', 'pitcher_id', 'exit_velo',\n",
      "       'launch_angle', 'spray_angle', 'hangtime', 'hit_type', 'batter_hand',\n",
      "       'pitcher_hand', 'batter_height', 'pitch_group', 'outcome', 'age'],\n",
      "      dtype='object')\n",
      "=== Raw data null counts ===\n",
      " • 'hangtime': 12429 missing\n",
      "Raw → (1246545, 15) //  Feature‑engineered → (1246545, 30)\n",
      "   season level_abbr  batter_id  pitcher_id  exit_velo  launch_angle  \\\n",
      "0    2023        MLB        235        1335    95.7352       47.2362   \n",
      "1    2023        MLB       3182        1335    95.9380        4.7291   \n",
      "2    2023        MLB       3856        1988    89.1404      -16.2251   \n",
      "3    2023        MLB       2017        1988    88.7278       -6.8385   \n",
      "4    2023        MLB       1594        1988    89.2888        0.5079   \n",
      "\n",
      "   spray_angle  hangtime     hit_type batter_hand  ... same_hand  hand_match  \\\n",
      "0      -6.4422    6.4960     FLY_BALL           R  ...      True      R_VS_R   \n",
      "1      -4.8052    0.7806  GROUND_BALL           L  ...     False      L_VS_R   \n",
      "2      15.2382    0.0311  GROUND_BALL           S  ...     False      S_VS_R   \n",
      "3     -11.5988    0.1215  GROUND_BALL           R  ...      True      R_VS_R   \n",
      "4     -22.1899    0.3802  GROUND_BALL           R  ...      True      R_VS_R   \n",
      "\n",
      "  pitch_hand_match player_ev_mean50  player_ev_std50  pitcher_ev_mean50  \\\n",
      "0        FB_R_VS_R              NaN              NaN                NaN   \n",
      "1        OS_L_VS_R              NaN              NaN                NaN   \n",
      "2        OS_S_VS_R              NaN              NaN                NaN   \n",
      "3        BB_R_VS_R              NaN              NaN                NaN   \n",
      "4        FB_R_VS_R              NaN              NaN                NaN   \n",
      "\n",
      "  age_centered  season_centered level_idx hitter_type  \n",
      "0          6.4              2.0         2     CONTACT  \n",
      "1          2.8              2.0         2     CONTACT  \n",
      "2          3.3              2.0         2       POWER  \n",
      "3         -3.0              2.0         2     CONTACT  \n",
      "4          8.9              2.0         2     CONTACT  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "ID columns:          ['season', 'batter_id', 'pitcher_id']\n",
      "Ordinal columns:     ['level_abbr', 'age_bin', 'la_bin', 'spray_bin']\n",
      "Nominal columns:     ['hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand', 'hitter_type']\n",
      "All categorical:     ['level_abbr', 'age_bin', 'la_bin', 'spray_bin', 'hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand', 'hitter_type']\n",
      "Numerical columns:   ['launch_angle', 'spray_angle', 'hangtime', 'height_diff', 'age_sq', 'age_centered', 'season_centered', 'level_idx', 'player_ev_mean50', 'player_ev_std50', 'pitcher_ev_mean50']\n",
      "Model features:      ['launch_angle', 'spray_angle', 'hangtime', 'height_diff', 'age_sq', 'age_centered', 'season_centered', 'level_idx', 'player_ev_mean50', 'player_ev_std50', 'pitcher_ev_mean50', 'hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand', 'hitter_type', 'level_abbr', 'age_bin', 'la_bin', 'spray_bin']\n",
      "Target columns:   exit_velo\n",
      "All raw columns:     ['season', 'batter_id', 'pitcher_id', 'level_abbr', 'age_bin', 'la_bin', 'spray_bin', 'hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand', 'hitter_type', 'launch_angle', 'spray_angle', 'hangtime', 'height_diff', 'age_sq', 'age_centered', 'season_centered', 'level_idx', 'player_ev_mean50', 'player_ev_std50', 'pitcher_ev_mean50']\n",
      "| column           |   original_null_count |   original_null_pct |   imputed_missing_count |   imputed_missing_pct |\n",
      "|:-----------------|----------------------:|--------------------:|------------------------:|----------------------:|\n",
      "| level_abbr       |                     0 |                   0 |                       0 |                     0 |\n",
      "| age_bin          |                     0 |                   0 |                       0 |                     0 |\n",
      "| la_bin           |                     0 |                   0 |                       0 |                     0 |\n",
      "| spray_bin        |                     0 |                   0 |                       0 |                     0 |\n",
      "| hit_type         |                     0 |                   0 |                       0 |                     0 |\n",
      "| pitch_group      |                     0 |                   0 |                       0 |                     0 |\n",
      "| batter_hand      |                     0 |                   0 |                       0 |                     0 |\n",
      "| pitcher_hand     |                     0 |                   0 |                       0 |                     0 |\n",
      "| hand_match       |                     0 |                   0 |                       0 |                     0 |\n",
      "| pitch_hand_match |                     0 |                   0 |                       0 |                     0 |\n",
      "| same_hand        |                     0 |                   0 |                       0 |                     0 |\n",
      "| hitter_type      |                     0 |                   0 |                       0 |                     0 |\n",
      "🛠️  Nulls in X before fit_transform:\n",
      "=== Null counts post-engineering ===\n",
      "hangtime             12429\n",
      "player_ev_mean50     30872\n",
      "player_ev_std50      30872\n",
      "pitcher_ev_mean50    35955\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Marlins_Data_Science_Project/src/features/preprocess.py:124: UserWarning: Dataset has 997236 rows (>200000); ensure you’re only fitting on TRAINING data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filter_bunts_and_popups] dropped 70,284 rows (7.05%) bunts/pop-ups\n",
      "[clip_extreme_ev] lower=49.01, upper=110.25\n",
      "  → 9,270 / 926,952 (1.00%) below lower\n",
      "  → 9,268 / 926,952 (1.00%) above upper\n",
      "[filter_low_event_batters] dropped 4,353 rows (0.47%) for batters with <15 events\n",
      "[filter_physical_implausibles] dropped 0 rows (0.00%) for hangtime<0.5 & exit_velo>115.0\n",
      "Processed shapes: (922599, 50) (227415, 50)\n",
      "==========Example of inverse transform:==========\n",
      "\n",
      "✅ Inverse‐transformed head (should mirror your original X_train):\n",
      "   launch_angle  spray_angle  hangtime  height_diff  age_sq  age_centered  \\\n",
      "0       20.8142       1.3858    3.8519      0.81666  686.44          -0.2   \n",
      "1      -75.5873      24.1692    0.0250      1.81666  817.96           2.2   \n",
      "2       -4.0382       5.8295    0.1551      3.81666  576.00          -2.4   \n",
      "3      -24.0564      42.8805    0.0543     -0.18334  823.69           2.3   \n",
      "4       33.4007      11.4086    5.6830     -3.18334  681.21          -0.3   \n",
      "\n",
      "   season_centered  level_idx  player_ev_mean50  player_ev_std50  ...  \\\n",
      "0             -2.0        0.0         87.179676        13.061763  ...   \n",
      "1              2.0        0.0         88.659742        11.356806  ...   \n",
      "2             -2.0        0.0         89.364194        15.835891  ...   \n",
      "3              1.0        2.0         90.890562        16.252956  ...   \n",
      "4              0.0        2.0         88.196544        15.333166  ...   \n",
      "\n",
      "              la_bin        spray_bin     hit_type pitch_group batter_hand  \\\n",
      "0    (20.24, 34.842]  (-12.378, 9.48]     FLY_BALL          FB           R   \n",
      "1  (-89.685, -8.162]  (9.48, 179.962]  GROUND_BALL          BB           R   \n",
      "2    (-8.162, 7.452]  (-12.378, 9.48]  GROUND_BALL          BB           R   \n",
      "3  (-89.685, -8.162]  (9.48, 179.962]  GROUND_BALL          OS           S   \n",
      "4    (20.24, 34.842]  (9.48, 179.962]     FLY_BALL          OS           S   \n",
      "\n",
      "  pitcher_hand hand_match pitch_hand_match same_hand hitter_type  \n",
      "0            R     R_VS_R        FB_R_VS_R      True     CONTACT  \n",
      "1            R     R_VS_R        BB_R_VS_R      True     CONTACT  \n",
      "2            R     R_VS_R        BB_R_VS_R      True     CONTACT  \n",
      "3            R     S_VS_R        OS_S_VS_R     False     CONTACT  \n",
      "4            R     S_VS_R        OS_S_VS_R     False     CONTACT  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "Shape: (922599, 27) → original X_train shape before transform: (922599, 50)\n",
      "Ridge regression RMSE: 12.2698\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/models/linear.py\n",
    "\n",
    "\"\"\"\n",
    "Fast linear baselines (OLS and Ridge).\n",
    "\n",
    "Usage\n",
    "-----\n",
    ">>> from src.models.linear import fit_ridge\n",
    ">>> fitted, rmse = fit_ridge(train_df, val_df)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def _split_xy(df: pd.DataFrame):\n",
    "    X = df.drop(columns=[\"exit_velo\"])\n",
    "    y = df[\"exit_velo\"]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fit_ridge(X_tr: pd.DataFrame,\n",
    "              y_tr: pd.DataFrame,\n",
    "              X_te: pd.DataFrame,\n",
    "              y_te: pd.DataFrame,\n",
    "              alpha: float = 1.0):\n",
    "    \"\"\"\n",
    "    Returns (sklearn Pipeline, RMSE on test set).\n",
    "    \"\"\"\n",
    "\n",
    "    model = Pipeline(\n",
    "        [(\"reg\" , Ridge(alpha=alpha, random_state=0))]\n",
    "    ).fit(X_tr, y_tr)\n",
    "\n",
    "    pred = model.predict(X_te)\n",
    "    rmse = np.sqrt(np.mean((pred - y_te) ** 2))\n",
    "    return model, rmse\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.data.ColumnSchema import _ColumnSchema\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from src.features.eda import summarize_categorical_missingness\n",
    "    from src.features.preprocess import fit_preprocessor, transform_preprocessor, inverse_transform_preprocessor\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "\n",
    "    # --- inspect nulls in the raw data ---\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values in raw data.\")\n",
    "    else:\n",
    "        print(\"=== Raw data null counts ===\")\n",
    "        for col, cnt in null_counts.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    # use list‐comprehension to drop target(s) from numerical features\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "\n",
    "    summary_df = summarize_categorical_missingness(df_fe)\n",
    "    print(summary_df.to_markdown(index=False))\n",
    "\n",
    "\n",
    "    # check nulls\n",
    "    print(\"🛠️  Nulls in X before fit_transform:\")\n",
    "    null_counts = df_fe.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values after feature engineering.\")\n",
    "    else:\n",
    "        print(\"=== Null counts post-engineering ===\")\n",
    "        print(null_counts)\n",
    "\n",
    "    train_df, test_df = train_test_split(df_fe, test_size=0.2, random_state=42)\n",
    "\n",
    "    \n",
    "    # run with debug prints\n",
    "    X_train, y_train, tf = fit_preprocessor(train_df, model_type='linear', debug=True)\n",
    "    X_test,  y_test      = transform_preprocessor(test_df, tf)\n",
    "\n",
    "        \n",
    "    print(\"Processed shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "    # Example of inverse transform: \n",
    "    print(\"==========Example of inverse transform:==========\")\n",
    "    df_back = inverse_transform_preprocessor(X_train, tf)\n",
    "    print(\"\\n✅ Inverse‐transformed head (should mirror your original X_train):\")\n",
    "    print(df_back.head())\n",
    "    print(\"Shape:\", df_back.shape, \"→ original X_train shape before transform:\", X_train.shape)\n",
    "    \n",
    "\n",
    "    # === NEW: Train & evaluate Ridge regression ===\n",
    "    model_ridge, rmse_ridge = fit_ridge(X_train, y_train, X_test,  y_test)\n",
    "    print(f\"Ridge regression RMSE: {rmse_ridge:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/gbm_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/gbm_utils.py\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def save_pipeline(model, preprocessor, path: str = \"data/models/saved_models/gbm_pipeline.joblib\"):\n",
    "    \"\"\"\n",
    "    Save both model and preprocessor together to a single file.\n",
    "    \"\"\"\n",
    "    out_path = Path(path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(\n",
    "        {\"model\": model, \"preprocessor\": preprocessor},\n",
    "        out_path\n",
    "    )\n",
    "    print(f\"✅ Pipeline saved to {out_path.resolve()}\")\n",
    "\n",
    "def load_pipeline(path: str = \"data/models/saved_models/gbm_pipeline.joblib\"):\n",
    "    \"\"\"\n",
    "    Load the model+preprocessor dict back into memory.\n",
    "    Returns a tuple: (model, preprocessor)\n",
    "    \"\"\"\n",
    "    saved = joblib.load(path)\n",
    "    return saved[\"model\"], saved[\"preprocessor\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows with NaN in exit_velo\n",
      "season               0\n",
      "level_abbr           0\n",
      "batter_id            0\n",
      "pitcher_id           0\n",
      "exit_velo        97979\n",
      "launch_angle      9130\n",
      "spray_angle       6227\n",
      "hangtime         75968\n",
      "hit_type           201\n",
      "batter_hand          0\n",
      "pitcher_hand         0\n",
      "batter_height        0\n",
      "pitch_group          0\n",
      "outcome              0\n",
      "age                  0\n",
      "dtype: int64\n",
      "after rows dropped with NaN in exit_velo\n",
      "season               0\n",
      "level_abbr           0\n",
      "batter_id            0\n",
      "pitcher_id           0\n",
      "exit_velo            0\n",
      "launch_angle         0\n",
      "spray_angle          0\n",
      "hangtime         12429\n",
      "hit_type             0\n",
      "batter_hand          0\n",
      "pitcher_hand         0\n",
      "batter_height        0\n",
      "pitch_group          0\n",
      "outcome              0\n",
      "age                  0\n",
      "dtype: int64\n",
      "⚠️  Nulls after target‑filter:\n",
      "  • hangtime         12,429 (1.00%)\n",
      "   season level_abbr  batter_id  pitcher_id  exit_velo  launch_angle  \\\n",
      "0    2023        mlb        235        1335    95.7352       47.2362   \n",
      "1    2023        mlb       3182        1335    95.9380        4.7291   \n",
      "2    2023        mlb       3856        1988    89.1404      -16.2251   \n",
      "3    2023        mlb       2017        1988    88.7278       -6.8385   \n",
      "4    2023        mlb       1594        1988    89.2888        0.5079   \n",
      "\n",
      "   spray_angle  hangtime     hit_type batter_hand pitcher_hand  batter_height  \\\n",
      "0      -6.4422    6.4960     fly_ball           R            R             72   \n",
      "1      -4.8052    0.7806  ground_ball           L            R             75   \n",
      "2      15.2382    0.0311  ground_ball           S            R             72   \n",
      "3     -11.5988    0.1215  ground_ball           R            R             69   \n",
      "4     -22.1899    0.3802  ground_ball           R            R             73   \n",
      "\n",
      "  pitch_group outcome   age  \n",
      "0          FB     out  32.8  \n",
      "1          OS     out  29.2  \n",
      "2          OS     out  29.7  \n",
      "3          BB     out  23.4  \n",
      "4          FB     out  35.3  \n",
      "Index(['season', 'level_abbr', 'batter_id', 'pitcher_id', 'exit_velo',\n",
      "       'launch_angle', 'spray_angle', 'hangtime', 'hit_type', 'batter_hand',\n",
      "       'pitcher_hand', 'batter_height', 'pitch_group', 'outcome', 'age'],\n",
      "      dtype='object')\n",
      "Raw → (1246545, 15) //  Feature‑engineered → (1246545, 30)\n",
      "   season level_abbr  batter_id  pitcher_id  exit_velo  launch_angle  \\\n",
      "0    2023        MLB        235        1335    95.7352       47.2362   \n",
      "1    2023        MLB       3182        1335    95.9380        4.7291   \n",
      "2    2023        MLB       3856        1988    89.1404      -16.2251   \n",
      "3    2023        MLB       2017        1988    88.7278       -6.8385   \n",
      "4    2023        MLB       1594        1988    89.2888        0.5079   \n",
      "\n",
      "   spray_angle  hangtime     hit_type batter_hand  ... same_hand  hand_match  \\\n",
      "0      -6.4422    6.4960     FLY_BALL           R  ...      True      R_VS_R   \n",
      "1      -4.8052    0.7806  GROUND_BALL           L  ...     False      L_VS_R   \n",
      "2      15.2382    0.0311  GROUND_BALL           S  ...     False      S_VS_R   \n",
      "3     -11.5988    0.1215  GROUND_BALL           R  ...      True      R_VS_R   \n",
      "4     -22.1899    0.3802  GROUND_BALL           R  ...      True      R_VS_R   \n",
      "\n",
      "  pitch_hand_match player_ev_mean50  player_ev_std50  pitcher_ev_mean50  \\\n",
      "0        FB_R_VS_R              NaN              NaN                NaN   \n",
      "1        OS_L_VS_R              NaN              NaN                NaN   \n",
      "2        OS_S_VS_R              NaN              NaN                NaN   \n",
      "3        BB_R_VS_R              NaN              NaN                NaN   \n",
      "4        FB_R_VS_R              NaN              NaN                NaN   \n",
      "\n",
      "  age_centered  season_centered level_idx hitter_type  \n",
      "0          6.4              2.0         2     CONTACT  \n",
      "1          2.8              2.0         2     CONTACT  \n",
      "2          3.3              2.0         2       POWER  \n",
      "3         -3.0              2.0         2     CONTACT  \n",
      "4          8.9              2.0         2     CONTACT  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "ID columns:          ['season', 'batter_id', 'pitcher_id']\n",
      "Ordinal columns:     ['level_abbr', 'age_bin', 'la_bin', 'spray_bin']\n",
      "Nominal columns:     ['hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand', 'hitter_type']\n",
      "All categorical:     ['level_abbr', 'age_bin', 'la_bin', 'spray_bin', 'hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand', 'hitter_type']\n",
      "Numerical columns:   ['launch_angle', 'spray_angle', 'hangtime', 'height_diff', 'age_sq', 'age_centered', 'season_centered', 'level_idx', 'player_ev_mean50', 'player_ev_std50', 'pitcher_ev_mean50']\n",
      "Model features:      ['launch_angle', 'spray_angle', 'hangtime', 'height_diff', 'age_sq', 'age_centered', 'season_centered', 'level_idx', 'player_ev_mean50', 'player_ev_std50', 'pitcher_ev_mean50', 'hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand', 'hitter_type', 'level_abbr', 'age_bin', 'la_bin', 'spray_bin']\n",
      "Target columns:   exit_velo\n",
      "All raw columns:     ['season', 'batter_id', 'pitcher_id', 'level_abbr', 'age_bin', 'la_bin', 'spray_bin', 'hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand', 'hitter_type', 'launch_angle', 'spray_angle', 'hangtime', 'height_diff', 'age_sq', 'age_centered', 'season_centered', 'level_idx', 'player_ev_mean50', 'player_ev_std50', 'pitcher_ev_mean50']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Marlins_Data_Science_Project/src/features/preprocess.py:124: UserWarning: Dataset has 997236 rows (>200000); ensure you’re only fitting on TRAINING data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filter_bunts_and_popups] dropped 70,284 rows (7.05%) bunts/pop-ups\n",
      "[clip_extreme_ev] lower=49.01, upper=110.25\n",
      "  → 9,270 / 926,952 (1.00%) below lower\n",
      "  → 9,268 / 926,952 (1.00%) above upper\n",
      "[filter_low_event_batters] dropped 4,353 rows (0.47%) for batters with <15 events\n",
      "[filter_physical_implausibles] dropped 0 rows (0.00%) for hangtime<0.5 & exit_velo>115.0\n",
      "Processed shapes: (922599, 50) (227415, 50)\n",
      "==========Example of inverse transform:==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-10 01:30:35,430] A new study created in memory with name: no-name-907d6353-5312-4851-a8b7-e97559368cb2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Inverse‐transformed head (should mirror your original X_train):\n",
      "   launch_angle  spray_angle  hangtime  height_diff  age_sq  age_centered  \\\n",
      "0       20.8142       1.3858    3.8519      0.81666  686.44          -0.2   \n",
      "1      -75.5873      24.1692    0.0250      1.81666  817.96           2.2   \n",
      "2       -4.0382       5.8295    0.1551      3.81666  576.00          -2.4   \n",
      "3      -24.0564      42.8805    0.0543     -0.18334  823.69           2.3   \n",
      "4       33.4007      11.4086    5.6830     -3.18334  681.21          -0.3   \n",
      "\n",
      "   season_centered  level_idx  player_ev_mean50  player_ev_std50  ...  \\\n",
      "0             -2.0        0.0         87.179676        13.061763  ...   \n",
      "1              2.0        0.0         88.659742        11.356806  ...   \n",
      "2             -2.0        0.0         89.364194        15.835891  ...   \n",
      "3              1.0        2.0         90.890562        16.252956  ...   \n",
      "4              0.0        2.0         88.196544        15.333166  ...   \n",
      "\n",
      "              la_bin        spray_bin     hit_type pitch_group batter_hand  \\\n",
      "0    (20.24, 34.842]  (-12.378, 9.48]     FLY_BALL          FB           R   \n",
      "1  (-89.685, -8.162]  (9.48, 179.962]  GROUND_BALL          BB           R   \n",
      "2    (-8.162, 7.452]  (-12.378, 9.48]  GROUND_BALL          BB           R   \n",
      "3  (-89.685, -8.162]  (9.48, 179.962]  GROUND_BALL          OS           S   \n",
      "4    (20.24, 34.842]  (9.48, 179.962]     FLY_BALL          OS           S   \n",
      "\n",
      "  pitcher_hand hand_match pitch_hand_match same_hand hitter_type  \n",
      "0            R     R_VS_R        FB_R_VS_R      True     CONTACT  \n",
      "1            R     R_VS_R        BB_R_VS_R      True     CONTACT  \n",
      "2            R     R_VS_R        BB_R_VS_R      True     CONTACT  \n",
      "3            R     S_VS_R        OS_S_VS_R     False     CONTACT  \n",
      "4            R     S_VS_R        OS_S_VS_R     False     CONTACT  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "Shape: (922599, 27) → original X_train shape before transform: (997236, 29)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "[I 2025-05-10 01:32:16,031] Trial 0 finished with value: 11.330794208658668 and parameters: {'n_estimators': 876, 'learning_rate': 0.001829411229902222, 'max_depth': 3, 'subsample': 0.9194359796075602, 'colsample_bytree': 0.7180324428269802}. Best is trial 0 with value: 11.330794208658668.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "[I 2025-05-10 01:34:47,269] Trial 1 finished with value: 11.11239686688854 and parameters: {'n_estimators': 952, 'learning_rate': 0.0011100115232957845, 'max_depth': 5, 'subsample': 0.6841529390951004, 'colsample_bytree': 0.594240580311729}. Best is trial 1 with value: 11.11239686688854.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "[I 2025-05-10 01:35:04,338] Trial 2 finished with value: 12.743320661190962 and parameters: {'n_estimators': 101, 'learning_rate': 0.0017024672055382917, 'max_depth': 5, 'subsample': 0.7107136256211728, 'colsample_bytree': 0.5138157668049286}. Best is trial 1 with value: 11.11239686688854.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "[I 2025-05-10 01:35:57,606] Trial 3 finished with value: 7.905642413870368 and parameters: {'n_estimators': 335, 'learning_rate': 0.06318084888306816, 'max_depth': 6, 'subsample': 0.7942759019575363, 'colsample_bytree': 0.9826284106316614}. Best is trial 3 with value: 7.905642413870368.\n",
      "[I 2025-05-10 01:39:15,438] Trial 4 finished with value: 10.233497694177446 and parameters: {'n_estimators': 835, 'learning_rate': 0.0010365152596794316, 'max_depth': 8, 'subsample': 0.6062359470176342, 'colsample_bytree': 0.8070757075560511}. Best is trial 3 with value: 7.905642413870368.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned params: {'n_estimators': 335, 'learning_rate': 0.06318084888306816, 'max_depth': 6, 'subsample': 0.7942759019575363, 'colsample_bytree': 0.9826284106316614}\n",
      "Tuned XGBoost RMSE: 7.8807\n",
      "✅ Pipeline saved to /workspaces/Marlins_Data_Science_Project/models/gbm_pipeline.joblib\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/models/gbm.py\n",
    "\n",
    "\"\"\"\n",
    "Gradient‑boosting baseline (XGBoost regressor).\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost.callback import EarlyStopping     # ① import the new callback\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import optuna\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from xgboost.core import XGBoostError\n",
    "from src.utils.gbm_utils import save_pipeline, load_pipeline\n",
    "\n",
    "# ————— Detect GPU support using modern API —————\n",
    "try:\n",
    "    XGBRegressor(tree_method=\"hist\", device=\"cuda\")\n",
    "    GPU_SUPPORT = True\n",
    "except XGBoostError:\n",
    "    GPU_SUPPORT = False\n",
    "\n",
    "def _split_xy(df: pd.DataFrame):\n",
    "    X = df.drop(columns=[\"exit_velo\"])\n",
    "    y = df[\"exit_velo\"]\n",
    "    return X, y\n",
    "\n",
    "def tune_gbm(X, y, n_trials: int = 50):\n",
    "    \"\"\"\n",
    "    Run an Optuna study to minimize CV RMSE of an XGBRegressor.\n",
    "    Uses device=cuda if available, else CPU.\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"random_state\": 0,\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "        params.update({\"tree_method\": \"hist\"})\n",
    "        model = XGBRegressor(**params)\n",
    "        scores = cross_val_score(\n",
    "            model, X, y,\n",
    "            scoring=\"neg_root_mean_squared_error\",\n",
    "            cv=3, n_jobs=-1\n",
    "        )\n",
    "        return -scores.mean()\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "\n",
    "\n",
    "def fit_gbm(X_tr, y_tr, X_te, y_te, **gbm_kw):\n",
    "    \"\"\"\n",
    "    Train an XGBRegressor with optional hyperparameters and early stopping.\n",
    "    Returns (fitted_model, rmse_on_test).\n",
    "    \"\"\"\n",
    "\n",
    "    # A) Default constructor args\n",
    "    constructor_defaults = dict(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        random_state=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "    )\n",
    "    constructor_defaults.update(gbm_kw)\n",
    "\n",
    "    # B) Extract early_stopping_rounds (pop it out)\n",
    "    early_stopping = constructor_defaults.pop(\"early_stopping_rounds\", None)\n",
    "\n",
    "    # C) Instantiate model\n",
    "    model = XGBRegressor(**constructor_defaults)\n",
    "\n",
    "    # D) Prepare fit kwargs\n",
    "    fit_kwargs = {}\n",
    "    # If an early_stopping value was provided, use the callback interface\n",
    "    if early_stopping:\n",
    "        fit_kwargs[\"callbacks\"] = [EarlyStopping(rounds=early_stopping)]\n",
    "        # pass eval_set so callback can work\n",
    "        fit_kwargs[\"eval_set\"] = [(X_te, y_te)]\n",
    "    # You can still pass verbose if desired\n",
    "    fit_kwargs[\"verbose\"] = False\n",
    "\n",
    "    # Fit the model with or without early stopping\n",
    "    model.fit(X_tr, y_tr, **fit_kwargs)\n",
    "\n",
    "    # E) Compute RMSE on test\n",
    "    preds = model.predict(X_te)\n",
    "    rmse  = root_mean_squared_error(y_te, preds)\n",
    "\n",
    "    return model, rmse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.features.preprocess import (fit_preprocessor\n",
    "                                        ,transform_preprocessor\n",
    "                                        ,inverse_transform_preprocessor)\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    df_fe = feature_engineer(df)\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "\n",
    "\n",
    "    train_df, test_df = train_test_split(df_fe, test_size=0.2, random_state=42)\n",
    "    # run with debug prints\n",
    "    # Record original feature count before preprocessing\n",
    "    X_orig = train_df.drop(columns=[\"exit_velo\"])\n",
    "    orig_shape = X_orig.shape\n",
    "\n",
    "    # Run preprocessing\n",
    "    X_train, y_train, tf = fit_preprocessor(train_df, model_type='linear', debug=True)\n",
    "    X_test,  y_test      = transform_preprocessor(test_df, tf)\n",
    "\n",
    "    print(\"Processed shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "    # Example of inverse transform: \n",
    "    print(\"==========Example of inverse transform:==========\")\n",
    "    df_back = inverse_transform_preprocessor(X_train, tf)\n",
    "    print(\"\\n✅ Inverse‐transformed head (should mirror your original X_train):\")\n",
    "    print(df_back.head())\n",
    "    print(f\"Shape: {df_back.shape} → original X_train shape before transform: {orig_shape}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    # === Hyperparameter tuning ===\n",
    "    best_params = tune_gbm(X_train, y_train, n_trials=5)\n",
    "    print(\"Tuned params:\", best_params)\n",
    "\n",
    "    # === Train & evaluate ===\n",
    "    gbm_model, rmse = fit_gbm(\n",
    "        X_train, y_train, X_test, y_test, **best_params\n",
    "    )\n",
    "    print(f\"Tuned XGBoost RMSE: {rmse:.4f}\")\n",
    "\n",
    "\n",
    "    # 3) save both at once\n",
    "    save_pipeline(gbm_model, tf, path=\"models/gbm_pipeline.joblib\")\n",
    "    \n",
    "    model, preprocessor = load_pipeline(\"models/gbm_pipeline.joblib\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows with NaN in exit_velo\n",
      "season               0\n",
      "level_abbr           0\n",
      "batter_id            0\n",
      "pitcher_id           0\n",
      "exit_velo        97979\n",
      "launch_angle      9130\n",
      "spray_angle       6227\n",
      "hangtime         75968\n",
      "hit_type           201\n",
      "batter_hand          0\n",
      "pitcher_hand         0\n",
      "batter_height        0\n",
      "pitch_group          0\n",
      "outcome              0\n",
      "age                  0\n",
      "dtype: int64\n",
      "after rows dropped with NaN in exit_velo\n",
      "season               0\n",
      "level_abbr           0\n",
      "batter_id            0\n",
      "pitcher_id           0\n",
      "exit_velo            0\n",
      "launch_angle         0\n",
      "spray_angle          0\n",
      "hangtime         12429\n",
      "hit_type             0\n",
      "batter_hand          0\n",
      "pitcher_hand         0\n",
      "batter_height        0\n",
      "pitch_group          0\n",
      "outcome              0\n",
      "age                  0\n",
      "dtype: int64\n",
      "⚠️  Nulls after target‑filter:\n",
      "  • hangtime         12,429 (1.00%)\n",
      "Mixed-effects model RMSE: 13.2382\n",
      "Mixed-effects model summary:\n",
      "            Mixed Linear Model Regression Results\n",
      "===========================================================\n",
      "Model:            MixedLM Dependent Variable: exit_velo    \n",
      "No. Observations: 923599  Method:             ML           \n",
      "No. Groups:       2659    Scale:              169.9808     \n",
      "Min. group size:  7       Log-Likelihood:     -3684852.9342\n",
      "Max. group size:  1760    Converged:          Yes          \n",
      "Mean group size:  347.3                                    \n",
      "-----------------------------------------------------------\n",
      "               Coef.  Std.Err.    z     P>|z| [0.025 0.975]\n",
      "-----------------------------------------------------------\n",
      "Intercept      89.049    0.055 1633.633 0.000 88.942 89.155\n",
      "level_ord      -0.277    0.031   -8.907 0.000 -0.337 -0.216\n",
      "age_centered   -0.012    0.010   -1.231 0.218 -0.031  0.007\n",
      "Group Var       4.903    0.013                             \n",
      "===========================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/models/mixed.py\n",
    "\n",
    "\"\"\"\n",
    "Frequentist mixed‑effects model using statsmodels MixedLM.\n",
    "\n",
    "Formula implemented:\n",
    "    exit_velo ~ 1 + level_ord + age_centered\n",
    "              + (1 | batter_id)\n",
    "\n",
    "We rely on columns already produced by preprocess():\n",
    "    • level_idx  (0,1,2)   – ordinal\n",
    "    • age_centered\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "def fit_mixed(train: pd.DataFrame,\n",
    "              test: pd.DataFrame):\n",
    "    \"\"\"Return (fitted model, RMSE on test).\"\"\"\n",
    "    # statsmodels wants a *single* DataFrame with all cols\n",
    "    # so we concatenates and keep row positions for slicing\n",
    "    combined = pd.concat([train, test], axis=0)\n",
    "    # ensure categorical dtype\n",
    "    combined[\"level_ord\"] = combined[\"level_idx\"].astype(int)\n",
    "\n",
    "    mdl = smf.mixedlm(\n",
    "        formula=\"exit_velo ~ 1 + level_ord + age_centered\",\n",
    "        data=combined.iloc[: len(train)],\n",
    "        groups=combined.iloc[: len(train)][\"batter_id\"]\n",
    "    ).fit(reml=False)\n",
    "\n",
    "    # predict on test set\n",
    "    pred = mdl.predict(exog=combined.iloc[len(train):])\n",
    "    true = test[\"exit_velo\"].values\n",
    "    rmse = np.sqrt(np.mean((pred - true) ** 2))\n",
    "    return mdl, rmse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.features.preprocess import prepare_for_mixed_and_hierarchical\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    # Prepare and split\n",
    "    df_model = prepare_for_mixed_and_hierarchical(df_fe)\n",
    "    train_df, test_df = train_test_split(df_model, test_size=0.2, random_state=42)\n",
    " \n",
    "    # Fit mixed-effects\n",
    "    mixed_model, rmse_mixed = fit_mixed(train_df, test_df)\n",
    "    print(f\"Mixed-effects model RMSE: {rmse_mixed:.4f}\")\n",
    "\n",
    "    # In the smoke test section: P-Value Checks for Mixed-Effects Models\n",
    "    print(\"Mixed-effects model summary:\\n\", mixed_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/bayesian_explainability.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/bayesian_explainability.py\n",
    "import arviz as az\n",
    "import shap, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- Posterior summaries -----------------\n",
    "def plot_parameter_forest(idata, var_names=None, hdi_prob=0.95):\n",
    "    \"\"\"Caterpillar/forest plot of posterior estimates.\"\"\"\n",
    "    return az.plot_forest(\n",
    "        idata,\n",
    "        var_names=var_names,\n",
    "        combined=True,\n",
    "        hdi_prob=hdi_prob,\n",
    "        kind=\"forest\",\n",
    "        figsize=(6, len(var_names or idata.posterior.data_vars) * 0.4),\n",
    "    )\n",
    "\n",
    "def posterior_table(idata, round_to=2):\n",
    "    \"\"\"\n",
    "    Return a nicely rounded HDI/mean table with significance.\n",
    "    \"\"\"\n",
    "    summary = az.summary(idata, hdi_prob=0.95).round(round_to)\n",
    "    summary[\"significant\"] = (summary[\"hdi_2.5%\"] > 0) | (summary[\"hdi_97.5%\"] < 0)\n",
    "    return summary\n",
    "\n",
    "# ---------------- Posterior‑predictive checks ---------\n",
    "def plot_ppc(idata, kind=\"overlay\"):\n",
    "    \"\"\"Visual PPC (over‑laid densities by default).\"\"\"\n",
    "    return az.plot_ppc(idata, kind=kind, alpha=0.1)\n",
    "\n",
    "# ---------------- SHAP-based feature importances ------\n",
    "def shap_explain(predict_fn, background_df, sample_df):\n",
    "    \"\"\"\n",
    "    Model‑agnostic Kernel SHAP on the *posterior mean predictor*.\n",
    "\n",
    "    predict_fn(df) must return a 1‑D numpy array of predictions.\n",
    "    \"\"\"\n",
    "    explainer = shap.KernelExplainer(predict_fn, background_df)\n",
    "    shap_values = explainer.shap_values(sample_df, nsamples=200)\n",
    "    shap.summary_plot(shap_values, sample_df, show=False)\n",
    "    plt.tight_layout()\n",
    "    return shap_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/bayesian_metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/bayesian_metrics.py\n",
    "import arviz as az\n",
    "from sklearn.metrics import mean_squared_error,root_mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_classical_metrics(idata, y_true):\n",
    "    \"\"\"Compute MSE, RMSE, MAE & R² from the posterior predictive distribution.\"\"\"\n",
    "    # Extract posterior predictive draws and compute mean prediction\n",
    "    y_ppc = (\n",
    "        idata.posterior_predictive['y_obs']\n",
    "        .stack(samples=('chain', 'draw'))\n",
    "        .values\n",
    "    )\n",
    "    y_pred = y_ppc.mean(axis=1)\n",
    "\n",
    "    # Classical regression metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"▶ Classical MSE : {mse:.2f}\")\n",
    "    print(f\"▶ Classical RMSE: {rmse:.2f}\")\n",
    "    print(f\"▶ Classical MAE : {mae:.2f}\")\n",
    "    print(f\"▶ Classical R²  : {r2:.3f}\")\n",
    "\n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _chunked_loo_waic(idata, chunk_size):\n",
    "    \"\"\"\n",
    "    Internal helper: compute pointwise LOO/WAIC in chunks of observations.\n",
    "    \"\"\"\n",
    "    # Extract raw log-likelihood array [chains, draws, obs]\n",
    "    ll = idata.log_likelihood[\"y_obs\"].values\n",
    "    n_obs = ll.shape[-1]\n",
    "\n",
    "    elpd_loo_i   = np.empty(n_obs, dtype=float)\n",
    "    p_loo_i      = np.empty(n_obs, dtype=float)\n",
    "    pareto_k_all = []\n",
    "    elpd_waic_i  = np.empty(n_obs, dtype=float)\n",
    "    p_waic_i     = np.empty(n_obs, dtype=float)\n",
    "\n",
    "    for start in range(0, n_obs, chunk_size):\n",
    "        end = min(start + chunk_size, n_obs)\n",
    "        subset = idata.isel(observed_data={\"obs\": slice(start, end)})\n",
    "\n",
    "        loo_sub  = az.loo(subset,  pointwise=True)\n",
    "        waic_sub = az.waic(subset, pointwise=True)\n",
    "\n",
    "        elpd_loo_i [start:end] = loo_sub[\"elpd_loo\"].values\n",
    "        p_loo_i    [start:end] = loo_sub[\"p_loo\"].values\n",
    "        pareto_k_all.extend(loo_sub[\"pareto_k\"].values.tolist())\n",
    "\n",
    "        elpd_waic_i[start:end] = waic_sub[\"elpd_waic\"].values\n",
    "        p_waic_i   [start:end] = waic_sub[\"p_waic\"].values\n",
    "\n",
    "    loo = az.ELPDData(\n",
    "        {\"elpd_loo\": (\"obs\", elpd_loo_i),\n",
    "         \"p_loo\":    (\"obs\", p_loo_i),\n",
    "         \"pareto_k\": (\"obs\", np.array(pareto_k_all))},\n",
    "        coords={\"obs\": np.arange(n_obs)},\n",
    "        dims   ={\"elpd_loo\": [\"obs\"], \"p_loo\": [\"obs\"], \"pareto_k\": [\"obs\"]}\n",
    "    )\n",
    "    waic = az.ELPDData(\n",
    "        {\"elpd_waic\": (\"obs\", elpd_waic_i),\n",
    "         \"p_waic\":    (\"obs\", p_waic_i)},\n",
    "        coords={\"obs\": np.arange(n_obs)},\n",
    "        dims   ={\"elpd_waic\": [\"obs\"], \"p_waic\": [\"obs\"]}\n",
    "    )\n",
    "    return loo, waic\n",
    "\n",
    "\n",
    "def compute_bayesian_metrics(idata, *, use_chunks: bool = False, chunk_size: int | None = None):\n",
    "    \"\"\"Compute PSIS-LOO, WAIC and Pareto k diagnostics for model comparison.\"\"\"\n",
    "    # 1) Guard missing log_likelihood\n",
    "    if 'log_likelihood' not in idata.groups():\n",
    "        print(\"⚠️ InferenceData has no log_likelihood; skipping LOO/WAIC.\")\n",
    "        return None\n",
    "\n",
    "    # Validate chunk parameters\n",
    "    if use_chunks:\n",
    "        if chunk_size is None or chunk_size <= 0:\n",
    "            raise ValueError(\"When use_chunks=True, chunk_size must be a positive integer\")\n",
    "\n",
    "    # 2) Compute LOO & WAIC (batch or chunked)\n",
    "    try:\n",
    "        if not use_chunks:\n",
    "            loo  = az.loo(idata,  pointwise=True)\n",
    "            waic = az.waic(idata, pointwise=True)\n",
    "        else:\n",
    "            loo, waic = _chunked_loo_waic(idata, chunk_size)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ LOO/WAIC computation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 3) Extract scalars\n",
    "    looic    = -2 * loo[\"elpd_loo\"].sum()\n",
    "    p_loo    = loo[\"p_loo\"].sum()\n",
    "    waic_val = -2 * waic[\"elpd_waic\"].sum()\n",
    "    p_waic   = waic[\"p_waic\"].sum()\n",
    "    prop_bad_k = np.mean(loo[\"pareto_k\"].values > 0.7)\n",
    "\n",
    "    # 4) Print results\n",
    "    print(f\"▶ LOOIC       : {looic:.1f}, p_loo: {p_loo:.1f}\")\n",
    "    print(f\"▶ WAIC        : {waic_val:.1f}, p_waic: {p_waic:.1f}\")\n",
    "    print(f\"▶ Pareto k>0.7: {prop_bad_k:.2%} of observations\")\n",
    "\n",
    "    return {'loo': loo, 'waic': waic, 'prop_bad_k': prop_bad_k}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_convergence_diagnostics(idata):\n",
    "    \"\"\"Print R̂ and ESS bulk/tail for key parameters.\"\"\"\n",
    "    if 'posterior' not in idata.groups():\n",
    "        print(\"No posterior group in InferenceData; skipping convergence diagnostics.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        summary = az.summary(\n",
    "            idata,\n",
    "            var_names=[\"mu\", \"beta\", \"sigma_e\"],\n",
    "            kind=\"diagnostics\",\n",
    "            round_to=2\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        print(f\"Convergence diagnostics skipped: missing vars {e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"▶ Convergence diagnostics (R̂, ESS):\")\n",
    "    print(summary[[\"r_hat\", \"ess_bulk\", \"ess_tail\"]])\n",
    "    return summary\n",
    "\n",
    "\n",
    "def compute_calibration(idata, y_true, hdi_prob=0.95):\n",
    "    \"\"\"\n",
    "    Compute calibration: the fraction of true observations that lie within the posterior predictive HDI.\n",
    "    \"\"\"\n",
    "    # Extract posterior predictive draws\n",
    "    y_ppc = (\n",
    "        idata.posterior_predictive['y_obs']\n",
    "        .stack(samples=('chain','draw'))\n",
    "        .values\n",
    "    )\n",
    "    # Compute lower/upper quantiles per observation\n",
    "    lower = np.percentile(y_ppc, (1-hdi_prob)/2*100, axis=1)\n",
    "    upper = np.percentile(y_ppc, (1+(hdi_prob))/2*100, axis=1)\n",
    "    within = ((y_true >= lower) & (y_true <= upper)).mean()\n",
    "    print(f\"▶ Calibration: {within:.2%} of true values within {int(hdi_prob*100)}% HDI\")\n",
    "    return within\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    import arviz as az\n",
    "\n",
    "    # 1) True values\n",
    "    np.random.seed(42)\n",
    "    y_true = np.random.normal(0, 1, size=10)\n",
    "\n",
    "    # 2) Posterior predictive draws (2 chains × 5 draws × 10 obs)\n",
    "    y_obs = np.random.normal(loc=y_true, scale=0.5, size=(2,5,10))\n",
    "\n",
    "    # 3) Posterior parameters μ, β, σₑ (2 chains × 5 draws)\n",
    "    posterior = {\n",
    "        'mu':      np.random.normal(0, 1, size=(2,5)),\n",
    "        'beta':    np.random.normal(0, 1, size=(2,5)),\n",
    "        'sigma_e': np.abs(np.random.normal(1, 0.2, size=(2,5))),\n",
    "    }\n",
    "\n",
    "    # 4) Log‑likelihood (for each chain, draw, obs)\n",
    "    log_lik = np.random.normal(-1, 0.5, size=(2,5,10))\n",
    "\n",
    "    # 5) Build InferenceData\n",
    "    idata = az.from_dict(\n",
    "        posterior=posterior,\n",
    "        posterior_predictive={'y_obs': y_obs},\n",
    "        log_likelihood     ={'y_obs': log_lik},\n",
    "        dims={\n",
    "            'y_obs': ['chain','draw','obs'],\n",
    "            'mu':    ['chain','draw'],\n",
    "            'beta':  ['chain','draw'],\n",
    "            'sigma_e':['chain','draw'],\n",
    "        },\n",
    "        coords={\n",
    "            'chain': [0,1],\n",
    "            'draw':  list(range(5)),\n",
    "            'obs':   list(range(10))\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"=== Classical Metrics ===\")\n",
    "    compute_classical_metrics(idata, y_true)\n",
    "\n",
    "    print(\"\\n=== Calibration ===\")\n",
    "    compute_calibration(idata, y_true)\n",
    "\n",
    "    print(\"\\n=== Bayesian Metrics ===\")\n",
    "    compute_bayesian_metrics(idata)\n",
    "\n",
    "    print(\"\\n=== Convergence Diagnostics ===\")\n",
    "    compute_convergence_diagnostics(idata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/hierarchical_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/hierarchical_utils.py\n",
    "\n",
    "import arviz as az\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(idata, file_path: str, overwrite: bool = True):\n",
    "    \"\"\"Save ArviZ InferenceData to NetCDF.\"\"\"\n",
    "    idata.to_netcdf(file_path, engine=\"h5netcdf\", overwrite_existing=overwrite)\n",
    "    print(f\"✔︎ saved model → {file_path}\")\n",
    "\n",
    "def load_model(file_path: str):\n",
    "    \"\"\"Load ArviZ InferenceData from NetCDF.\"\"\"\n",
    "    idata = az.from_netcdf(file_path, engine=\"h5netcdf\")\n",
    "    print(f\"✔︎ loaded model ← {file_path}\")\n",
    "    return idata\n",
    "\n",
    "def save_preprocessor(transformer, file_path: str):\n",
    "    \"\"\"Save the scikit-learn transformer to a joblib file.\"\"\"\n",
    "    joblib.dump(transformer, file_path)\n",
    "    print(f\"✔︎ saved preprocessor → {file_path}\")\n",
    "\n",
    "def load_preprocessor(file_path: str):\n",
    "    \"\"\"Load the scikit-learn transformer from a joblib file.\"\"\"\n",
    "    transformer = joblib.load(file_path)\n",
    "    print(f\"✔︎ loaded preprocessor ← {file_path}\")\n",
    "    return transformer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # === Editable settings ===\n",
    "    # Path to the saved model (NetCDF format)\n",
    "    MODEL_PATH = \"data/models/saved_models/model.nc\"\n",
    "    # Path to the preprocessor\n",
    "    PREPROC_PATH = \"data/models/saved_models/preprocessor.joblib\"\n",
    "    # Input data for prediction (raw CSV with exit velocity data)\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    # Output predictions file (CSV) or set to None to print to console\n",
    "    OUTPUT_PREDS_2024 = \"data/predictions/predictions_2024.csv\"  # <-- EDITABLE: set output CSV path or None\n",
    "    \n",
    "    model = load_model(MODEL_PATH)\n",
    "    save_model(model, MODEL_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/posterior.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/posterior.py\n",
    "# src/utils/posterior.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "\n",
    "# ── REPLACEMENT: paste over the whole function ─────────────────────────\n",
    "import json, pathlib, numpy as np, pandas as pd, arviz as az\n",
    "\n",
    "JSON_GLOBAL = pathlib.Path(\"data/models/saved_models/global_effects.json\")\n",
    "\n",
    "def posterior_to_frame(idata: az.InferenceData) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a batter‑level summary **AND** writes global effects to JSON.\n",
    "\n",
    "    File written  ➜  data/models/saved_models/global_effects.json\n",
    "    \"\"\"\n",
    "    post = idata.posterior\n",
    "\n",
    "    # ------- per‑batter u summaries -----------------------------------\n",
    "    u   = post[\"u\"]                                         # (chain,draw,batter)\n",
    "    stats = {\n",
    "        \"u_mean\"  : u.mean((\"chain\",\"draw\")).values,\n",
    "        \"u_sd\"    : u.std ((\"chain\",\"draw\")).values,\n",
    "        \"u_q2.5\"  : np.percentile(u.values,  2.5, axis=(0,1)),\n",
    "        \"u_q50\"   : np.percentile(u.values, 50.0, axis=(0,1)),\n",
    "        \"u_q97.5\" : np.percentile(u.values,97.5, axis=(0,1)),\n",
    "    }\n",
    "    df = pd.DataFrame({\"batter_idx\": np.arange(u.shape[-1]), **stats})\n",
    "\n",
    "    # ------- global effects ------------------------------------------\n",
    "    mu_mean = post[\"mu\"].mean().item()\n",
    "\n",
    "    # β_age  ➜ last entry of beta vector (age_centered was added last)\n",
    "    beta  = post[\"beta\"]\n",
    "    feat_dim = [d for d in beta.dims if d not in (\"chain\",\"draw\")][0]\n",
    "    beta_age = beta.isel({feat_dim: -1}).mean().item()\n",
    "\n",
    "    beta_level = post[\"beta_level\"].mean((\"chain\",\"draw\")).values.tolist()\n",
    "    sigma_b    = post[\"sigma_b\"].mean().item()\n",
    "    sigma_e    = post[\"sigma_e\"].mean().item()\n",
    "\n",
    "    global_eff = dict(\n",
    "        mu_mean=mu_mean,\n",
    "        beta_age=beta_age,\n",
    "        beta_level=beta_level,\n",
    "        sigma_b=sigma_b,\n",
    "        sigma_e=sigma_e,\n",
    "        median_age=idata.attrs.get(\"median_age\", 26.0),\n",
    "        beta=post[\"beta\"].mean((\"chain\",\"draw\")).values.tolist(),  # Save all beta coefficients\n",
    "        feature_names=idata.attrs.get(\"feature_names\", [])  # Also save feature names if available\n",
    "    )\n",
    "\n",
    "    # ➜  write side‑car JSON (overwrite every run)\n",
    "    JSON_GLOBAL.write_text(json.dumps(global_eff, indent=2))\n",
    "    print(f\"✔︎ wrote global effects → {JSON_GLOBAL}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def align_batter_codes(df_roster: pd.DataFrame,\n",
    "                       train_cats: pd.Index) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert integer batter_ids in *roster* into the categorical codes\n",
    "    **identical** to what the model saw during training.\n",
    "\n",
    "    Any unseen batter gets code = -1 (handled later).\n",
    "    \"\"\"\n",
    "    cat = pd.Categorical(df_roster[\"batter_id\"], categories=train_cats)\n",
    "    return pd.Series(cat.codes, index=df_roster.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/validation.py\n",
    "\"\"\"\n",
    "Generic K‑fold validator.\n",
    "\n",
    "• Works for sklearn Pipelines *or* PyMC idata.\n",
    "• Decides how to extract predictions based on\n",
    "  the object returned by `fit_func`.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from arviz import InferenceData\n",
    "import arviz as az\n",
    "from typing import Callable, List, Union\n",
    "import statsmodels as sm\n",
    "\n",
    "\n",
    "\n",
    "def _rmse(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return np.sqrt(np.mean((a - b) ** 2))\n",
    "\n",
    "\n",
    "def run_kfold_cv(\n",
    "    fit_func: Callable[[pd.DataFrame, pd.DataFrame], tuple],\n",
    "    df: pd.DataFrame,\n",
    "    k: int = 5,\n",
    "    random_state: int = 0,\n",
    "    **fit_kw\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Sklearn-style K-fold CV:\n",
    "      fit_func(train_df, test_df, **fit_kw) -> (model, rmse)\n",
    "    Returns a list of rmse scores.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    rmses: List[float] = []\n",
    "    for train_idx, test_idx in kf.split(df):\n",
    "        train, test = df.iloc[train_idx], df.iloc[test_idx]\n",
    "        _, rmse = fit_func(train, test, **fit_kw)\n",
    "        rmses.append(rmse)\n",
    "    return rmses\n",
    "\n",
    "\n",
    "\n",
    "def run_kfold_cv_pymc(\n",
    "    fit_func: Callable[[pd.DataFrame], InferenceData],\n",
    "    df: pd.DataFrame,\n",
    "    k: int = 5,\n",
    "    random_state: int = 0\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    PyMC K-fold CV: \n",
    "      fit_func(train_df) -> ArviZ InferenceData with posterior a[group] & sigma.\n",
    "    Returns RMSE on each hold-out fold.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    rmses: list[float] = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(df):\n",
    "        train_df, test_df = df.iloc[train_idx], df.iloc[test_idx]\n",
    "        idata = fit_func(train_df)\n",
    "\n",
    "        # 1) grab posterior samples of the group intercepts and noise σ\n",
    "        #    stack chain+draw into one dim of length M\n",
    "        a_samples = (\n",
    "            idata.posterior[\"a\"]\n",
    "            .stack(sample=(\"chain\", \"draw\"))    # dims now (\"group\",\"sample\")\n",
    "            .transpose(\"sample\", \"group\")       # shape (M, n_groups)\n",
    "            .values\n",
    "        )\n",
    "        sigma_samples = (\n",
    "            idata.posterior[\"sigma\"]\n",
    "            .stack(sample=(\"chain\", \"draw\"))    # shape (M,)\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        # 2) for each test row, pull the intercept for its group\n",
    "        groups = test_df[\"group\"].values       # shape (n_test,)\n",
    "        # a_samples[:, groups] → shape (M, n_test)\n",
    "        pred_samples = a_samples[:, groups]\n",
    "\n",
    "        # 3) point estimate = posterior mean for each test row\n",
    "        pred_mean = pred_samples.mean(axis=0)  # shape (n_test,)\n",
    "\n",
    "        # 4) compute RMSE against true exit_velo\n",
    "        true_vals = test_df[\"exit_velo\"].values\n",
    "        rmse = np.sqrt(np.mean((pred_mean - true_vals) ** 2))\n",
    "        rmses.append(rmse)\n",
    "\n",
    "    return rmses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# helper to score a *single* train/test split for idata\n",
    "def rmse_pymc(idata: InferenceData, test_df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Compute RMSE by comparing posterior-mean predictions\n",
    "    (using the group intercept `a`) against observed `exit_velo`.\n",
    "    \"\"\"\n",
    "    # 1) Stack (chain, draw) into a single \"sample\" dimension\n",
    "    a_samples = (\n",
    "        idata.posterior[\"a\"]\n",
    "        .stack(sample=(\"chain\", \"draw\"))    # dims → (\"group\",\"sample\")\n",
    "        .transpose(\"sample\", \"group\")       # shape → (M, n_groups)\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # 2) Gather the sampled intercepts for each test row\n",
    "    groups = test_df[\"group\"].values       # shape → (n_test,)\n",
    "    pred_samples = a_samples[:, groups]    # shape → (M, n_test)\n",
    "\n",
    "    # 3) Posterior mean prediction for each test instance\n",
    "    pred_mean = pred_samples.mean(axis=0)  # shape → (n_test,)\n",
    "\n",
    "    # 4) Compute and return RMSE versus the true values\n",
    "    true_vals = test_df[\"exit_velo\"].values\n",
    "    return np.sqrt(np.mean((pred_mean - true_vals) ** 2))\n",
    "\n",
    "\n",
    "def posterior_predictive_check(idata, df, batter_idx):\n",
    "    \"\"\"\n",
    "    Plot observed vs. simulated exit-velo histograms.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    obs = df[\"exit_velo\"].values\n",
    "    sim = (\n",
    "        idata.posterior_predictive[\"y_obs\"]\n",
    "        .stack(samples=(\"chain\", \"draw\"))\n",
    "        .values.flatten()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    ax[0].hist(obs, bins=30)\n",
    "    ax[0].set_title(\"Observed\")\n",
    "    ax[1].hist(sim, bins=30)\n",
    "    ax[1].set_title(\"Simulated\")\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prediction_interval(model, X, alpha=0.05, method='linear'):\n",
    "    \"\"\"\n",
    "    Compute prediction intervals for a model.\n",
    "    \"\"\"\n",
    "    if method == 'linear':\n",
    "        # For OLS and Ridge\n",
    "        X_const = sm.add_constant(X)\n",
    "        preds = model.get_prediction(X_const)\n",
    "        pred_int = preds.conf_int(alpha=alpha)\n",
    "        return preds.predicted_mean, pred_int[:, 0], pred_int[:, 1]\n",
    "    elif method == 'bayesian':\n",
    "        # For Bayesian models\n",
    "        hdi = az.hdi(model, hdi=1 - alpha)\n",
    "        return (\n",
    "            hdi.posterior_predictive.y_obs.sel(hdi=f\"{alpha/2*100}%\"),\n",
    "            hdi.posterior_predictive.y_obs.sel(hdi=f\"{(1-alpha/2)*100}%\")\n",
    "        )\n",
    "    elif method == 'gbm':\n",
    "        # For XGBoost quantile regression\n",
    "        lower = model.predict(X, pred_contribs=False, iteration_range=(0, model.best_iteration))\n",
    "        upper = model.predict(X, pred_contribs=False, iteration_range=(0, model.best_iteration))\n",
    "        return lower, upper  # Replace with actual quantile regression\n",
    "    else:\n",
    "        raise ValueError(\"Method not supported\")\n",
    "\n",
    "# Example for bootstrapping GBM\n",
    "def bootstrap_prediction_interval(model, X, n_bootstraps=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute nonparametric bootstrap confidence intervals for predictions.\n",
    "\n",
    "    Must support either:\n",
    "      - model.predict(X) -> array_like shape (n_obs,)\n",
    "      - model.get_prediction(X).predicted_mean -> array_like shape (n_obs,)\n",
    "\n",
    "    Returns lower, upper arrays of shape (n_obs,) at the given alpha level.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Determine number of observations\n",
    "    try:\n",
    "        n_obs = X.shape[0]\n",
    "    except Exception:\n",
    "        raise ValueError(\"X must have a valid `.shape[0]`\")\n",
    "\n",
    "    # Allocate storage\n",
    "    all_preds = np.zeros((n_bootstraps, n_obs))\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # Sample indices with replacement\n",
    "        idx = np.random.randint(0, n_obs, size=n_obs)\n",
    "        sampled = X.iloc[idx] if hasattr(X, \"iloc\") else X[idx]\n",
    "\n",
    "        # Dispatch to correct predict method\n",
    "        if hasattr(model, \"predict\"):\n",
    "            preds_i = model.predict(sampled)\n",
    "        elif hasattr(model, \"get_prediction\"):\n",
    "            result = model.get_prediction(sampled)\n",
    "            preds_i = getattr(result, \"predicted_mean\", None)\n",
    "            if preds_i is None:\n",
    "                raise AttributeError(\n",
    "                    \"get_prediction(...) did not return `predicted_mean`\"\n",
    "                )\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                f\"Model {model!r} has no `predict` or `get_prediction` method\"\n",
    "            )\n",
    "\n",
    "        all_preds[i, :] = preds_i\n",
    "\n",
    "    # Compute CI percentiles\n",
    "    lower = np.percentile(all_preds, 100 * (alpha / 2), axis=0)\n",
    "    upper = np.percentile(all_preds, 100 * (1 - alpha / 2), axis=0)\n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import arviz as az\n",
    "    import statsmodels.api as sm\n",
    "    import pymc as pm\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # from src.utils.validation import (\n",
    "    # run_kfold_cv_pymc, prediction_interval, \n",
    "    # bootstrap_prediction_interval, rmse_pymc, \n",
    "    # posterior_predictive_check\n",
    "    # )\n",
    "\n",
    "    # 1) Synthetic data for sklearn\n",
    "    np.random.seed(42)\n",
    "    X = np.linspace(0, 10, 40)\n",
    "    y = 2 * X + 1 + np.random.normal(0, 1, size=X.shape)\n",
    "    df_skl = pd.DataFrame({\"feature\": X, \"exit_velo\": y})\n",
    "\n",
    "    def fit_sklearn(train, test):\n",
    "        X_tr, y_tr = train[[\"feature\"]], train[\"exit_velo\"]\n",
    "        X_te, y_te = test[[\"feature\"]], test[\"exit_velo\"]\n",
    "        m = LinearRegression().fit(X_tr, y_tr)\n",
    "        preds = m.predict(X_te)\n",
    "        rmse = np.sqrt(np.mean((preds - y_te) ** 2))\n",
    "        return m, rmse\n",
    "\n",
    "    print(\"\\n--- sklearn CV ---\")\n",
    "    print(\"RMSEs:\", run_kfold_cv(fit_sklearn, df_skl, k=4))\n",
    "\n",
    "    # 2) Synthetic hierarchical data for PyMC\n",
    "    #    (3 groups, group‐level intercepts)\n",
    "    n_per = 20\n",
    "    groups = np.repeat(np.arange(3), n_per)\n",
    "    true_alpha = np.array([1.0, 3.0, 5.0])\n",
    "    y_hier = true_alpha[groups] + np.random.normal(0, 1, size=groups.size)\n",
    "    df_hier = pd.DataFrame({\"exit_velo\": y_hier, \"group\": groups})\n",
    "\n",
    "    def fit_hierarchical(train_df: pd.DataFrame) -> az.InferenceData:\n",
    "        \"\"\"\n",
    "        Fit a hierarchical model and append posterior_predictive samples for y_obs.\n",
    "        Returns:\n",
    "            InferenceData with .posterior and .posterior_predictive[\"y_obs\"].\n",
    "        \"\"\"\n",
    "        coords = {\"group\": np.unique(train_df[\"group\"])}\n",
    "        with pm.Model(coords=coords) as model:\n",
    "            mu_a = pm.Normal(\"mu_a\", 0, 5)\n",
    "            sigma_a = pm.HalfNormal(\"sigma_a\", 5)\n",
    "            a = pm.Normal(\"a\", mu=mu_a, sigma=sigma_a, dims=\"group\")\n",
    "            sigma = pm.HalfNormal(\"sigma\", 1)\n",
    "            y_grp = a[train_df[\"group\"].values]\n",
    "            pm.Normal(\"y_obs\", mu=y_grp, sigma=sigma,\n",
    "                    observed=train_df[\"exit_velo\"].values)\n",
    "\n",
    "            # 1) Draw posterior samples\n",
    "            idata = pm.sample(\n",
    "                draws=500, tune=500,\n",
    "                chains=2, cores=1,\n",
    "                return_inferencedata=True,\n",
    "                progressbar=False\n",
    "            )\n",
    "\n",
    "            # 2) Generate posterior_predictive samples for 'y_obs'\n",
    "            pm.sample_posterior_predictive(\n",
    "                idata,\n",
    "                model=model,\n",
    "                var_names=[\"y_obs\"],\n",
    "                extend_inferencedata=True,\n",
    "                random_seed=42,\n",
    "                progressbar=False\n",
    "            )\n",
    "\n",
    "        # 3) Return the enriched InferenceData\n",
    "        return idata\n",
    "\n",
    "\n",
    "    print(\"\\n--- PyMC hierarchical CV ---\")\n",
    "    bayes_rmse = run_kfold_cv_pymc(fit_hierarchical, df_hier, k=3)\n",
    "    print(\"Bayesian RMSEs:\", bayes_rmse)\n",
    "\n",
    "    # 3) Test prediction_interval (linear OLS)\n",
    "    df_ols = df_skl.copy()\n",
    "    df_ols[\"const\"] = 1.0\n",
    "    ols = sm.OLS(df_ols[\"exit_velo\"], df_ols[[\"const\", \"feature\"]]).fit()\n",
    "    mean, lo, hi = prediction_interval(ols, df_skl[[\"feature\"]], method=\"linear\")\n",
    "    print(\"\\n--- OLS prediction_interval shapes ---\")\n",
    "    print(\"mean:\", mean.shape, \"lower:\", lo.shape, \"upper:\", hi.shape)\n",
    "\n",
    "    # 4) Test bootstrap_prediction_interval with LinearRegression\n",
    "    lr = LinearRegression().fit(df_skl[[\"feature\"]], df_skl[\"exit_velo\"])\n",
    "    lower, upper = bootstrap_prediction_interval(lr, df_skl[[\"feature\"]], n_bootstraps=200)\n",
    "    print(\"\\n--- bootstrap_prediction_interval shapes ---\")\n",
    "    print(\"lower:\", lower.shape, \"upper:\", upper.shape)\n",
    "\n",
    "    # 5) rmse_pymc + posterior_predictive_check on a single InferenceData\n",
    "    #    (we can re-use the last fold of the hierarchical example)\n",
    "    last_idata = fit_hierarchical(df_hier)\n",
    "    rmse_val = rmse_pymc(last_idata, df_hier)\n",
    "    print(\"\\n--- rmse_pymc on full data ---\", rmse_val)\n",
    "\n",
    "    # 6) posterior_predictive_check plot\n",
    "    print(\"\\n--- posterior_predictive_check (figure) ---\")\n",
    "    fig = posterior_predictive_check(last_idata, df_hier, batter_idx=None)\n",
    "    fig.savefig(\"ppc_hist.png\")\n",
    "    print(\"Saved histogram to ppc_hist.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/jax_gpu_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/jax_gpu_utils.py\n",
    "\n",
    "import os, subprocess, json, logging, platform, jax\n",
    "from jax.lib import xla_bridge\n",
    "\n",
    "def gpu_diagnostics():\n",
    "    info = {\n",
    "        \"backend\":      xla_bridge.get_backend().platform,\n",
    "        \"devices\":      [str(d) for d in jax.devices()],\n",
    "        \"python\":       platform.python_version(),\n",
    "        \"ld_library_path\": os.getenv(\"LD_LIBRARY_PATH\",\"<unset>\"),\n",
    "    }\n",
    "    if shutil.which(\"nvidia-smi\"):\n",
    "        try:\n",
    "            smi = subprocess.check_output(\n",
    "                [\"nvidia-smi\", \"--query-gpu=name,driver_version,memory.total\",\n",
    "                 \"--format=csv,noheader,nounits\"]\n",
    "            )\n",
    "            info[\"nvidia-smi\"] = smi.decode().strip()\n",
    "        except Exception as exc:\n",
    "            info[\"nvidia-smi-error\"] = repr(exc)\n",
    "    return info\n",
    "\n",
    "def log_gpu_diagnostics(level=logging.INFO):\n",
    "    logging.getLogger(__name__).log(\n",
    "        level,\n",
    "        \"GPU-diag: %s\",\n",
    "        json.dumps(gpu_diagnostics(), indent=2)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [alpha, beta, sigma]\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "Sampling 4 chains for 500 tune and 500 draw iterations (2_000 + 2_000 draws total) took 1 seconds.\n",
      "Sampling: [y]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64a738799c4843eb90cdd261a50214bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished [100%]: Average Loss = 228.02\n",
      "03:57:24 - cmdstanpy - INFO - compiling stan file /tmp/tmpppht171a.stan to exe file /tmp/tmpppht171a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit_bayesian_pymc_advi] done in 0.82s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:57:38 - cmdstanpy - INFO - compiled model executable: /tmp/tmpppht171a\n",
      "03:57:38 - cmdstanpy - INFO - CmdStan start processing\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe8317ef1c2541f986e7ad5689623c56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chain 1 |          | 00:00 Status"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7e434ba645a47a696799a63524e0813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chain 2 |          | 00:00 Status"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a7c356d3d4147b5a9016635185ca981",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chain 3 |          | 00:00 Status"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b04e8387d8d4a4087589f23ad635b33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "chain 4 |          | 00:00 Status"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                                                                                                                                                                                                "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "03:57:38 - cmdstanpy - INFO - CmdStan done processing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[fit_bayesian_cmdstanpy] done in 11.06s\n",
      "💾 JAGS model written to /tmp/tmp5ddoykwu.bug\n",
      "adapting: iterations 2000 of 2000, elapsed 0:00:00, remaining 0:00:00\n",
      "sampling: iterations 2000 of 2000, elapsed 0:00:00, remaining 0:00:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_90002/3433313796.py:345: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n",
      "  mcmc = MCMC(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit_bayesian_pyjags] done in 0.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/tensorflow_probability/python/mcmc/sample.py:339: UserWarning: Tracing all kernel results by default is deprecated. Set the `trace_fn` argument to None (the future default value) or an explicit callback that traces the values you are interested in.\n",
      "  warnings.warn('Tracing all kernel results by default is deprecated. Set '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit_bayesian_tfp_hmc] done in 1.43s\n",
      "\n",
      "▼▼ PyMC-HMC\n",
      "▶ Classical MSE : 0.36\n",
      "▶ Classical RMSE: 0.60\n",
      "▶ Classical MAE : 0.50\n",
      "▶ Classical R²  : 0.956\n",
      "\n",
      "▼▼ PyMC-ADVI\n",
      "▶ Classical MSE : 5.90\n",
      "▶ Classical RMSE: 2.43\n",
      "▶ Classical MAE : 2.13\n",
      "▶ Classical R²  : 0.284\n",
      "\n",
      "▼▼ Stan\n",
      "▶ Classical MSE : 0.36\n",
      "▶ Classical RMSE: 0.60\n",
      "▶ Classical MAE : 0.50\n",
      "▶ Classical R²  : 0.956\n",
      "\n",
      "▼▼ JAGS\n",
      "▶ Classical MSE : 0.37\n",
      "▶ Classical RMSE: 0.61\n",
      "▶ Classical MAE : 0.51\n",
      "▶ Classical R²  : 0.955\n",
      "\n",
      "▼▼ NumPyro\n",
      "▶ Classical MSE : 0.36\n",
      "▶ Classical RMSE: 0.60\n",
      "▶ Classical MAE : 0.50\n",
      "▶ Classical R²  : 0.956\n",
      "\n",
      "▼▼ TFP-HMC\n",
      "▶ Classical MSE : 0.38\n",
      "▶ Classical RMSE: 0.61\n",
      "▶ Classical MAE : 0.50\n",
      "▶ Classical R²  : 0.954\n",
      "\n",
      "✔︎ Smoke-test figure saved → smoke_test_fits.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "save_posterior_summary: variable 'u' not found in posterior vars ['alpha', 'beta', 'sigma']; writing empty summary.\n",
      "save_global_effects → missing \"No variable named 'beta_age'. Did you mean one of ('beta',)?\" in posterior groups ['alpha', 'beta', 'sigma'] – skipping global-effects write for this engine.\n",
      "save_posterior_summary: variable 'u' not found in posterior vars ['N', 'alpha', 'beta', 'sigma', 'tau', 'x', 'y', 'y_obs']; writing empty summary.\n",
      "save_global_effects → missing \"No variable named 'beta_age'. Did you mean one of ('beta',)?\" in posterior groups ['N', 'alpha', 'beta', 'sigma', 'tau', 'x', 'y', 'y_obs'] – skipping global-effects write for this engine.\n",
      "save_posterior_summary: variable 'u' not found in posterior vars ['alpha', 'beta', 'sigma']; writing empty summary.\n",
      "save_global_effects → missing \"No variable named 'beta_age'. Did you mean one of ('beta',)?\" in posterior groups ['alpha', 'beta', 'sigma'] – skipping global-effects write for this engine.\n",
      "save_posterior_summary: variable 'u' not found in posterior vars ['alpha', 'beta', 'log_sigma', 'sigma']; writing empty summary.\n",
      "save_global_effects → missing \"No variable named 'beta_age'. Did you mean one of ('beta',)?\" in posterior groups ['alpha', 'beta', 'log_sigma', 'sigma'] – skipping global-effects write for this engine.\n",
      "save_posterior_summary: variable 'u' not found in posterior vars ['alpha', 'beta', 'sigma']; writing empty summary.\n",
      "save_global_effects → missing \"No variable named 'beta_age'. Did you mean one of ('beta',)?\" in posterior groups ['alpha', 'beta', 'sigma'] – skipping global-effects write for this engine.\n",
      "save_posterior_summary: variable 'u' not found in posterior vars ['alpha', 'beta', 'sigma']; writing empty summary.\n",
      "save_global_effects → missing \"No variable named 'beta_age'. Did you mean one of ('beta',)?\" in posterior groups ['alpha', 'beta', 'sigma'] – skipping global-effects write for this engine.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Warning] no global-effects for 'Stan', skipping predictions\n",
      "[Warning] no global-effects for 'JAGS', skipping predictions\n",
      "[Warning] no global-effects for 'NumPyro', skipping predictions\n",
      "[Warning] no global-effects for 'TFP-HMC', skipping predictions\n",
      "[Warning] no global-effects for 'PyMC-HMC', skipping predictions\n",
      "[Warning] no global-effects for 'PyMC-ADVI', skipping predictions\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%writefile src/models/bayesian_alternatives.py\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# src/models/bayesian_alternatives.py\n",
    "# Implements additional Bayesian engines that all return\n",
    "# arviz.InferenceData so downstream metrics remain unchanged.\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "\n",
    "# ─── Configure JAX/GPU ──────────────────────────────────────────────\n",
    "from src.utils.jax_memory_fix_module import apply_jax_memory_fix\n",
    "apply_jax_memory_fix(fraction=0.10, preallocate=False)\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# ─── Timing helper ─────────────────────────────────────────────────\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "@contextmanager\n",
    "def _timed_section(label: str):\n",
    "    t0 = time.perf_counter()\n",
    "    yield\n",
    "    print(f\"[{label}] done in {time.perf_counter() - t0:.2f}s\")\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import tempfile\n",
    "import os\n",
    "from cmdstanpy import CmdStanModel\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyjags\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import json\n",
    "from pathlib import Path\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_posterior_summary(\n",
    "    idata: az.InferenceData,\n",
    "    roster_df: pd.DataFrame,\n",
    "    output_parquet: Path,\n",
    "    u_var: str = \"u\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract per-batter random effect 'u' quantiles and save to Parquet.\n",
    "    Ensures that, even if u_var is missing, an (empty) parquet is written.\n",
    "    \"\"\"\n",
    "    import logging\n",
    "    # ensure directory exists\n",
    "    output_parquet.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    posterior_vars = set(idata.posterior.data_vars)\n",
    "    if u_var not in posterior_vars:\n",
    "        logging.getLogger(__name__).warning(\n",
    "            \"save_posterior_summary: variable %r not found in posterior vars %s; writing empty summary.\",\n",
    "            u_var, sorted(posterior_vars)\n",
    "        )\n",
    "        # empty with expected schema\n",
    "        empty = pd.DataFrame(columns=[\n",
    "            \"batter_id\", \"batter_idx\", \"u_q2.5\", \"u_q50\", \"u_q97.5\"\n",
    "        ])\n",
    "        empty.to_parquet(output_parquet, index=False)\n",
    "        return empty\n",
    "\n",
    "    # proceed normally\n",
    "    summary = (\n",
    "        az.summary(idata, var_names=[u_var], hdi_prob=0.95)\n",
    "        .reset_index()\n",
    "        .rename(columns={\n",
    "            \"mean\":     \"u_q50\",\n",
    "            \"hdi_2.5%\": \"u_q2.5\",\n",
    "            \"hdi_97.5%\":\"u_q97.5\"\n",
    "        })\n",
    "    )\n",
    "    df = (\n",
    "        roster_df[[\"batter_id\", \"batter_idx\"]]\n",
    "        .merge(summary, left_on=\"batter_idx\", right_on=\"index\", how=\"left\")\n",
    "        .loc[:, [\"batter_id\", \"batter_idx\", \"u_q2.5\", \"u_q50\", \"u_q97.5\"]]\n",
    "    )\n",
    "    df.to_parquet(output_parquet, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def save_global_effects(\n",
    "    idata: az.InferenceData,\n",
    "    training_df: pd.DataFrame,\n",
    "    output_json: Path,\n",
    "    *,\n",
    "    age_col: str = \"age\",\n",
    "    level_idx: int = 2,\n",
    "    var_alias: dict | None = None,      # NEW\n",
    ") -> dict | None:\n",
    "    \"\"\"\n",
    "    Extract global intercept & slopes. If the model does not contain the\n",
    "    required variables, emit *None* and skip saving.\n",
    "    \"\"\"\n",
    "    import json, logging\n",
    "    log = logging.getLogger(__name__)\n",
    "    output_json.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # --- 1) Resolve variable names ---------------------------------------\n",
    "    default_alias = {\"Intercept\": \"alpha\", \"beta_age\": \"beta_age\",\n",
    "                     \"beta_level\": \"beta_level\"}\n",
    "    alias = {**default_alias, **(var_alias or {})}\n",
    "\n",
    "    posterior = idata.posterior\n",
    "    try:\n",
    "        mu_mean    = posterior[alias[\"Intercept\"]].mean().item()\n",
    "        beta_age   = posterior[alias[\"beta_age\"]].mean().item()\n",
    "        beta_level = (\n",
    "            posterior[alias[\"beta_level\"]][..., level_idx].mean().item()\n",
    "        )\n",
    "    except KeyError as err:\n",
    "        log.warning(\n",
    "            \"save_global_effects → missing %s in posterior groups %s – \"\n",
    "            \"skipping global-effects write for this engine.\",\n",
    "            err, list(posterior.data_vars)\n",
    "        )\n",
    "        return None                         # ← early-exit\n",
    "\n",
    "    glob = {\n",
    "        \"mu_mean\":    mu_mean,\n",
    "        \"beta_age\":   beta_age,\n",
    "        \"beta_level\": {str(level_idx): beta_level},\n",
    "        \"median_age\": float(training_df[age_col].median()),\n",
    "    }\n",
    "    output_json.write_text(json.dumps(glob, indent=2))\n",
    "    return glob\n",
    "\n",
    "\n",
    "def summarize_coefficients(idata, var_names=None):\n",
    "    \"\"\"\n",
    "    Return a DataFrame of mean + 95% HDI for selected variables.\n",
    "    \"\"\"\n",
    "    df = az.summary(idata, var_names=var_names, hdi_prob=0.95)[\n",
    "        ['mean','hdi_2.5%','hdi_97.5%']\n",
    "    ]\n",
    "    return df\n",
    "\n",
    "def plot_forest_coefficients(idata, var_names=None, figsize=(8,6)):\n",
    "    \"\"\"\n",
    "    Show a forest plot for the specified coefficients.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "    az.plot_forest(idata, var_names=var_names, combined=True, credible_interval=0.95)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_posterior_distributions(idata, var_names=None):\n",
    "    \"\"\"\n",
    "    Show posterior density plots for selected variables.\n",
    "    \"\"\"\n",
    "    az.plot_posterior(idata, var_names=var_names, hdi_prob=0.95)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 1)  CmdStanPy  ---------------------------------------------------------\n",
    "def fit_bayesian_cmdstanpy(\n",
    "    stan_code: str,\n",
    "    stan_data: dict,\n",
    "    *,\n",
    "    draws: int = 1000,\n",
    "    warmup: int = 500,\n",
    "    chains: int = 4,\n",
    "    seed: int = 42,\n",
    ") -> az.InferenceData:\n",
    "    \"\"\"\n",
    "    Compile Stan code, run sampling with CmdStanPy, and return ArviZ InferenceData\n",
    "    with both 'posterior' and 'posterior_predictive' groups.\n",
    "    \"\"\"\n",
    "    with _timed_section(\"fit_bayesian_cmdstanpy\"):\n",
    "        # Write Stan code to a temp file\n",
    "        with tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".stan\", delete=False) as tmp:\n",
    "            tmp.write(stan_code)\n",
    "            stan_file = tmp.name\n",
    "\n",
    "        try:\n",
    "            # Compile & sample\n",
    "            model = CmdStanModel(stan_file=stan_file, force_compile=True)\n",
    "            fit   = model.sample(\n",
    "                data=stan_data,\n",
    "                iter_sampling=draws,\n",
    "                iter_warmup=warmup,\n",
    "                chains=chains,\n",
    "                seed=seed\n",
    "            )\n",
    "\n",
    "            # Convert → InferenceData, specifying y_obs as the predictive variable\n",
    "            idata = az.from_cmdstanpy(\n",
    "                posterior=fit,\n",
    "                posterior_predictive=[\"y_obs\"]  # ✔️ correct usage\n",
    "            )\n",
    "        finally:\n",
    "            # Cleanup temp file\n",
    "            try:\n",
    "                os.remove(stan_file)\n",
    "            except OSError:\n",
    "                pass\n",
    "\n",
    "    return idata\n",
    "\n",
    "\n",
    "\n",
    "# 2)  PyJAGS (Gibbs)  ----------------------------------------------------\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# UPDATED: fit_bayesian_pyjags  (drop-in replacement)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import tempfile, os, arviz as az, pyjags, itertools\n",
    "from contextlib import contextmanager\n",
    "from typing import Sequence, Optional, List\n",
    "\n",
    "# Valid RNG factories supported by JAGS’ base / lecuyer modules\n",
    "VALID_RNGS: List[str] = [\n",
    "    \"base::Wichmann-Hill\",\n",
    "    \"base::Marsaglia-Multicarry\",\n",
    "    \"base::Super-Duper\",\n",
    "    \"base::Mersenne-Twister\",\n",
    "    \"lecuyer::RngStream\",\n",
    "]\n",
    "\n",
    "def fit_bayesian_pyjags(\n",
    "    jags_model: str,\n",
    "    jags_data: dict,\n",
    "    *,\n",
    "    draws: int = 5_000,\n",
    "    burn: int = 1_000,\n",
    "    thin: int = 1,\n",
    "    chains: int = 4,\n",
    "    seed: int = 42,\n",
    "    rng_name: Optional[str] = None,\n",
    ") -> az.InferenceData:\n",
    "    \"\"\"\n",
    "    Fit a BUGS model in JAGS via PyJAGS and return ArviZ InferenceData.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    rng_name : str | None\n",
    "        One of the names in `VALID_RNGS.  If None (default) the function\n",
    "        rotates valid RNGs across chains so every chain uses an independent\n",
    "        generator, per JAGS best-practice.\n",
    "    \"\"\"\n",
    "    # 1) Write BUGS code to a temporary file\n",
    "    tmp = tempfile.NamedTemporaryFile(mode=\"w\", suffix=\".bug\", delete=False)\n",
    "    try:\n",
    "        tmp.write(jags_model)\n",
    "        tmp.close()\n",
    "        print(f\"💾 JAGS model written to {tmp.name}\")\n",
    "\n",
    "        # 2) Choose RNGs for each chain\n",
    "        if rng_name is not None:\n",
    "            if rng_name not in VALID_RNGS:\n",
    "                raise ValueError(\n",
    "                    f\"rng_name must be one of {VALID_RNGS}, got {rng_name!r}\"\n",
    "                )\n",
    "            rngs: Sequence[str] = [rng_name] * chains\n",
    "        else:\n",
    "            # Rotate through the supported generators\n",
    "            rngs = list(itertools.islice(itertools.cycle(VALID_RNGS), chains))\n",
    "\n",
    "        # 3) Initial-value dicts (independent seeds per chain)\n",
    "        init_vals = [\n",
    "            {\".RNG.name\": rngs[c],\n",
    "             \".RNG.seed\": seed + c * 10}\n",
    "            for c in range(chains)\n",
    "        ]\n",
    "\n",
    "        # 4) Build and adapt the model\n",
    "        with _timed_section(\"fit_bayesian_pyjags\"):\n",
    "            model = pyjags.Model(\n",
    "                file=tmp.name,\n",
    "                data=jags_data,\n",
    "                chains=chains,\n",
    "                adapt=burn,\n",
    "                init=init_vals\n",
    "            )\n",
    "\n",
    "            # 5) Draw samples\n",
    "            samples = model.sample(draws, vars=None, thin=thin)\n",
    "\n",
    "        # 6) Convert → InferenceData\n",
    "        return az.from_pyjags(posterior=samples)\n",
    "\n",
    "    finally:\n",
    "        # Always clean up temp file\n",
    "        try:\n",
    "            os.remove(tmp.name)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 3)  NumPyro (NUTS)  ----------------------------------------------------\n",
    "def fit_bayesian_numpyro(\n",
    "    numpyro_model,\n",
    "    rng_key,\n",
    "    *model_args,\n",
    "    draws: int = 1000,\n",
    "    warmup: int = 500,\n",
    "    chains: int = 4,\n",
    "    progress_bar: bool = False,\n",
    "    **model_kwargs\n",
    ") -> az.InferenceData:\n",
    "    \"\"\"\n",
    "    Run NUTS in NumPyro and return ArviZ InferenceData.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    numpyro_model : callable\n",
    "        A NumPyro model accepting positional args and kwargs (e.g., x, y=y).\n",
    "    rng_key : jax.random.PRNGKey\n",
    "        Random number generator key for reproducibility.\n",
    "    *model_args : tuple\n",
    "        Positional arguments to pass into the model (e.g., x_data).\n",
    "    draws : int\n",
    "        Number of posterior samples per chain.\n",
    "    warmup : int\n",
    "        Number of warmup (tuning) iterations.\n",
    "    chains : int\n",
    "        Number of MCMC chains.\n",
    "    progress_bar : bool\n",
    "        Whether to show a progress bar during sampling.\n",
    "    **model_kwargs : dict\n",
    "        Keyword arguments to pass into the model (e.g., y=y_obs).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    idata : arviz.InferenceData\n",
    "        InferenceData containing posterior samples and diagnostics.\n",
    "    \"\"\"\n",
    "    import numpyro\n",
    "    from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "    # 1) Instantiate the NUTS kernel and MCMC controller\n",
    "    kernel = NUTS(numpyro_model)\n",
    "    mcmc = MCMC(\n",
    "        kernel,\n",
    "        num_warmup=warmup,\n",
    "        num_samples=draws,\n",
    "        num_chains=chains,\n",
    "        progress_bar=progress_bar\n",
    "    )\n",
    "\n",
    "    # 2) Run sampling, passing model_args and model_kwargs\n",
    "    #    model_args and model_kwargs map to NumPyro's model_args/model_kwargs\n",
    "    mcmc.run(rng_key, *model_args, **model_kwargs)  # :contentReference[oaicite:3]{index=3}\n",
    "\n",
    "    # 3) Convert to ArviZ InferenceData and return\n",
    "    return az.from_numpyro(mcmc)\n",
    "\n",
    "\n",
    "# 4)  TensorFlow Probability HMC  ---------------------------------------\n",
    "def fit_bayesian_tfp_hmc(\n",
    "    target_log_prob_fn,\n",
    "    init_state,\n",
    "    *,\n",
    "    step_size: float = 0.05,\n",
    "    leapfrog_steps: int = 5,\n",
    "    draws: int = 1000,\n",
    "    burnin: int = 500,\n",
    "    seed: int = 42,\n",
    ") -> az.InferenceData:\n",
    "    \"\"\"\n",
    "    Single-chain HMC in TensorFlow Probability, returning posterior\n",
    "    variables named 'alpha', 'beta', 'log_sigma', and 'sigma'.\n",
    "    \"\"\"\n",
    "    import tensorflow as tf\n",
    "    import tensorflow_probability as tfp\n",
    "    import numpy as np\n",
    "\n",
    "    tfd, tfmcmc = tfp.distributions, tfp.mcmc\n",
    "\n",
    "    with _timed_section(\"fit_bayesian_tfp_hmc\"):\n",
    "        # Build & adapt the HMC kernel\n",
    "        hmc = tfmcmc.HamiltonianMonteCarlo(\n",
    "            target_log_prob_fn=target_log_prob_fn,\n",
    "            step_size=step_size,\n",
    "            num_leapfrog_steps=leapfrog_steps,\n",
    "        )\n",
    "        adaptive = tfmcmc.SimpleStepSizeAdaptation(\n",
    "            inner_kernel=hmc,\n",
    "            num_adaptation_steps=int(0.8 * burnin),\n",
    "        )\n",
    "\n",
    "        # Run the chain\n",
    "        @tf.function(autograph=False, jit_compile=True)\n",
    "        def _run_chain():\n",
    "            return tfmcmc.sample_chain(\n",
    "                num_results=draws,\n",
    "                current_state=init_state,\n",
    "                kernel=adaptive,\n",
    "                num_burnin_steps=burnin,\n",
    "                seed=seed,\n",
    "            )\n",
    "\n",
    "        # Unpack raw states: (alpha, beta, log_sigma)\n",
    "        (alpha_t, beta_t, log_sigma_t), _ = _run_chain()\n",
    "\n",
    "    # To NumPy\n",
    "    alpha_np     = alpha_t.numpy()\n",
    "    beta_np      = beta_t.numpy()\n",
    "    log_sigma_np = log_sigma_t.numpy()\n",
    "    sigma_np     = np.exp(log_sigma_np)  # transform\n",
    "\n",
    "    # Build posterior dict with both raw and transformed sigma\n",
    "    posterior = {\n",
    "        \"alpha\":     alpha_np,\n",
    "        \"beta\":      beta_np,\n",
    "        \"log_sigma\": log_sigma_np,  # keep raw chain\n",
    "        \"sigma\":     sigma_np,      # transformed for usability\n",
    "    }\n",
    "\n",
    "    # Return as ArviZ InferenceData\n",
    "    return az.from_dict(posterior=posterior)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5)  PyMC ADVI (fast VI baseline)  --------------------------------------\n",
    "def fit_bayesian_pymc_advi(\n",
    "    pymc_model,\n",
    "    *,\n",
    "    draws: int = 1_000,\n",
    "    tune:  int = 10_000,\n",
    "    progressbar: bool = False,\n",
    ") -> az.InferenceData:\n",
    "    \"\"\"\n",
    "    Run Automatic Differentiation VI (ADVI) in PyMC and return InferenceData.\n",
    "\n",
    "    * Works with both PyMC <5 and ≥5.\n",
    "    * Falls back gracefully if a custom start dict is invalid.\n",
    "    * Emits timing + initial-point diagnostics for reproducibility.\n",
    "    \"\"\"\n",
    "    import pymc as pm\n",
    "    import logging\n",
    "\n",
    "    logger = logging.getLogger(__name__)\n",
    "\n",
    "    with _timed_section(\"fit_bayesian_pymc_advi\"):\n",
    "        with pymc_model:\n",
    "            # ------------------------------------------------------------------\n",
    "            # 1) Obtain a *numeric* initial point compatible with the PyMC core\n",
    "            # ------------------------------------------------------------------\n",
    "            start = None\n",
    "            try:\n",
    "                iprop = pymc_model.initial_point      # may be dict *or* callable\n",
    "                start = iprop() if callable(iprop) else iprop\n",
    "                if not isinstance(start, dict):\n",
    "                    raise TypeError(\"initial_point did not return a dict.\")\n",
    "                logger.info(\n",
    "                    \"Using model.initial_point (keys=%s...)\",\n",
    "                    list(start)[:4]\n",
    "                )\n",
    "            except Exception as err:  # noqa: BLE001\n",
    "                logger.warning(\n",
    "                    \"Falling back to PyMC auto-initialisation (err=%s)\", err\n",
    "                )\n",
    "                start = None          # Let pm.fit decide\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # 2) Fit ADVI\n",
    "            # ------------------------------------------------------------------\n",
    "            approx = pm.fit(\n",
    "                n=tune,\n",
    "                method=\"advi\",\n",
    "                start=start,\n",
    "                progressbar=progressbar,\n",
    "            )\n",
    "\n",
    "            # ------------------------------------------------------------------\n",
    "            # 3) Draw posterior samples & convert to InferenceData\n",
    "            # ------------------------------------------------------------------\n",
    "            idata = approx.sample(draws)\n",
    "\n",
    "    return idata\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import jax.random as jr\n",
    "    import tensorflow_probability as tfp\n",
    "    import tensorflow as tf\n",
    "    import pymc as pm\n",
    "    from cmdstanpy import CmdStanModel\n",
    "    import arviz as az\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # ─── Additional imports for summaries & interpretability ────────────\n",
    "    from src.models.hierarchical_predict import (\n",
    "        predict_from_summaries,\n",
    "        get_top_hitters\n",
    "    )\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    import numpyro, numpyro.distributions as dist\n",
    "    from src.utils.bayesian_metrics import compute_classical_metrics\n",
    "    # ─── User-configurable sampling parameters ──────────────────────────\n",
    "    N_CHAINS   = 4\n",
    "    N_DRAWS    = 500\n",
    "    N_TUNE     = 500\n",
    "    PP_DRAWS   = 250\n",
    "    JAGS_DRAWS = 500\n",
    "    JAGS_BURN  = 500\n",
    "    SEED       = 42\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # ─── Synthetic data ────────────────────────────────────────────────\n",
    "    N       = 50\n",
    "    rng     = np.random.default_rng(SEED)\n",
    "    x_data  = rng.uniform(-2, 2, size=N)\n",
    "    y_true  = 1.0 + 2.5 * x_data\n",
    "    y_obs   = y_true + rng.normal(0, 0.8, size=N)\n",
    "\n",
    "    # ─── Helper to attach posterior predictive draws ───────────────────\n",
    "    def _attach_ppc(idata, x, alpha=\"alpha\", beta=\"beta\", sigma=\"sigma\", draws=None):\n",
    "        post = idata.posterior\n",
    "        a_flat = post[alpha].stack(samples=(\"chain\",\"draw\")).values\n",
    "        b_flat = post[beta ].stack(samples=(\"chain\",\"draw\")).values\n",
    "        s_flat = post[sigma].stack(samples=(\"chain\",\"draw\")).values\n",
    "\n",
    "        n_chains = post.sizes.get(\"chain\", 1)\n",
    "        n_draws_param = post.sizes[\"draw\"]\n",
    "        n_obs = x.shape[0]\n",
    "\n",
    "        if draws is not None:\n",
    "            total = min(a_flat.size, draws * n_chains)\n",
    "            per_chain = total // n_chains\n",
    "        else:\n",
    "            total = a_flat.size\n",
    "            per_chain = n_draws_param\n",
    "\n",
    "        mu = a_flat[:total, None] + b_flat[:total, None] * x[None, :]\n",
    "        ypp = rng.normal(loc=mu, scale=s_flat[:total, None])\n",
    "        arr = ypp.reshape(n_chains, per_chain, n_obs)\n",
    "\n",
    "        idata.add_groups(\n",
    "            posterior_predictive={\"y_obs\": arr},\n",
    "            dims={\"y_obs\": [\"obs\"]}\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ─── 1) PyMC HMC (NUTS) ───────────────────────────────────────────\n",
    "    coords = {\"obs\": np.arange(N)}\n",
    "    with pm.Model(coords=coords) as pymc_model:\n",
    "        alpha = pm.Normal(\"alpha\", 0, 5)\n",
    "        beta  = pm.Normal(\"beta\",  0, 5)\n",
    "        sigma = pm.HalfNormal(\"sigma\", 1)\n",
    "        mu    = alpha + beta * x_data\n",
    "        pm.Normal(\"y\", mu, sigma, observed=y_obs, dims=\"obs\")\n",
    "\n",
    "    idata_pymc_hmc = pm.sample(\n",
    "        N_DRAWS,\n",
    "        tune=N_TUNE,\n",
    "        chains=N_CHAINS,\n",
    "        random_seed=SEED,\n",
    "        progressbar=False,\n",
    "        return_inferencedata=True,\n",
    "        model=pymc_model\n",
    "    )\n",
    "    pm.sample_posterior_predictive(\n",
    "        idata_pymc_hmc,\n",
    "        var_names=[\"y\"],\n",
    "        extend_inferencedata=True,\n",
    "        model=pymc_model\n",
    "    )\n",
    "    idata_pymc_hmc.posterior_predictive = \\\n",
    "        idata_pymc_hmc.posterior_predictive.rename({\"y\": \"y_obs\"})\n",
    "\n",
    "    # ─── 2) PyMC ADVI ─────────────────────────────────────────────────\n",
    "    idata_pymc_advi = fit_bayesian_pymc_advi(pymc_model,\n",
    "                                             draws=PP_DRAWS,\n",
    "                                             tune=N_TUNE*5)\n",
    "    _attach_ppc(idata_pymc_advi, x_data, draws=PP_DRAWS)\n",
    "\n",
    "    # ─── 3) CmdStanPy (Stan HMC) ──────────────────────────────────────\n",
    "    stan_code = \"\"\"\n",
    "    data { int<lower=0> N; vector[N] x; vector[N] y; }\n",
    "    parameters { real alpha; real beta; real<lower=0> sigma; }\n",
    "    model { y ~ normal(alpha+beta*x, sigma); }\n",
    "    generated quantities {\n",
    "      vector[N] y_obs;\n",
    "      for (n in 1:N)\n",
    "        y_obs[n] = normal_rng(alpha+beta*x[n], sigma);\n",
    "    }\n",
    "    \"\"\"\n",
    "    stan_data = {\"N\": N, \"x\": x_data, \"y\": y_obs}\n",
    "    idata_stan = fit_bayesian_cmdstanpy(stan_code,\n",
    "                                        stan_data,\n",
    "                                        draws=N_DRAWS,\n",
    "                                        warmup=N_TUNE,\n",
    "                                        chains=N_CHAINS,\n",
    "                                        seed=SEED)\n",
    "\n",
    "    # ─── 4) PyJAGS (Gibbs Sampling) ───────────────────────────────────\n",
    "    jags_model = \"\"\"\n",
    "    model {\n",
    "      for (i in 1:N) {\n",
    "        y[i] ~ dnorm(alpha+beta*x[i], tau);\n",
    "        y_obs[i] ~ dnorm(alpha + beta*x[i], tau);\n",
    "\n",
    "      }\n",
    "      alpha ~ dnorm(0, .001);\n",
    "      beta  ~ dnorm(0, .001);\n",
    "      sigma ~ dunif(0, 10);\n",
    "      tau   <- pow(sigma, -2);\n",
    "    }\n",
    "    \"\"\"\n",
    "    jags_data = {\"N\": N, \"x\": x_data, \"y\": y_obs}\n",
    "    idata_jags = fit_bayesian_pyjags(jags_model,\n",
    "                                     jags_data,\n",
    "                                     draws=JAGS_DRAWS,\n",
    "                                     burn=JAGS_BURN,\n",
    "                                     chains=N_CHAINS,\n",
    "                                     seed=SEED)\n",
    "    _attach_ppc(idata_jags, x_data, draws=PP_DRAWS)\n",
    "\n",
    "    # ─── 5) NumPyro (NUTS) ─────────────────────────────────────────────\n",
    "    def numpyro_model(x, y=None):\n",
    "        a = numpyro.sample(\"alpha\", dist.Normal(0,5))\n",
    "        b = numpyro.sample(\"beta\",  dist.Normal(0,5))\n",
    "        s = numpyro.sample(\"sigma\", dist.HalfNormal(1))\n",
    "        mu = a + b * x\n",
    "        numpyro.sample(\"y\", dist.Normal(mu, s), obs=y)\n",
    "    rng_key = jr.PRNGKey(SEED)\n",
    "    idata_numpyro = fit_bayesian_numpyro(\n",
    "        numpyro_model,\n",
    "        rng_key,\n",
    "        x_data,                # positional model argument\n",
    "        y=y_obs,               # keyword model argument\n",
    "        draws=N_DRAWS,\n",
    "        warmup=N_TUNE,\n",
    "        chains=N_CHAINS\n",
    "    )\n",
    "\n",
    "    _attach_ppc(idata_numpyro, x_data, draws=PP_DRAWS)\n",
    "\n",
    "    # ─── 6) TFP HMC ───────────────────────────────────────────────────\n",
    "    tfd, tfmcmc = tfp.distributions, tfp.mcmc\n",
    "    def target_log_prob_fn(alpha, beta, log_sigma):\n",
    "        sigma = tf.exp(log_sigma)\n",
    "        yhat  = alpha + beta * tf.constant(x_data, tf.float32)\n",
    "        return tf.reduce_sum(tfd.Normal(yhat, sigma).log_prob(y_obs)) + \\\n",
    "               tfd.Normal(0,5).log_prob(alpha) + \\\n",
    "               tfd.Normal(0,5).log_prob(beta) + \\\n",
    "               tfd.Normal(0,1).log_prob(log_sigma)\n",
    "\n",
    "    init_state = [tf.zeros([]), tf.zeros([]), tf.zeros([])]\n",
    "    idata_tfp  = fit_bayesian_tfp_hmc(target_log_prob_fn,\n",
    "                                      init_state,\n",
    "                                      draws=N_DRAWS,\n",
    "                                      burnin=N_TUNE,\n",
    "                                      seed=SEED)\n",
    "\n",
    "    # 2) Attach posterior predictive draws with positive sigma\n",
    "    _attach_ppc(\n",
    "        idata_tfp,\n",
    "        x_data,\n",
    "        alpha=\"alpha\",\n",
    "        beta=\"beta\",\n",
    "        sigma=\"sigma\",\n",
    "        draws=PP_DRAWS\n",
    "    )\n",
    "\n",
    "    # ─── Compare & Plot ───────────────────────────────────────────────\n",
    "    engines = {\n",
    "      \"PyMC-HMC\":  idata_pymc_hmc,\n",
    "      \"PyMC-ADVI\": idata_pymc_advi,\n",
    "      \"Stan\":      idata_stan,\n",
    "      \"JAGS\":      idata_jags,\n",
    "      \"NumPyro\":   idata_numpyro,\n",
    "      \"TFP-HMC\":   idata_tfp,\n",
    "    }\n",
    "    for name,idata in engines.items():\n",
    "        print(f\"\\n▼▼ {name}\")\n",
    "        compute_classical_metrics(idata, y_obs)\n",
    "\n",
    "    # quick visual check (posterior mean lines)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(x_data, y_obs, label=\"obs\", alpha=.6)\n",
    "    line_x = np.linspace(-2, 2, 100)\n",
    "    colors = plt.cm.tab10(np.linspace(0,1,len(engines)))\n",
    "    for c,(name,id_) in zip(colors, engines.items()):\n",
    "        a = idata_tfp.posterior[\"alpha\"].mean().item()\n",
    "        b = idata_tfp.posterior[\"beta\"].mean().item()\n",
    "\n",
    "        plt.plot(line_x, a + b*line_x, color=c, label=name, lw=1.2)\n",
    "    plt.legend(); plt.title(\"Posterior mean fits – smoke test\")\n",
    "    plt.tight_layout()\n",
    "    Path(\"data/images/smoke_test_fits.png\").write_bytes(plt.savefig(\"/tmp/_smoke.png\") or b\"\")\n",
    "    print(\"\\n✔︎ Smoke-test figure saved → smoke_test_fits.png\")\n",
    "\n",
    "\n",
    "    # ─── Prepare real roster & training data for prediction ───────────\n",
    "    BASE = Path(\"data/models\")\n",
    "    ROSTER = Path(\"data/Research Data Project/Research Data Project/exit_velo_validate_data.csv\")\n",
    "    RAW    = Path(\"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\")\n",
    "    # 2) run hierarchical exit‐velo pipeline\n",
    "\n",
    "    roster_df   = pd.read_csv(ROSTER)\n",
    "    training_df = feature_engineer(pd.read_csv(RAW))\n",
    "\n",
    "    engines = {\n",
    "      \"Stan\":      idata_stan,\n",
    "      \"JAGS\":      idata_jags,\n",
    "      \"NumPyro\":   idata_numpyro,\n",
    "      \"TFP-HMC\":   idata_tfp,\n",
    "      \"PyMC-HMC\":  idata_pymc_hmc,\n",
    "      \"PyMC-ADVI\": idata_pymc_advi,\n",
    "    }\n",
    "\n",
    "    for name, idata in engines.items():\n",
    "        summary_file = BASE/name/\"posterior_summary.parquet\"\n",
    "        globals_file = BASE/name/\"global_effects.json\"\n",
    "\n",
    "        save_posterior_summary(idata, roster_df, summary_file, u_var=\"u\")\n",
    "        # 1) Save globals; skip this engine if it returns None\n",
    "        glob_res = save_global_effects(\n",
    "            idata, training_df, globals_file, age_col=\"age\"\n",
    "        )\n",
    "        if glob_res is None:\n",
    "            print(f\"[Warning] no global-effects for '{name}', skipping predictions\")\n",
    "            continue\n",
    "\n",
    "        # 2) Attempt predictions; catch file‑or‑schema errors and skip\n",
    "        try:\n",
    "            df_pred = predict_from_summaries(\n",
    "                roster_csv=ROSTER,\n",
    "                raw_csv=RAW,\n",
    "                posterior_parquet=summary_file,\n",
    "                global_effects_json=globals_file,\n",
    "                output_csv=BASE/name/\"predictions_2024.csv\",\n",
    "                verbose=True\n",
    "            )\n",
    "        except ValueError as err:\n",
    "            print(f\"[Error] skipping '{name}': {err}\")\n",
    "            continue\n",
    "\n",
    "        # 3) If we got this far, we have predictions → extract top hitters\n",
    "        get_top_hitters(df_pred, hitter_col=\"hitter_type\", n=5, verbose=True)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 02:10:00,799 - src.utils.jax_gpu_utils - INFO - GPU-diag: {\n",
      "  \"backend\": \"gpu\",\n",
      "  \"devices\": [\n",
      "    \"cuda:0\"\n",
      "  ],\n",
      "  \"python\": \"3.10.17\",\n",
      "  \"ld_library_path\": \"/opt/conda/envs/marlins-ds-gpu/lib\",\n",
      "  \"nvidia-smi\": \"NVIDIA GeForce RTX 5080 Laptop GPU, 572.96, 16303\"\n",
      "}\n",
      "2025-05-10 02:10:00,826 - src.utils.jax_gpu_utils - INFO - GPU-diag: {\n",
      "  \"backend\": \"gpu\",\n",
      "  \"devices\": [\n",
      "    \"cuda:0\"\n",
      "  ],\n",
      "  \"python\": \"3.10.17\",\n",
      "  \"ld_library_path\": \"/opt/conda/envs/marlins-ds-gpu/lib\",\n",
      "  \"nvidia-smi\": \"NVIDIA GeForce RTX 5080 Laptop GPU, 572.96, 16303\"\n",
      "}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 JAX module: <module 'jax' from '/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/jax/__init__.py'>\n",
      "🔍 JAX path:   /opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/jax/__init__.py\n",
      "✅ JAX version: 0.5.2\n",
      "🔍 PyMC version: 5.22.0\n",
      "🔍 JAX backend: gpu\n",
      "Dropping rows with NaN in exit_velo\n",
      "season               0\n",
      "level_abbr           0\n",
      "batter_id            0\n",
      "pitcher_id           0\n",
      "exit_velo        97979\n",
      "launch_angle      9130\n",
      "spray_angle       6227\n",
      "hangtime         75968\n",
      "hit_type           201\n",
      "batter_hand          0\n",
      "pitcher_hand         0\n",
      "batter_height        0\n",
      "pitch_group          0\n",
      "outcome              0\n",
      "age                  0\n",
      "dtype: int64\n",
      "after rows dropped with NaN in exit_velo\n",
      "season               0\n",
      "level_abbr           0\n",
      "batter_id            0\n",
      "pitcher_id           0\n",
      "exit_velo            0\n",
      "launch_angle         0\n",
      "spray_angle          0\n",
      "hangtime         12429\n",
      "hit_type             0\n",
      "batter_hand          0\n",
      "pitcher_hand         0\n",
      "batter_height        0\n",
      "pitch_group          0\n",
      "outcome              0\n",
      "age                  0\n",
      "dtype: int64\n",
      "⚠️  Nulls after target‑filter:\n",
      "  • hangtime         12,429 (1.00%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Marlins_Data_Science_Project/src/features/preprocess.py:124: UserWarning: Dataset has 1154499 rows (>200000); ensure you’re only fitting on TRAINING data.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current GPU memory usage: 714MB / 16303MB (4.38%)\n",
      "Target memory usage: 80.00%\n",
      "[alloc] initial used 714 MB  (4.4%)  target 80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-10 02:10:48.972639: W external/xla/xla/stream_executor/cuda/subprocess_compilation.cc:237] Falling back to the CUDA driver for PTX compilation; ptxas does not support CC 12.0\n",
      "2025-05-10 02:10:48.972654: W external/xla/xla/stream_executor/cuda/subprocess_compilation.cc:240] Used ptxas at /usr/local/cuda/bin/ptxas\n",
      "2025-05-10 02:10:48.972697: W external/xla/xla/stream_executor/gpu/redzone_allocator_kernel_cuda.cc:135] UNIMPLEMENTED: /usr/local/cuda/bin/ptxas ptxas too old. Falling back to the driver to compile.\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[alloc] step 1: 1722 MB  (10.6%)\n",
      "[alloc] step 2: 2722 MB  (16.7%)\n",
      "[alloc] step 3: 3722 MB  (22.8%)\n",
      "[alloc] step 4: 4722 MB  (29.0%)\n",
      "[alloc] step 5: 5722 MB  (35.1%)\n",
      "[alloc] step 6: 6722 MB  (41.2%)\n",
      "[alloc] step 7: 7722 MB  (47.4%)\n",
      "[alloc] step 8: 8722 MB  (53.5%)\n",
      "[alloc] step 9: 9722 MB  (59.6%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 20 samples per chain. Reliable r-hat and ESS diagnostics require longer chains for accurate estimate.\n",
      "2025-05-10 02:11:04,351 - pymc.sampling.mcmc - WARNING - Only 20 samples per chain. Reliable r-hat and ESS diagnostics require longer chains for accurate estimate.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[alloc] step 10: 10722 MB  (65.8%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/pymc/sampling/jax.py:475: UserWarning: There are not enough devices to run parallel chains: expected 4 but got 1. Chains will be drawn sequentially. If you are running MCMC in CPU, consider using `numpyro.set_host_device_count(4)` at the beginning of your program. You can double-check how many devices are available in your system using `jax.local_device_count()`.\n",
      "  pmap_numpyro = MCMC(\n",
      "sample: 100%|██████████| 40/40 [03:21<00:00,  5.03s/it, 1023 steps of size 5.88e-03. acc. prob=0.77]\n",
      "sample: 100%|██████████| 40/40 [02:57<00:00,  4.44s/it, 1023 steps of size 2.00e-03. acc. prob=0.95]\n",
      "sample: 100%|██████████| 40/40 [03:27<00:00,  5.19s/it, 1023 steps of size 7.13e-03. acc. prob=0.89]\n",
      "sample: 100%|██████████| 40/40 [02:55<00:00,  4.40s/it, 1023 steps of size 2.06e-03. acc. prob=0.94]\n",
      "The number of samples is too small to check convergence reliably.\n",
      "2025-05-10 02:23:49,374 - pymc.stats.convergence - INFO - The number of samples is too small to check convergence reliably.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔︎ saved model → data/models/saved_models/exitvelo_hmc.nc\n",
      "✔︎ saved preprocessor → data/models/saved_models/preprocessor.joblib\n",
      "✔︎ wrote global effects → data/models/saved_models/global_effects.json\n",
      "✅ training complete – artefacts written:\n",
      "   • data/models/saved_models/exitvelo_hmc.nc\n",
      "   • data/models/saved_models/posterior_summary.parquet\n",
      "   • data/models/saved_models/preprocessor.joblib\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/models/hierarchical.py\n",
    "\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import json\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from src.features.preprocess import transform_preprocessor\n",
    "\n",
    "from src.utils.jax_memory_fix_module import apply_jax_memory_fix\n",
    "apply_jax_memory_fix(fraction=0.45, preallocate=False)\n",
    "\n",
    "import jax, jaxlib\n",
    "import logging\n",
    "import pymc.sampling.jax as pmjax\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from tqdm.auto import tqdm\n",
    "from jax.lib import xla_bridge\n",
    "\n",
    "from src.utils.jax_gpu_utils import log_gpu_diagnostics\n",
    "from src.utils.jax_memory_monitor import (\n",
    "    monitor_memory_usage,\n",
    "    take_memory_snapshot,\n",
    "    print_memory_snapshot,\n",
    "    force_allocation_if_needed,\n",
    "    generate_memory_report,\n",
    ")\n",
    "\n",
    "log_gpu_diagnostics()\n",
    "\n",
    "@contextmanager\n",
    "def _timed_section(label: str):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"[{label}] finished in {time.time() - t0:,.1f} s\")\n",
    "\n",
    "USE_JAX = True\n",
    "try:\n",
    "    print(f\"🔍 JAX module: {jax!r}\")\n",
    "    print(f\"🔍 JAX path:   {getattr(jax, '__file__', 'builtin')}\")\n",
    "    if not hasattr(jax, \"__version__\"):\n",
    "        raise ImportError(\"jax.__version__ missing—possible circular import\")\n",
    "    print(f\"✅ JAX version: {jax.__version__}\")\n",
    "    jax.config.update(\"jax_enable_x64\", True)\n",
    "    log_gpu_diagnostics()\n",
    "    print(\"🔍 PyMC version:\", pm.__version__)\n",
    "    print(\"🔍 JAX backend:\", xla_bridge.get_backend().platform)\n",
    "except Exception as e:\n",
    "    USE_JAX = False\n",
    "    print(f\"⚠️  Warning: could not import/configure JAX ({e}). Falling back to CPU sampling.\")\n",
    "\n",
    "def fit_bayesian_hierarchical(\n",
    "    df_raw,\n",
    "    transformer,\n",
    "    batter_idx,\n",
    "    level_idx,\n",
    "    season_idx,\n",
    "    pitcher_idx,\n",
    "    *,\n",
    "    feature_list=None,\n",
    "    inference=\"nuts\",\n",
    "    minibatch_size=2_048,\n",
    "    draws=1_000,\n",
    "    tune=1_000,\n",
    "    chains=4,\n",
    "    target_accept=0.9,\n",
    "    floatx=\"float32\",\n",
    "    nuts_chain_method=\"parallel\",\n",
    "    monitor_memory: bool = False,\n",
    "    force_memory_allocation: bool = False,\n",
    "    allocation_target: float = 0.8,\n",
    "    **kwargs,\n",
    "):\n",
    "    # 0. Prepare data\n",
    "    X_all, y_ser = transform_preprocessor(df_raw, transformer)\n",
    "    names        = transformer.get_feature_names_out()\n",
    "    mask         = np.isin(names, feature_list or names)\n",
    "    X_sel        = X_all[:, mask].astype(floatx)\n",
    "    y            = y_ser.values.astype(floatx)\n",
    "    N, D         = X_sel.shape\n",
    "\n",
    "    pm.floatX = floatx\n",
    "\n",
    "    # 1. Build model and run inference\n",
    "    with pm.Model() as model:\n",
    "        mu          = pm.Normal(\"mu\", 88.0, 30.0)\n",
    "        beta_level  = pm.Normal(\"beta_level\", 0, 5, shape=int(level_idx.max()) + 1)\n",
    "        beta        = pm.Normal(\"beta\",       0, 5, shape=D)\n",
    "\n",
    "        sigma_b     = pm.HalfNormal(\"sigma_b\", 10.0)\n",
    "        u           = pm.Normal(\"u\", 0, sigma_b, shape=int(batter_idx.max()) + 1)\n",
    "\n",
    "        sigma_s     = pm.HalfNormal(\"sigma_s\", 10.0)\n",
    "        v           = pm.Normal(\"v\", 0, sigma_s, shape=int(season_idx.max()) + 1)\n",
    "\n",
    "        sigma_p     = pm.HalfNormal(\"sigma_p\", 10.0)\n",
    "        p           = pm.Normal(\"p\", 0, sigma_p, shape=int(pitcher_idx.max()) + 1)\n",
    "\n",
    "        # ── NEW: model the observation noise\n",
    "        sigma_e     = pm.HalfNormal(\"sigma_e\", 10.0)\n",
    "\n",
    "        if inference == \"svi\":\n",
    "            X_mb     = pm.Minibatch(X_sel, minibatch_size, total_size=N)\n",
    "            y_mb     = pm.Minibatch(y,     minibatch_size, total_size=N)\n",
    "            theta    = (\n",
    "                mu\n",
    "                + beta_level[level_idx[:minibatch_size]]\n",
    "                + pm.math.dot(X_mb, beta)\n",
    "                + u[batter_idx[:minibatch_size]]\n",
    "                + v[season_idx[:minibatch_size]]\n",
    "                + p[pitcher_idx[:minibatch_size]]\n",
    "            )\n",
    "            pm.Normal(\"y_obs\", theta, sigma_e, observed=y_mb)\n",
    "\n",
    "            # ── memory & allocation hooks ─────────────────────────\n",
    "            if force_memory_allocation:\n",
    "                force_allocation_if_needed(allocation_target)\n",
    "            if monitor_memory:\n",
    "                take_memory_snapshot(\"before_svi_fit\")\n",
    "\n",
    "            approx = pm.fit(n=30_000, method=\"advi\", obj_optimizer=pm.adam(2e-3))\n",
    "            idata  = approx.sample(draws)\n",
    "\n",
    "            if monitor_memory:\n",
    "                take_memory_snapshot(\"after_svi_sample\")\n",
    "                generate_memory_report()\n",
    "\n",
    "        else:\n",
    "            theta = (\n",
    "                mu\n",
    "                + beta_level[level_idx]\n",
    "                + pm.math.dot(X_sel, beta)\n",
    "                + u[batter_idx]\n",
    "                + v[season_idx]\n",
    "                + p[pitcher_idx]\n",
    "            )\n",
    "            pm.Normal(\"y_obs\", theta, sigma_e, observed=y)\n",
    "\n",
    "            # ── memory & allocation hooks ─────────────────────────\n",
    "            if force_memory_allocation:\n",
    "                force_allocation_if_needed(allocation_target)\n",
    "            if monitor_memory:\n",
    "                take_memory_snapshot(\"before_nuts_sample\")\n",
    "\n",
    "            idata = pm.sample(\n",
    "                draws=draws,\n",
    "                tune=tune,\n",
    "                chains=chains,\n",
    "                target_accept=target_accept,\n",
    "                nuts_sampler=\"numpyro\",\n",
    "                nuts_sampler_kwargs={\"chain_method\": nuts_chain_method},\n",
    "                idata_kwargs={\"log_likelihood\": False, \"keep_untransformed\": False},\n",
    "                progressbar=True,\n",
    "            )\n",
    "            # ── draw posterior predictive and attach it ───────────────\n",
    "            ppc = pm.sample_posterior_predictive(\n",
    "                idata,\n",
    "                var_names=[\"y_obs\"],             # only need y_obs\n",
    "                return_inferencedata=True\n",
    "            )\n",
    "            idata.extend(ppc)                   # ← crucial!\n",
    "            if monitor_memory:\n",
    "                take_memory_snapshot(\"after_nuts_sample\")\n",
    "                generate_memory_report()\n",
    "\n",
    "    # 3. Attach feature names\n",
    "    idata.attrs[\"feature_names\"] = [n for n, m in zip(names, mask) if m]\n",
    "    return idata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from pathlib import Path\n",
    "    import pandas as pd, numpy as np, arviz as az\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.features.preprocess import (fit_preprocessor,\n",
    "                                        prepare_for_mixed_and_hierarchical)\n",
    "    # from src.models.hierarchical import fit_bayesian_hierarchical\n",
    "    from src.utils.hierarchical_utils import save_model, save_preprocessor\n",
    "    from src.utils.posterior import posterior_to_frame\n",
    "    from src.utils.bayesian_metrics import (compute_classical_metrics,\n",
    "                                            compute_bayesian_metrics,\n",
    "                                            compute_convergence_diagnostics,\n",
    "                                            compute_calibration)\n",
    "\n",
    "    from src.utils.validation import (\n",
    "        run_kfold_cv_pymc,\n",
    "        rmse_pymc,\n",
    "        posterior_predictive_check,\n",
    "    )\n",
    "    import json  # Added import for JSON operations\n",
    "\n",
    "    RAW   = Path(\"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\")\n",
    "    OUT_NC = Path(\"data/models/saved_models/exitvelo_hmc.nc\")\n",
    "    OUT_POST = Path(\"data/models/saved_models/posterior_summary.parquet\")\n",
    "    OUT_PREPROC = Path(\"data/models/saved_models/preprocessor.joblib\")\n",
    "\n",
    "    # 1 · prep\n",
    "    df = load_and_clean_data(RAW)\n",
    "    df_fe = feature_engineer(df)\n",
    "    df_model = prepare_for_mixed_and_hierarchical(df_fe)\n",
    "\n",
    "    _, _, tf = fit_preprocessor(df_model, model_type=\"linear\", debug=False)\n",
    "\n",
    "    b_idx = df_model[\"batter_id\"].cat.codes.values\n",
    "    l_idx = df_model[\"level_idx\"].values\n",
    "    s_idx = df_model[\"season_idx\"].values\n",
    "    p_idx = df_model[\"pitcher_idx\"].values\n",
    "    draws_and_tune = 20\n",
    "    target_accept=0.95\n",
    "    chains=4\n",
    "    # 2 · fit\n",
    "    idata = fit_bayesian_hierarchical(\n",
    "        df_model,\n",
    "        tf,\n",
    "        b_idx,            # batter codes\n",
    "        l_idx,            # level codes\n",
    "        s_idx,            # season codes\n",
    "        p_idx,            # pitcher codes\n",
    "        sampler=\"jax\",\n",
    "        draws=draws_and_tune,\n",
    "        tune=draws_and_tune,\n",
    "        target_accept=target_accept,\n",
    "        chains=chains,\n",
    "        monitor_memory=True,  # Enable memory monitoring\n",
    "        force_memory_allocation=True,  # Force memory allocation\n",
    "        allocation_target=0.8,  # Target 80% memory utilization\n",
    "        direct_feature_input=None  # No direct feature input for this example\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    idata.attrs[\"median_age\"] = df_model[\"age\"].median()   # ← NEW\n",
    "\n",
    "    # 3 · persist\n",
    "    save_model(idata, OUT_NC)\n",
    "    save_preprocessor(tf, OUT_PREPROC)\n",
    "    posterior_to_frame(idata).to_parquet(OUT_POST)\n",
    "    print(\"✅ training complete – artefacts written:\")\n",
    "    print(\"   •\", OUT_NC)\n",
    "    print(\"   •\", OUT_POST)\n",
    "    print(\"   •\", OUT_PREPROC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔︎ loaded model ← data/models/saved_models/exitvelo_hmc.nc\n",
      "✔︎ loaded preprocessor ← data/models/saved_models/preprocessor.joblib\n",
      "🔎 InferenceData groups: ['posterior', 'posterior_predictive', 'sample_stats', 'observed_data']\n",
      "✅ posterior_predictive present\n",
      "=== Classical Metrics ===\n",
      "▶ Classical MSE : 151.65\n",
      "▶ Classical RMSE: 12.31\n",
      "▶ Classical MAE : 9.94\n",
      "▶ Classical R²  : 0.129\n",
      "\n",
      "=== Calibration ===\n",
      "▶ Calibration: 93.91% of true values within 95% HDI\n",
      "\n",
      "=== Bayesian Metrics ===\n",
      "⚠️ InferenceData has no log_likelihood; skipping LOO/WAIC.\n",
      "\n",
      "=== Convergence Diagnostics ===\n",
      "▶ Convergence diagnostics (R̂, ESS):\n",
      "          r_hat  ess_bulk  ess_tail\n",
      "mu         2.57      7.75     17.67\n",
      "beta[0]    1.16     23.73     63.03\n",
      "beta[1]    1.03     69.70     60.55\n",
      "beta[2]    1.04     68.15     99.67\n",
      "beta[3]    1.96      8.88     36.41\n",
      "beta[4]    1.86      9.20     23.55\n",
      "beta[5]    1.85      9.23     51.29\n",
      "beta[6]    1.49     13.79     15.92\n",
      "beta[7]    2.60      7.70     20.37\n",
      "beta[8]    2.32      7.92     38.30\n",
      "beta[9]    1.99      8.86     37.26\n",
      "beta[10]   1.86      9.01     42.29\n",
      "beta[11]   1.02     90.57     95.54\n",
      "beta[12]   2.34      7.96     15.92\n",
      "beta[13]   2.36      7.94     15.92\n",
      "beta[14]   1.00    105.29     65.71\n",
      "beta[15]   2.59      7.68     21.35\n",
      "beta[16]   1.12     30.97     60.55\n",
      "beta[17]   1.02     86.81     41.73\n",
      "beta[18]   1.09    112.75     65.71\n",
      "beta[19]   0.99    121.14    117.54\n",
      "beta[20]   1.02     92.32     65.71\n",
      "beta[21]   2.95      7.45     16.24\n",
      "beta[22]   2.33      7.99     16.24\n",
      "beta[23]   1.48     11.59     52.15\n",
      "beta[24]   1.63     10.26     59.94\n",
      "beta[25]   2.94      7.52     20.02\n",
      "beta[26]   2.47      7.82     15.92\n",
      "beta[27]   2.39      7.99     20.91\n",
      "beta[28]   1.58     10.27     28.67\n",
      "beta[29]   1.84      9.08     18.49\n",
      "beta[30]   1.62     10.22     15.92\n",
      "beta[31]   2.21      8.21     15.92\n",
      "beta[32]   2.10      8.45     18.49\n",
      "beta[33]   2.88      7.50     16.24\n",
      "beta[34]   2.55      7.83     20.91\n",
      "beta[35]   1.80      9.19     15.92\n",
      "beta[36]   2.91      7.48     15.92\n",
      "beta[37]   2.40      7.89     15.92\n",
      "beta[38]   2.63      7.69     16.24\n",
      "beta[39]   1.66      9.78     25.83\n",
      "beta[40]   1.80      9.24     16.24\n",
      "beta[41]   2.17      8.26     30.69\n",
      "beta[42]   2.10      8.44     16.93\n",
      "beta[43]   1.77      9.35     30.69\n",
      "beta[44]   2.64      7.71     21.35\n",
      "beta[45]   1.74      9.55     21.35\n",
      "beta[46]   2.42      7.99     22.88\n",
      "beta[47]   2.31      8.02     15.92\n",
      "beta[48]   1.95      8.86     18.49\n",
      "beta[49]   1.45     12.02     19.62\n",
      "sigma_e    1.43     12.33     93.62\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/models/hierarchical_metrics.py\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "\n",
    "from src.utils.hierarchical_utils import load_model, load_preprocessor\n",
    "from src.features.feature_engineering import feature_engineer\n",
    "from src.features.preprocess import (\n",
    "    transform_preprocessor,\n",
    "    prepare_for_mixed_and_hierarchical,\n",
    ")\n",
    "from src.utils.bayesian_metrics import (\n",
    "    compute_classical_metrics,\n",
    "    compute_bayesian_metrics,\n",
    "    compute_convergence_diagnostics,\n",
    "    compute_calibration\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_hierarchical(\n",
    "    model_path: Path,\n",
    "    data_path: Path,\n",
    "    preproc_path: Path,\n",
    "    *,\n",
    "    use_chunks: bool = False,\n",
    "    chunk_size: int | None = None,\n",
    "):\n",
    "    \"\"\"Load saved artifacts and run all metrics.\"\"\"\n",
    "    # 1) Load artifacts\n",
    "    idata   = load_model(model_path)\n",
    "    preproc = load_preprocessor(preproc_path)\n",
    "\n",
    "    # 🔎 Debug: show which InferenceData groups we have\n",
    "    print(\"🔎 InferenceData groups:\", idata.groups())\n",
    "    if \"posterior_predictive\" in idata.groups():\n",
    "        print(\"✅ posterior_predictive present\")\n",
    "    else:\n",
    "        print(\"⚠️  posterior_predictive missing; metrics may fail\")\n",
    "\n",
    "    # 2) Grab the exact y_obs vector the model saw\n",
    "    #    This replaces any manual reconstruction that can lead to length mismatches.\n",
    "    y_true = idata.observed_data[\"y_obs\"].values\n",
    "\n",
    "    # 3) Compute and print metrics\n",
    "    print(\"=== Classical Metrics ===\")\n",
    "    compute_classical_metrics(idata, y_true)\n",
    "\n",
    "    print(\"\\n=== Calibration ===\")\n",
    "    compute_calibration(idata, y_true)\n",
    "\n",
    "    print(\"\\n=== Bayesian Metrics ===\")\n",
    "    compute_bayesian_metrics(\n",
    "        idata, use_chunks=use_chunks, chunk_size=chunk_size\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Convergence Diagnostics ===\")\n",
    "    compute_convergence_diagnostics(idata)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    DATA_PATH   = Path(\"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\")\n",
    "    OUT_POST = Path(\"data/models/saved_models/posterior_summary.parquet\")\n",
    "\n",
    "    # ─── User configuration: set these paths and flags as needed ─────────────\n",
    "    MODEL_PATH   = Path(\"data/models/saved_models/exitvelo_hmc.nc\")\n",
    "    PREPROC_PATH = Path(\"data/models/saved_models/preprocessor.joblib\")\n",
    "    USE_CHUNKS   = False       # Set to True to enable chunked LOO/WAIC\n",
    "    CHUNK_SIZE   = None        # e.g. 100_000 when USE_CHUNKS is True\n",
    "    evaluate_hierarchical(\n",
    "        MODEL_PATH,\n",
    "        DATA_PATH,\n",
    "        PREPROC_PATH,\n",
    "        use_chunks=USE_CHUNKS,\n",
    "        chunk_size=CHUNK_SIZE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   season level_abbr  batter_id  pitcher_id  exit_velo  launch_angle  \\\n",
      "0    2023        MLB        235        1335    95.7352       47.2362   \n",
      "1    2023        MLB       3182        1335    95.9380        4.7291   \n",
      "2    2023        MLB       3856        1988    89.1404      -16.2251   \n",
      "3    2023        MLB       2017        1988    88.7278       -6.8385   \n",
      "4    2023        MLB       1594        1988    89.2888        0.5079   \n",
      "\n",
      "   spray_angle  hangtime     hit_type batter_hand  ... same_hand  hand_match  \\\n",
      "0      -6.4422    6.4960     FLY_BALL           R  ...      True      R_VS_R   \n",
      "1      -4.8052    0.7806  GROUND_BALL           L  ...     False      L_VS_R   \n",
      "2      15.2382    0.0311  GROUND_BALL           S  ...     False      S_VS_R   \n",
      "3     -11.5988    0.1215  GROUND_BALL           R  ...      True      R_VS_R   \n",
      "4     -22.1899    0.3802  GROUND_BALL           R  ...      True      R_VS_R   \n",
      "\n",
      "  pitch_hand_match player_ev_mean50  player_ev_std50  pitcher_ev_mean50  \\\n",
      "0        FB_R_VS_R              NaN              NaN                NaN   \n",
      "1        OS_L_VS_R              NaN              NaN                NaN   \n",
      "2        OS_S_VS_R              NaN              NaN                NaN   \n",
      "3        BB_R_VS_R              NaN              NaN                NaN   \n",
      "4        FB_R_VS_R              NaN              NaN                NaN   \n",
      "\n",
      "  age_centered  season_centered level_idx hitter_type  \n",
      "0          6.5              2.0         2     CONTACT  \n",
      "1          2.9              2.0         2     CONTACT  \n",
      "2          3.4              2.0         2       POWER  \n",
      "3         -2.9              2.0         2     CONTACT  \n",
      "4          9.0              2.0         2     CONTACT  \n",
      "\n",
      "[5 rows x 30 columns]\n",
      "Index(['season', 'level_abbr', 'batter_id', 'pitcher_id', 'exit_velo',\n",
      "       'launch_angle', 'spray_angle', 'hangtime', 'hit_type', 'batter_hand',\n",
      "       'pitcher_hand', 'batter_height', 'pitch_group', 'outcome', 'age',\n",
      "       'age_sq', 'age_bin', 'height_diff', 'la_bin', 'spray_bin', 'same_hand',\n",
      "       'hand_match', 'pitch_hand_match', 'player_ev_mean50', 'player_ev_std50',\n",
      "       'pitcher_ev_mean50', 'age_centered', 'season_centered', 'level_idx',\n",
      "       'hitter_type'],\n",
      "      dtype='object')\n",
      "[Debug] loaded beta_level of type list: [-0.353977080992005, -0.5353907515010331, -0.8696753536598472]\n",
      "[Validation Prep] level_idx set to 2 for all rows, age_centered stats: min=-6.099999999999998, max=13.700000000000003\n",
      "[Save] predictions → data/predictions/exitvelo_predictions_2024.csv\n",
      "   season  batter_id   age hitter_type  level_idx  age_centered  batter_idx  \\\n",
      "0    2024          2  25.4       POWER          2          -1.0           2   \n",
      "1    2024          6  29.2     CONTACT          2           2.8           6   \n",
      "2    2024          9  22.3     CONTACT          2          -4.1           9   \n",
      "3    2024         11  26.8       POWER          2           0.4          11   \n",
      "4    2024         12  34.8     CONTACT          2           8.4          12   \n",
      "\n",
      "     u_mean      u_sd    u_q2.5     u_q50   u_q97.5  contrib_age  \\\n",
      "0 -1.476915  0.458188 -2.162735 -1.443566 -0.645898     0.001965   \n",
      "1  0.961216  0.561292  0.026142  1.013106  2.110992    -0.005503   \n",
      "2  0.611845  0.433431 -0.048392  0.536306  1.528437     0.008058   \n",
      "3  0.803041  0.385378  0.005060  0.898370  1.442172    -0.000786   \n",
      "4  1.105576  0.626402  0.003895  1.007161  2.587997    -0.016509   \n",
      "\n",
      "   contrib_level  contrib_u  pred_mean  pred_lo95  pred_hi95  \n",
      "0      -0.869675  -1.443566  83.985087  83.265919  84.782755  \n",
      "1      -0.869675   1.013106  86.434291  85.447327  87.532177  \n",
      "2      -0.869675   0.536306  85.971053  85.386354  86.963183  \n",
      "3      -0.869675   0.898370  86.324272  85.430962  86.868075  \n",
      "4      -0.869675   1.007161  86.417341  85.414075  87.998177  \n",
      "[Top 5] power hitters:\n",
      "     batter_id hitter_type  pred_mean  pred_lo95  pred_hi95\n",
      "65         380       POWER  87.259616  86.250926  88.789226\n",
      "17          79       POWER  87.097762  86.151007  88.055441\n",
      "100        580       POWER  86.989984  86.278739  87.869618\n",
      "139        839       POWER  86.879728  85.982018  88.178373\n",
      "295       1918       POWER  86.858473  86.226796  87.900796\n",
      "[Top 5] contact hitters:\n",
      "     batter_id hitter_type  pred_mean  pred_lo95  pred_hi95\n",
      "309       2017     CONTACT  88.075181  86.571633  89.142363\n",
      "325       2139     CONTACT  87.456491  86.494285  88.664693\n",
      "113        673     CONTACT  87.438199  86.439856  88.703100\n",
      "140        842     CONTACT  87.417448  86.563134  88.700259\n",
      "249       1594     CONTACT  87.397205  86.197671  88.627086\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/models/hierarchical_predict.py\n",
    "\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.posterior import align_batter_codes\n",
    "from src.features.feature_engineering import feature_engineer\n",
    "\n",
    "\n",
    "# ─── new helper at top of file ────────────────────────────────────────────\n",
    "def get_top_hitters(\n",
    "    df: pd.DataFrame,\n",
    "    hitter_col: str = \"hitter_type\",\n",
    "    n: int = 5,\n",
    "    verbose: bool = False\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Given a DataFrame with predictions and a 'hitter_type' column,\n",
    "    return two DataFrames of the top-n POWER and CONTACT hitters by pred_mean.\n",
    "    If the column is missing, returns two empty DataFrames (and prints a warning).\n",
    "    \"\"\"\n",
    "    if hitter_col not in df.columns:\n",
    "        if verbose:\n",
    "            print(f\"[Warning] Column '{hitter_col}' not found; skipping top-hitter extraction.\")\n",
    "        empty = pd.DataFrame(columns=[ \"batter_id\", hitter_col, \"pred_mean\", \"pred_lo95\", \"pred_hi95\" ])\n",
    "        return empty, empty\n",
    "\n",
    "    # 1) Filter power vs. contact, case‐insensitive\n",
    "    power_mask   = df[hitter_col].str.contains(\"POWER\",   case=False, na=False)\n",
    "    contact_mask = df[hitter_col].str.contains(\"CONTACT\", case=False, na=False)\n",
    "\n",
    "    power_df   = df.loc[power_mask]\n",
    "    contact_df = df.loc[contact_mask]\n",
    "\n",
    "    # 2) Take the top-n by pred_mean\n",
    "    top_power   = power_df.nlargest(n, \"pred_mean\")[ [\"batter_id\", hitter_col, \"pred_mean\", \"pred_lo95\", \"pred_hi95\"] ]\n",
    "    top_contact = contact_df.nlargest(n, \"pred_mean\")[ [\"batter_id\", hitter_col, \"pred_mean\", \"pred_lo95\", \"pred_hi95\"] ]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[Top {n}] power hitters:\\n{top_power}\")\n",
    "        print(f\"[Top {n}] contact hitters:\\n{top_contact}\")\n",
    "\n",
    "    return top_power, top_contact\n",
    "\n",
    "\n",
    "\n",
    "def simplified_prepare_validation(df_val: pd.DataFrame, median_age: float, verbose: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare validation dataset with simplified approach, handling missing columns gracefully.\n",
    "\n",
    "    Assumptions:\n",
    "    - All predictions are for MLB-level competition (level_idx = 2).\n",
    "    - Missing age defaults to median training age (age_centered = 0).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_val : pd.DataFrame\n",
    "        Raw validation dataframe (must include 'season', 'batter_id', optional 'age').\n",
    "    median_age : float\n",
    "        Median age computed from training data for centering.\n",
    "    verbose : bool\n",
    "        If True, prints debug information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added 'age_centered' and 'level_idx'.\n",
    "    \"\"\"\n",
    "    df_val = df_val.copy()\n",
    "\n",
    "    # Default to MLB level for all observations\n",
    "    df_val['level_idx'] = 2\n",
    "\n",
    "    # Center age\n",
    "    if 'age' in df_val.columns:\n",
    "        df_val['age_centered'] = df_val['age'] - median_age\n",
    "    else:\n",
    "        # Using median training age => zero effect\n",
    "        df_val['age_centered'] = 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[Validation Prep] level_idx set to 2 for all rows, age_centered stats: min={df_val['age_centered'].min()}, max={df_val['age_centered'].max()}\")\n",
    "\n",
    "    return df_val\n",
    "\n",
    "\n",
    "\n",
    "def predict_from_summaries(\n",
    "    *,\n",
    "    roster_csv: Path,\n",
    "    raw_csv: Path,\n",
    "    posterior_parquet: Path,\n",
    "    global_effects_json: Path,\n",
    "    output_csv: Path,\n",
    "    verbose: bool = False,\n",
    "    level_idx: int = 2,  # allow override if you ever want a different level\n",
    ") -> pd.DataFrame:\n",
    "    # --- 1) Load artefacts ------------------------------------------------\n",
    "    df_post   = pd.read_parquet(posterior_parquet)\n",
    "    df_roster = pd.read_csv(roster_csv)\n",
    "    df_raw    = pd.read_csv(raw_csv)\n",
    "\n",
    "    # --- 2) Load & validate global effects -------------------------------\n",
    "    try:\n",
    "        glob = json.loads(global_effects_json.read_text())\n",
    "    except FileNotFoundError:\n",
    "        raise ValueError(f\"Global-effects file {global_effects_json} not found\")\n",
    "\n",
    "    if not glob:\n",
    "        raise ValueError(\n",
    "            f\"{global_effects_json} is empty – missing Intercept/beta_age/beta_level\"\n",
    "        )\n",
    "\n",
    "    # --- 2a) Inspect beta_level representation (for debugging) -----------\n",
    "    raw_bl = glob.get(\"beta_level\", None)\n",
    "    if verbose:\n",
    "        print(f\"[Debug] loaded beta_level of type {type(raw_bl).__name__}: {raw_bl}\")\n",
    "\n",
    "    # --- 2b) Robust retrieval of beta_level -----------------------------\n",
    "    if isinstance(raw_bl, dict):\n",
    "        # dict may use string keys or possibly int keys\n",
    "        beta_level = raw_bl.get(str(level_idx),\n",
    "                        raw_bl.get(level_idx, 0.0))\n",
    "    elif isinstance(raw_bl, (list, tuple)):\n",
    "        # list index\n",
    "        try:\n",
    "            beta_level = raw_bl[level_idx]\n",
    "        except (IndexError, TypeError):\n",
    "            beta_level = 0.0\n",
    "    else:\n",
    "        # unrecognized format\n",
    "        beta_level = 0.0\n",
    "\n",
    "    # pull the rest\n",
    "    post_mu  = glob.get(\"mu_mean\",    0.0)\n",
    "    beta_age = glob.get(\"beta_age\",    0.0)\n",
    "    med_age  = glob.get(\n",
    "        \"median_age\",\n",
    "        float(df_raw.get('age', pd.Series()).median() or 0.0)\n",
    "    )\n",
    "\n",
    "    # --- 3) Feature engineering (unchanged) ------------------------------\n",
    "    df_fe = feature_engineer(df_raw)\n",
    "    mapping = (\n",
    "        df_fe[df_fe['season']==2023][['batter_id','hitter_type']]\n",
    "             .dropna()\n",
    "             .groupby('batter_id')['hitter_type']\n",
    "             .agg(lambda s: s.mode().iloc[0] if not s.mode().empty else s.iloc[0])\n",
    "             .to_dict()\n",
    "    )\n",
    "    df_roster['hitter_type'] = df_roster['batter_id'].map(mapping).fillna(\"UNKNOWN\")\n",
    "\n",
    "    # --- 4) Validation prep & merge --------------------------------------\n",
    "    df_val = simplified_prepare_validation(df_roster, med_age, verbose)\n",
    "    df_val['batter_idx'] = align_batter_codes(df_val, df_post['batter_idx'])\n",
    "    df = df_val.merge(df_post, on='batter_idx', how='left')\n",
    "\n",
    "    # --- 5) Compute contributions & predictions --------------------------\n",
    "    df['contrib_age']   = beta_age   * df['age_centered']\n",
    "    df['contrib_level'] = beta_level\n",
    "    df['contrib_u']     = df['u_q50']\n",
    "\n",
    "    df['pred_mean'] = post_mu + (\n",
    "        df['contrib_age'] + df['contrib_level'] + df['contrib_u']\n",
    "    )\n",
    "    df['pred_lo95'] = post_mu + df['contrib_age'] + df['contrib_level'] + df['u_q2.5']\n",
    "    df['pred_hi95'] = post_mu + df['contrib_age'] + df['contrib_level'] + df['u_q97.5']\n",
    "\n",
    "    # --- 6) Persist with auto-mkdir + locked column order ---------------\n",
    "    output_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cols = [\n",
    "        \"season\",\"batter_id\",\"age\",\"hitter_type\",\"level_idx\",\"age_centered\",\n",
    "        \"batter_idx\",\"u_q2.5\",\"u_q50\",\"u_q97.5\",\n",
    "        \"contrib_age\",\"contrib_level\",\"contrib_u\",\n",
    "        \"pred_mean\",\"pred_lo95\",\"pred_hi95\"\n",
    "    ]\n",
    "    df[cols].to_csv(output_csv, index=False)\n",
    "    if verbose:\n",
    "        print(f\"[Save] predictions → {output_csv}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from src.utils.validation import (\n",
    "        prediction_interval,\n",
    "        bootstrap_prediction_interval,\n",
    "    )\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "\n",
    "    BASE    = Path('data/models/saved_models')\n",
    "    SUMMARY = BASE / 'posterior_summary.parquet'\n",
    "    GLOBAL  = BASE / 'global_effects.json'\n",
    "    ROSTER  = Path('data/Research Data Project/Research Data Project/exit_velo_validate_data.csv')\n",
    "    RAW     = Path('data/Research Data Project/Research Data Project/exit_velo_project_data.csv')  # your training file\n",
    "    OUT     = Path('data/predictions/exitvelo_predictions_2024.csv')\n",
    "\n",
    "    # load raw data\n",
    "    df_raw = pd.read_csv(RAW)\n",
    "    df_fe    = feature_engineer(df_raw)\n",
    "    print(df_fe.head())\n",
    "    print(df_fe.columns)\n",
    "    # load posterior summary\n",
    "    df_post = pd.read_parquet(SUMMARY)\n",
    "    # load global effects\n",
    "    glob = json.loads(GLOBAL.read_text())\n",
    "    \n",
    "\n",
    "    # 1) Generate predictions + CI columns\n",
    "    predict_df = predict_from_summaries(\n",
    "        roster_csv=ROSTER,\n",
    "        raw_csv=RAW, \n",
    "        posterior_parquet=SUMMARY,\n",
    "        global_effects_json=GLOBAL,\n",
    "        output_csv=OUT,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(predict_df.head())\n",
    "\n",
    "    # 2) Empirical RMSE if 'exit_velo' present\n",
    "    df_val = pd.read_csv(ROSTER)\n",
    "    if 'exit_velo' in df_val.columns:\n",
    "        y_true = df_val['exit_velo'].values\n",
    "        y_pred = predict_df['pred_mean'].values\n",
    "        rmse_val = np.sqrt(np.mean((y_pred - y_true)**2))\n",
    "        print(\"\\n--- empirical RMSE on validation set ---\", rmse_val)\n",
    "\n",
    "    # 3) Prepare a preds DataFrame for CI routines\n",
    "    preds = predict_df['pred_mean'].to_frame(name='pred')\n",
    "    lo95  = predict_df['pred_lo95'].values\n",
    "    hi95  = predict_df['pred_hi95'].values\n",
    "\n",
    "\n",
    "    # 4) Safely extract top hitters (requires a 'hitter_type' column)\n",
    "    top_power, top_contact = get_top_hitters(predict_df, hitter_col=\"hitter_type\", n=5, verbose=True)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/utils/shap_utils.py\n",
    "\n",
    "\"\"\"\n",
    "Utilities for safely handling SHAP value calculations.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "def safe_shap_values(model, X, feature_names=None, max_features=100, sample_size=None):\n",
    "    \"\"\"\n",
    "    Safely calculate SHAP values for a model, handling common issues:\n",
    "    1. Too many features causing memory or dimension mismatch errors\n",
    "    2. NaN values in data\n",
    "    3. Different model types requiring different explainers\n",
    "    4. XGBoost dimension mismatch errors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator\n",
    "        The trained model\n",
    "    X : pandas DataFrame or numpy array\n",
    "        The input data for SHAP value calculation\n",
    "    feature_names : list, optional\n",
    "        Feature names to use. If None, will use X.columns if X is a DataFrame\n",
    "    max_features : int, default=100\n",
    "        Maximum number of features to include in SHAP calculation\n",
    "    sample_size : int, optional\n",
    "        If provided, will sample this many rows from X to reduce computation time\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (shap_values, expected_value) or (None, None) if SHAP values couldn't be calculated\n",
    "    \"\"\"\n",
    "    import shap\n",
    "    \n",
    "    # Convert X to DataFrame if it's not already\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        if feature_names is not None:\n",
    "            X = pd.DataFrame(X, columns=feature_names)\n",
    "        else:\n",
    "            X = pd.DataFrame(X)\n",
    "    \n",
    "    # Handle missing values\n",
    "    if X.isna().any().any():\n",
    "        X = X.fillna(X.mean())\n",
    "    \n",
    "    # Sample if requested\n",
    "    if sample_size is not None and len(X) > sample_size:\n",
    "        X = X.sample(sample_size, random_state=42)\n",
    "    \n",
    "    # Get feature names\n",
    "    if feature_names is None:\n",
    "        feature_names = X.columns.tolist()\n",
    "    \n",
    "    # Limit features if there are too many\n",
    "    if len(feature_names) > max_features:\n",
    "        # Try to get feature importance if available\n",
    "        if hasattr(model, 'feature_importances_') and len(model.feature_importances_) == len(feature_names):\n",
    "            importance = pd.Series(model.feature_importances_, index=feature_names)\n",
    "            top_features = importance.sort_values(ascending=False).head(max_features).index.tolist()\n",
    "        elif hasattr(model, 'get_booster') and hasattr(model.get_booster(), 'get_score'):\n",
    "            # XGBoost models\n",
    "            importance_dict = model.get_booster().get_score(importance_type='gain')\n",
    "            # Map feature indices to names if needed\n",
    "            if all(k.startswith('f') for k in importance_dict.keys()):\n",
    "                importance_dict = {feature_names[int(k[1:])]: v for k, v in importance_dict.items() \n",
    "                                   if k[1:].isdigit() and int(k[1:]) < len(feature_names)}\n",
    "            importance = pd.Series(importance_dict)\n",
    "            top_features = importance.sort_values(ascending=False).head(max_features).index.tolist()\n",
    "        else:\n",
    "            # No feature importance available, just take the first max_features\n",
    "            top_features = feature_names[:max_features]\n",
    "        \n",
    "        X = X[top_features]\n",
    "        feature_names = top_features\n",
    "    \n",
    "    # Try to extract actual model from pipeline\n",
    "    actual_model = model\n",
    "    if hasattr(model, 'steps'):\n",
    "        actual_model = model.steps[-1][1]\n",
    "    \n",
    "    # Determine the right explainer to use\n",
    "    try:\n",
    "        if hasattr(actual_model, 'get_booster'):\n",
    "            # XGBoost model - try different approaches to handle dimension mismatches\n",
    "            try:\n",
    "                # First attempt: Default behavior (normal tree_limit)\n",
    "                explainer = shap.TreeExplainer(actual_model, X)\n",
    "                shap_values = explainer.shap_values(X)\n",
    "                return shap_values, explainer.expected_value\n",
    "            except Exception as e1:\n",
    "                try:\n",
    "                    # Second attempt: Manually set a tree_limit\n",
    "                    n_trees = len(actual_model.get_booster().get_dump())\n",
    "                    tree_limit = min(100, n_trees)  # Limit to at most 100 trees\n",
    "                    explainer = shap.TreeExplainer(actual_model, X)\n",
    "                    shap_values = explainer.shap_values(X, tree_limit=tree_limit)\n",
    "                    return shap_values, explainer.expected_value\n",
    "                except Exception as e2:\n",
    "                    try:\n",
    "                        # Third attempt: Use the original_model approach\n",
    "                        class ModelWrapper:\n",
    "                            def __init__(self, model):\n",
    "                                self.original_model = model\n",
    "                                self.model_output = \"raw\"\n",
    "\n",
    "                        explainer = shap.TreeExplainer(ModelWrapper(actual_model))\n",
    "                        # Use model.predict instead of shap_values directly\n",
    "                        shap_values = explainer(X)\n",
    "                        return shap_values.values, shap_values.base_values\n",
    "                    except Exception as e3:\n",
    "                        print(f\"All XGBoost SHAP attempts failed:\")\n",
    "                        print(f\"Attempt 1: {e1}\")\n",
    "                        print(f\"Attempt 2: {e2}\")\n",
    "                        print(f\"Attempt 3: {e3}\")\n",
    "                        return None, None\n",
    "                        \n",
    "        elif hasattr(actual_model, 'predict'):\n",
    "            # Generic ML model - use Kernel explainer as fallback\n",
    "            if hasattr(actual_model, 'predict_proba'):\n",
    "                # For classifiers\n",
    "                explainer = shap.KernelExplainer(\n",
    "                    actual_model.predict_proba, shap.kmeans(X, min(20, len(X)))\n",
    "                )\n",
    "            else:\n",
    "                # For regressors\n",
    "                explainer = shap.KernelExplainer(\n",
    "                    actual_model.predict, shap.kmeans(X, min(20, len(X)))\n",
    "                )\n",
    "            shap_values = explainer.shap_values(X, nsamples=100)\n",
    "            return shap_values, explainer.expected_value\n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating SHAP values: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def create_shap_summary_plot(model, X, feature_names=None, max_features=10, plot_type='bar', \n",
    "                             class_names=None, show=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Create and optionally save a SHAP summary plot\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator\n",
    "        The trained model\n",
    "    X : pandas DataFrame or numpy array\n",
    "        The input data for SHAP value calculation\n",
    "    feature_names : list, optional\n",
    "        Feature names to use. If None, will use X.columns if X is a DataFrame\n",
    "    max_features : int, default=10\n",
    "        Maximum number of features to show in the plot\n",
    "    plot_type : str, default='bar'\n",
    "        Type of plot ('bar', 'violin', or 'dot')\n",
    "    class_names : list, optional\n",
    "        For classification, the class names to display\n",
    "    show : bool, default=True\n",
    "        Whether to display the plot\n",
    "    save_path : str, optional\n",
    "        If provided, will save the plot to this path\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (shap_values, expected_value) that were used to generate the plot\n",
    "    \"\"\"\n",
    "    import shap\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Calculate SHAP values safely\n",
    "    shap_values, expected_value = safe_shap_values(model, X, feature_names, max_features)\n",
    "    \n",
    "    if shap_values is None:\n",
    "        print(\"Could not calculate SHAP values\")\n",
    "        return None, None\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Handle plot type\n",
    "    if plot_type == 'bar':\n",
    "        shap.summary_plot(shap_values, X, plot_type='bar', show=False, max_display=max_features)\n",
    "    elif plot_type == 'violin':\n",
    "        shap.summary_plot(shap_values, X, show=False, max_display=max_features)\n",
    "    elif plot_type == 'dot':\n",
    "        shap.summary_plot(shap_values, X, plot_type='dot', show=False, max_display=max_features)\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    \n",
    "    # Show if requested\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "    \n",
    "    return shap_values, expected_value\n",
    "\n",
    "def shap_feature_dependence_plot(model, X, feature, interaction_feature=None, \n",
    "                                 show=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a SHAP dependence plot for a single feature,\n",
    "    optionally with an interaction feature\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator\n",
    "        The trained model\n",
    "    X : pandas DataFrame\n",
    "        The input data\n",
    "    feature : str\n",
    "        The feature to plot\n",
    "    interaction_feature : str, optional\n",
    "        The interaction feature to include\n",
    "    show : bool, default=True\n",
    "        Whether to display the plot\n",
    "    save_path : str, optional\n",
    "        If provided, will save the plot to this path\n",
    "    \"\"\"\n",
    "    import shap\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Calculate SHAP values safely\n",
    "    shap_values, expected_value = safe_shap_values(model, X)\n",
    "    \n",
    "    if shap_values is None:\n",
    "        print(\"Could not calculate SHAP values\")\n",
    "        return\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Handle single feature or interaction\n",
    "    if interaction_feature:\n",
    "        shap.dependence_plot(feature, shap_values, X, interaction_index=interaction_feature, show=False)\n",
    "    else:\n",
    "        shap.dependence_plot(feature, shap_values, X, show=False)\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, bbox_inches='tight', dpi=150)\n",
    "    \n",
    "    # Show if requested\n",
    "    if show:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()\n",
    "\n",
    "def shap_force_plot(model, X, row_index=0, show=True, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a SHAP force plot for a single instance\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model : estimator\n",
    "        The trained model\n",
    "    X : pandas DataFrame\n",
    "        The input data\n",
    "    row_index : int or list, default=0\n",
    "        The index of the row to explain, or a list of indices for multiple rows\n",
    "    show : bool, default=True\n",
    "        Whether to display the plot\n",
    "    save_path : str, optional\n",
    "        If provided, will save the plot to this path\n",
    "    \"\"\"\n",
    "    import shap\n",
    "    import matplotlib.pyplot as plt\n",
    "    \n",
    "    # Calculate SHAP values safely\n",
    "    shap_values, expected_value = safe_shap_values(model, X)\n",
    "    \n",
    "    if shap_values is None:\n",
    "        print(\"Could not calculate SHAP values\")\n",
    "        return\n",
    "    \n",
    "    # Convert single index to list if needed\n",
    "    if isinstance(row_index, int):\n",
    "        row_indices = [row_index]\n",
    "    else:\n",
    "        row_indices = row_index\n",
    "    \n",
    "    # Create the force plot\n",
    "    force_plot = shap.force_plot(\n",
    "        expected_value, \n",
    "        shap_values[row_indices], \n",
    "        X.iloc[row_indices]\n",
    "    )\n",
    "    \n",
    "    # Save if requested\n",
    "    if save_path:\n",
    "        shap.save_html(save_path, force_plot)\n",
    "    \n",
    "    # Show if requested\n",
    "    if show:\n",
    "        return force_plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/dash_compat.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/dash_compat.py\n",
    "# src/utils/dash_compat.py\n",
    "import dash\n",
    "import functools\n",
    "import dash_bootstrap_components as dbc\n",
    "\n",
    "# ── 3.0 compat: if Dash.run_server() was removed, alias it to run()\n",
    "if not hasattr(dash.Dash, \"run_server\"):\n",
    "    dash.Dash.run_server = dash.Dash.run\n",
    "\n",
    "_DROPDOWN_PATCH_KEY = \"_ghadf_dropdown_patched\"\n",
    "\n",
    "def patch_dropdown_right_once() -> None:\n",
    "    \"\"\"\n",
    "    Back-compat shim: translate deprecated `right=` → `align_end=`,\n",
    "    and patch it exactly once per Python process.\n",
    "    \"\"\"\n",
    "    if getattr(dbc, _DROPDOWN_PATCH_KEY, False):\n",
    "        return\n",
    "\n",
    "    orig_init = dbc.DropdownMenu.__init__\n",
    "\n",
    "    @functools.wraps(orig_init)\n",
    "    def _init(self, *args, **kwargs):\n",
    "        if \"right\" in kwargs and \"align_end\" not in kwargs:\n",
    "            kwargs[\"align_end\"] = kwargs.pop(\"right\")\n",
    "        return orig_init(self, *args, **kwargs)\n",
    "\n",
    "    dbc.DropdownMenu.__init__ = _init\n",
    "    setattr(dbc, _DROPDOWN_PATCH_KEY, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/dash_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/dash_utils.py\n",
    "\n",
    "from __future__ import annotations\n",
    "import dash\n",
    "from src.utils.dash_compat import patch_dropdown_right_once\n",
    "import pandas as pd\n",
    "import dash_bootstrap_components as dbc\n",
    "from explainerdashboard import RegressionExplainer, ExplainerDashboard\n",
    "import socket, contextlib, time\n",
    "import numpy as np\n",
    "from src.utils.shap_utils import safe_shap_values\n",
    "\n",
    "def _in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        ip = get_ipython()\n",
    "        return ip is not None and ip.has_trait(\"kernel\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Apply necessary shims at import time\n",
    "patch_dropdown_right_once()\n",
    "\n",
    "\n",
    "\n",
    "def _port_in_use(host: str, port: int) -> bool:\n",
    "    with contextlib.closing(socket.socket()) as s:\n",
    "        s.settimeout(0.001)\n",
    "        return s.connect_ex((host, port)) == 0\n",
    "\n",
    "def _get_free_port() -> int:\n",
    "    with contextlib.closing(socket.socket()) as s:\n",
    "        s.bind((\"\", 0))\n",
    "        return s.getsockname()[1]\n",
    "\n",
    "def patch_explainer_for_xgboost(explainer, X_df, debug=False):\n",
    "    \"\"\"\n",
    "    Apply patches to the explainer to handle XGBoost SHAP value calculation issues.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    explainer : RegressionExplainer\n",
    "        The explainer to patch\n",
    "    X_df : pd.DataFrame\n",
    "        The feature dataframe\n",
    "    debug : bool, default=False\n",
    "        Whether to print debug information\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    bool\n",
    "        True if patched successfully, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import shap\n",
    "        from types import MethodType\n",
    "        \n",
    "        # 1. Override the default get_shap_values_df method\n",
    "        def patched_get_shap_values_df(self, *args, **kwargs):\n",
    "            if debug:\n",
    "                print(\"Using patched get_shap_values_df method\")\n",
    "            \n",
    "            if hasattr(self, '_shap_values_df') and self._shap_values_df is not None:\n",
    "                return self._shap_values_df\n",
    "            \n",
    "            # Extract the model from pipeline if needed\n",
    "            model = self.model\n",
    "            if hasattr(model, 'steps'):\n",
    "                model = model.steps[-1][1]\n",
    "            \n",
    "            # Use our safe implementation\n",
    "            shap_values, expected_value = safe_shap_values(\n",
    "                model, \n",
    "                X_df,\n",
    "                max_features=min(100, X_df.shape[1]),\n",
    "                sample_size=min(1000, len(X_df))\n",
    "            )\n",
    "            \n",
    "            if shap_values is None:\n",
    "                if debug:\n",
    "                    print(\"Failed to calculate SHAP values, falling back to permutation importance\")\n",
    "                # Fall back to permutation importance for feature importance\n",
    "                self._shap_values_df = None\n",
    "                return pd.DataFrame(0, index=X_df.index, columns=X_df.columns)\n",
    "            \n",
    "            if debug:\n",
    "                print(f\"SHAP values shape: {np.array(shap_values).shape}\")\n",
    "            \n",
    "            # Handle different explainer output formats\n",
    "            if hasattr(shap_values, 'values'):  # New SHAP output format\n",
    "                shap_df = pd.DataFrame(shap_values.values, columns=X_df.columns, index=X_df.index)\n",
    "            else:  # Old-style output\n",
    "                shap_df = pd.DataFrame(shap_values, columns=X_df.columns, index=X_df.index)\n",
    "            \n",
    "            # Cache the result\n",
    "            self._shap_values_df = shap_df\n",
    "            self.expected_value = expected_value\n",
    "            \n",
    "            return shap_df\n",
    "        \n",
    "        # 2. Apply the patch\n",
    "        explainer.get_shap_values_df = MethodType(patched_get_shap_values_df, explainer)\n",
    "        \n",
    "        if debug:\n",
    "            print(\"Successfully patched explainer for XGBoost compatibility\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        if debug:\n",
    "            print(f\"Failed to patch explainer: {e}\")\n",
    "        return False\n",
    "\n",
    "def launch_explainer_dashboard(\n",
    "    pipeline,\n",
    "    preprocessor,\n",
    "    X_raw: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    *,\n",
    "    cats=None,\n",
    "    descriptions=None,\n",
    "    title: str = \"Model Explainer\",\n",
    "    bootstrap=dbc.themes.FLATLY,\n",
    "    inline: bool | None = None,\n",
    "    host: str = \"127.0.0.1\",\n",
    "    port: int = 8050,\n",
    "    debug: bool = False,\n",
    "    **db_kwargs,\n",
    "):\n",
    "    \"\"\"Create & display an ExplainerDashboard.\n",
    "\n",
    "    • Inline in notebooks by default.  \n",
    "    • Auto-terminates any previous inline dashboard on the same port.  \n",
    "    • Falls back to a free port if the requested one is still busy.\n",
    "    • Uses custom SHAP implementation to avoid XGBoost dimension errors.\n",
    "    \"\"\"\n",
    "    if inline is None:\n",
    "        inline = _in_notebook()\n",
    "\n",
    "    # ---------- tidy up any existing inline server ----------\n",
    "    if inline and _port_in_use(host, port):\n",
    "        try:\n",
    "            ExplainerDashboard.terminate(port)\n",
    "        except Exception:             # pragma: no cover\n",
    "            pass                      # nothing running or unsupported version\n",
    "        # give the OS a moment to release the socket\n",
    "        for _ in range(10):\n",
    "            if not _port_in_use(host, port):\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "        else:                          # still busy after 1 s\n",
    "            port = _get_free_port()\n",
    "\n",
    "    # ---------- prepare dataset ----------\n",
    "    X_proc = preprocessor.transform(X_raw)\n",
    "    feat_names = preprocessor.get_feature_names_out()\n",
    "    X_df = pd.DataFrame(X_proc, columns=feat_names, index=X_raw.index)\n",
    "    \n",
    "    # Debug information\n",
    "    if debug:\n",
    "        print(f\"Debug - X_raw shape: {X_raw.shape}\")\n",
    "        print(f\"Debug - X_proc shape: {X_proc.shape}\")\n",
    "        print(f\"Debug - X_df shape: {X_df.shape}\")\n",
    "        print(f\"Debug - Number of features: {len(feat_names)}\")\n",
    "        print(f\"Debug - y shape: {y.shape}\")\n",
    "        print(f\"Debug - Pipeline type: {type(pipeline)}\")\n",
    "        # Check for NaN values in processed data\n",
    "        print(f\"Debug - NaN values in X_df: {X_df.isna().sum().sum()}\")\n",
    "        \n",
    "    if cats:\n",
    "        old_cats = list(cats)\n",
    "        cats = [c for c in cats if any(fn.startswith(f\"{c}_\") for fn in feat_names)]\n",
    "        if debug and len(old_cats) != len(cats):\n",
    "            print(f\"Debug - Filtered categories from {len(old_cats)} to {len(cats)}\")\n",
    "            print(f\"Debug - Original cats: {old_cats}\")\n",
    "            print(f\"Debug - Filtered cats: {cats}\")\n",
    "\n",
    "    try:\n",
    "        # Create the explainer with error handling\n",
    "        explainer_kwargs = {\n",
    "            'cats': cats or [],\n",
    "            'descriptions': descriptions or {},\n",
    "            'precision': \"float32\",\n",
    "        }\n",
    "        \n",
    "        # Add options to avoid XGBoost issues\n",
    "        reduce_dims = X_df.shape[1] > 50  # If more than 50 features, reduce dimensions\n",
    "        if reduce_dims:\n",
    "            if debug:\n",
    "                print(f\"Debug - Too many features ({X_df.shape[1]}), sampling 50 for explainer dashboard\")\n",
    "            # Extract model to check for feature importance\n",
    "            model = pipeline\n",
    "            if hasattr(pipeline, 'steps'):\n",
    "                model = pipeline.steps[-1][1]\n",
    "                \n",
    "            # Try to get feature importance to select top features\n",
    "            try:\n",
    "                if hasattr(model, 'feature_importances_'):\n",
    "                    importance = pd.Series(model.feature_importances_, index=feat_names)\n",
    "                    top_features = importance.sort_values(ascending=False).head(50).index.tolist()\n",
    "                    if debug:\n",
    "                        print(f\"Debug - Selected top 50 features by importance\")\n",
    "                elif hasattr(model, 'get_booster'):\n",
    "                    # XGBoost case\n",
    "                    importance_dict = model.get_booster().get_score(importance_type='gain')\n",
    "                    # Map feature indices to names if needed\n",
    "                    if all(k.startswith('f') for k in importance_dict.keys()):\n",
    "                        idx_to_name = {f'f{i}': name for i, name in enumerate(feat_names)}\n",
    "                        importance_dict = {idx_to_name.get(k, k): v for k, v in importance_dict.items()}\n",
    "                    importance = pd.Series(importance_dict)\n",
    "                    top_features = importance.sort_values(ascending=False).head(50).index.tolist()\n",
    "                    if debug:\n",
    "                        print(f\"Debug - Selected top 50 features by XGBoost importance\")\n",
    "                else:\n",
    "                    # No feature importance available\n",
    "                    top_features = feat_names[:50]\n",
    "                    if debug:\n",
    "                        print(f\"Debug - Selected first 50 features\")\n",
    "                        \n",
    "                # Use only top features for the explainer\n",
    "                X_df_reduced = X_df[top_features]\n",
    "                if debug:\n",
    "                    print(f\"Debug - Reduced features from {X_df.shape[1]} to {X_df_reduced.shape[1]}\")\n",
    "                    \n",
    "                # Update X_df to use reduced features\n",
    "                X_df = X_df_reduced\n",
    "            except Exception as e:\n",
    "                if debug:\n",
    "                    print(f\"Debug - Error reducing dimensions: {e}\")\n",
    "        \n",
    "        # Create the explainer\n",
    "        explainer = RegressionExplainer(\n",
    "            pipeline,\n",
    "            X_df,\n",
    "            y,\n",
    "            **explainer_kwargs\n",
    "        )\n",
    "        \n",
    "        # Patch the explainer to avoid XGBoost issues\n",
    "        patch_explainer_for_xgboost(explainer, X_df, debug=debug)\n",
    "        \n",
    "        # Fix merged columns\n",
    "        explainer.merged_cols = explainer.merged_cols.intersection(X_df.columns)\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Debug - Explainer created successfully\")\n",
    "            print(f\"Debug - Merged cols: {len(explainer.merged_cols)}\")\n",
    "        \n",
    "        # Disable shap interaction if too many features\n",
    "        if X_df.shape[1] > 20 and 'shap_interaction' not in db_kwargs:\n",
    "            db_kwargs['shap_interaction'] = False\n",
    "            if debug:\n",
    "                print(f\"Debug - Disabled shap_interaction due to high feature count\")\n",
    "        \n",
    "        # ---------- launch dashboard ----------\n",
    "        if inline:\n",
    "            ExplainerDashboard(\n",
    "                explainer,\n",
    "                title=title,\n",
    "                bootstrap=bootstrap,\n",
    "                mode=\"inline\",\n",
    "                **db_kwargs,\n",
    "            ).run(host=host, port=port)\n",
    "        else:\n",
    "            ExplainerDashboard(\n",
    "                explainer,\n",
    "                title=title,\n",
    "                bootstrap=bootstrap,\n",
    "                **db_kwargs,\n",
    "            ).run(host=host, port=port)\n",
    "            \n",
    "    except Exception as e:\n",
    "        # Print more helpful error information\n",
    "        print(f\"ERROR: Failed to create dashboard: {type(e).__name__}: {str(e)}\")\n",
    "        if debug:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.features.preprocess import transform_preprocessor\n",
    "    from src.utils.gbm_utils import load_pipeline\n",
    "    from src.data.ColumnSchema import _ColumnSchema\n",
    "\n",
    "    # 1) load your trained pipeline + preprocessor\n",
    "    model_pipeline, preprocessor = load_pipeline(\"data/models/saved_models/gbm_pipeline.joblib\")\n",
    "\n",
    "    # 2) load & prepare a small sample\n",
    "    df_raw = load_and_clean_data(\n",
    "        \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    ).sample(200, random_state=42)\n",
    "    df_fe  = feature_engineer(df_raw)\n",
    "    X_raw = df_fe.drop(columns=[\"exit_velo\"])\n",
    "    y_raw = df_fe[\"exit_velo\"]\n",
    "\n",
    "    # 3) category grouping helper\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    # 4) launch the dashboard on port 8050\n",
    "    launch_explainer_dashboard(\n",
    "        pipeline      = model_pipeline,\n",
    "        preprocessor  = preprocessor,\n",
    "        X_raw         = X_raw,\n",
    "        y             = y_raw,\n",
    "        cats          = cols.nominal(),\n",
    "        descriptions  = {c: c for c in preprocessor.get_feature_names_out()},\n",
    "        whatif        = True,\n",
    "        shap_interaction = False,\n",
    "        hide_wizard     = True,\n",
    "        debug           = True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marlins-ds-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
