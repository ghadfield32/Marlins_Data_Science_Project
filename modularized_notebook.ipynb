{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tree:\n",
    ".\n",
    "├── data/\n",
    "│   └── Research Data Project/\n",
    "│       └── exit_velo_project_data.csv\n",
    "├── src/\n",
    "│   ├── data/\n",
    "│   │   ├── load_data.py\n",
    "│   │   └── ColumnSchema.py\n",
    "│   ├── features/\n",
    "│   │   ├── data_prep.py\n",
    "│   │   ├── eda.py\n",
    "│   │   ├── feature_engineering.py\n",
    "│   │   ├── feature_selection.py\n",
    "│   │   ├── preprocess.py\n",
    "│   │   └── __init__.py\n",
    "│   ├── models/\n",
    "│   │   ├── bayesian_alternatives.py\n",
    "│   │   ├── gbm.py\n",
    "│   │   ├── hierarchical.py\n",
    "│   │   ├── linear.py\n",
    "│   │   ├── mixed.py\n",
    "│   │   └── hierarchical_predict.py\n",
    "│   ├── utils/\n",
    "│   │   ├── bayesian_explainability.py\n",
    "│   │   ├── bayesian_metrics.py\n",
    "│   │   ├── gbm_utils.py\n",
    "│   │   ├── hierarchical_utils.py\n",
    "│   │   ├── jax_gpu_utils.py\n",
    "│   │   ├── jax_memory_monitor.py\n",
    "│   │   ├── posterior.py\n",
    "│   │   ├── validation.py\n",
    "│   │   └── __init__.py\n",
    "│   └── train.py\n",
    "└── requirements.txt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "src/data\n",
    "\n",
    "    load_data.py\n",
    "    • load_data: Reads the raw CSV of exit-velo observations.\n",
    "    • clean_raw: Drops rows missing the target (exit_velo) or key columns, reports remaining nulls.\n",
    "    • load_and_clean_data: Convenience wrapper that runs both in sequence.\n",
    "    Purpose: Centralize raw-data ingestion and initial cleanup so downstream code always sees a non-mutated, null-filtered DataFrame.\n",
    "\n",
    "    ColumnSchema.py\n",
    "    • Defines _ColumnSchema, with lists of ID, ordinal, nominal, numerical, and target columns.\n",
    "    • Exposes methods like .numerical(), .categorical(), .model_features() for type-safe feature lists.\n",
    "    Purpose: Avoid hard-coding column names everywhere—everything downstream (preprocessing, modeling) refers to this single source of truth.\n",
    "\n",
    "src/features\n",
    "\n",
    "    feature_engineering.py\n",
    "    • Pure functions that add derived columns:\n",
    "    – binned features (age_bin, la_bin, spray_bin)\n",
    "    – rolling, lagged statistics (player_ev_mean50, etc.)\n",
    "    – categorical flags (same_hand, hitter_type)\n",
    "    Purpose: Enrich raw data with predictive covariates while avoiding leakage.\n",
    "\n",
    "    eda.py\n",
    "    • Quick checks and diagnostics: null summaries, correlation tables, ANOVA for league-level effects, LOWESS age trends, Durbin–Watson tests, distribution plots, etc.\n",
    "    Purpose: Exploratory-data-analysis helpers to understand data shape, spot red flags (small samples, outliers), and guide model design (e.g. hierarchical effects).\n",
    "\n",
    "    data_prep.py\n",
    "    • Low-level utilities for domain-specific filtering & clipping:\n",
    "    – compute_clip_bounds\n",
    "    – clip_extreme_ev (trim 1st/99th-pct EVs)\n",
    "    – filter_bunts_and_popups, filter_low_event_batters, filter_physical_implausibles\n",
    "    Purpose: Apply baseball-specific rules to remove implausible or uninformative observations before modeling.\n",
    "\n",
    "    preprocess.py\n",
    "    • Builds full scikit-learn pipelines:\n",
    "    – filter_and_clip for domain cleaning\n",
    "    – fit_preprocessor/transform_preprocessor to impute, encode, scale, and preserve clipping bounds\n",
    "    – inverse_transform_preprocessor to map transformed arrays back to original features\n",
    "    – prepare_for_mixed_and_hierarchical adds category codes for mixed-effects and Bayesian models\n",
    "    Purpose: Turn your feature-engineered DataFrame into numeric arrays ready for any downstream estimator, with consistent handling of missingness and train/test separation.\n",
    "\n",
    "    feature_selection.py\n",
    "    • Baseline RandomForest (train_baseline_model)\n",
    "    • Permutation importance (compute_permutation_importance) and SHAP importance (compute_shap_importance)\n",
    "    • Helpers to threshold and intersect feature lists, plus I/O for saving/loading your final feature list\n",
    "    Purpose: Automate ranking and selection of the most predictive features before you commit to a final modeling dataset.\n",
    "\n",
    "src/models\n",
    "\n",
    "    linear.py\n",
    "    • OLS/Ridge regression wrappers with simple fit_ridge interface.\n",
    "    Purpose: Quick, interpretable baselines for exit-velo prediction.\n",
    "\n",
    "    gbm.py\n",
    "    • XGBoost regressor pipeline with GPU detection, Optuna-based tuning (tune_gbm), early stopping and save/load via gbm_utils.\n",
    "    Purpose: High-performing gradient-boosting baseline that scales to larger datasets.\n",
    "\n",
    "    mixed.py\n",
    "    • Frequentist mixed-effects model (statsmodels.MixedLM) capturing random intercepts by batter.\n",
    "    Purpose: Model batter-level variability explicitly, bridging simple regressions and full Bayesian hierarchies.\n",
    "\n",
    "    hierarchical.py\n",
    "    • Full Bayesian hierarchical model in PyMC (optionally accelerated via JAX).\n",
    "    • Priors on global mean, level effects, batter/season/pitcher random effects, plus memory-monitoring hooks.\n",
    "    Purpose: Capture multi-level structure (league, batter, season, pitcher) and quantify uncertainty via full posterior inference.\n",
    "\n",
    "    bayesian_alternatives.py\n",
    "    • Plug-and-play interfaces for other Bayesian engines: CmdStanPy, PyJAGS, NumPyro, TensorFlow-Probability, PyMC-ADVI.\n",
    "    Purpose: Compare different probabilistic back-ends under the same InferenceData API.\n",
    "\n",
    "    hierarchical_predict.py\n",
    "    • Post-hoc prediction utilities that merge saved global effects JSON and posterior summaries with a new roster to produce point forecasts and intervals for 2024.\n",
    "    Purpose: Turn your trained hierarchical model into actionable predictions on fresh data.\n",
    "\n",
    "    model_shap_reports.py\n",
    "    • Wrappers around ExplainerDashboard (Dash), Shapash, and Shapiq for interactive and static explainability reports.\n",
    "    Purpose: Provide end-to-end explainability for your chosen model.\n",
    "\n",
    "src/utils\n",
    "\n",
    "    gbm_utils.py: save/load combined GBM + preprocessor pipelines via joblib.\n",
    "\n",
    "    bayesian_explainability.py: Arviz forest plots, posterior-predictive checks, plus kernel SHAP on posterior means.\n",
    "\n",
    "    bayesian_metrics.py: Compute classical (RMSE, MAE, R²) and Bayesian model‐comparison metrics (LOO, WAIC).\n",
    "\n",
    "    hierarchical_utils.py: Save/load ArviZ InferenceData to NetCDF and preprocessor joblib.\n",
    "\n",
    "    posterior.py: Extract per-batter random‐effect summaries into a DataFrame, and write global effects out to JSON.\n",
    "\n",
    "    validation.py: K-fold CV for both sklearn estimators and PyMC models, plus prediction‐interval helpers (analytic and bootstrap).\n",
    "\n",
    "    jax_gpu_utils.py: GPU diagnostics for JAX backends (checks nvidia-smi, device list, LD_LIBRARY_PATH).\n",
    "\n",
    "    (plus memory-monitoring and JAX memory-fix modules)\n",
    "\n",
    "Purpose: Miscellaneous building blocks for model persistence, diagnostics, metrics, and workflow orchestration.\n",
    "Top-Level Scripts\n",
    "\n",
    "    train.py\n",
    "    Orchestrates a 70/30 split to fit and compare Ridge, GBM, mixed-LM, and quick Bayesian HMC, printing RMSEs side by side.\n",
    "    Purpose: Unified entry point to benchmark all four modeling paradigms on exit-velo data.\n",
    "\n",
    "Overall Pipeline Flow\n",
    "\n",
    "    Load & clean raw CSV → 2. Feature-engineer (new covariates) → 3. Preprocess (impute, encode, scale, clip) → 4. Select top features → 5. Fit model(s) → 6. Evaluate & compare metrics → 7. Explain results → 8. Predict on 2024 roster with saved summaries.\n",
    "\n",
    "Each module slots neatly into one of those stages, making your codebase both modular and extensible for future seasons or new model families."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/data/load_data.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/data/load_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path='data/Research Data Project/Research Data Project/exit_velo_project_data.csv'):\n",
    "    \"\"\"\n",
    "    Load raw exit velocity data from the data directory.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Raw exit velocity data\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "# ──  utility ─────────────────────────────────────────────────────────\n",
    "def clean_raw(df: pd.DataFrame, \n",
    "              target: str = \"exit_velo\"\n",
    "              ,debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Central place to:\n",
    "      1. Drop rows with NaN in *target*.\n",
    "      2. Print a concise null‑summary afterwards (one line per column).\n",
    "    \n",
    "    Returns a *fresh copy* (never mutates in‑place).\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Dropping rows with NaN in {target}\")\n",
    "        print(df.isna().sum())\n",
    "        \n",
    "    drop_columns = [target, \"hit_type\", \"launch_angle\"]\n",
    "    out = df.dropna(subset=drop_columns).copy()   \n",
    "\n",
    "    if debug:\n",
    "        print(f\"after rows dropped with NaN in {target}\")\n",
    "        print(out.isna().sum())\n",
    "    # quick dashboard\n",
    "    nulls = out.isna().sum()\n",
    "    non_zero = nulls[nulls > 0]\n",
    "    if non_zero.empty:\n",
    "        print(f\"✅  After target‑filter, no other nulls (n={len(out):,}).\")\n",
    "    else:\n",
    "        print(\"⚠️  Nulls after target‑filter:\")\n",
    "        for col, cnt in non_zero.items():\n",
    "            pct = cnt / len(out)\n",
    "            print(f\"  • {col:<15} {cnt:>7,} ({pct:5.2%})\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_and_clean_data(path='data/Research Data Project/Research Data Project/exit_velo_project_data.csv'\n",
    "                        ,debug: bool = False):\n",
    "    df = load_data(path)\n",
    "    df = clean_raw(df, debug = True) \n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = 'data/Research Data Project/Research Data Project/exit_velo_project_data.csv'\n",
    "\n",
    "    df = load_and_clean_data(path, debug = True)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/features/feature_engineering.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/features/feature_engineering.py\n",
    "\"\"\"Feature engineering utilities for the exit‑velo project.\n",
    "\n",
    "All helpers are pure functions that take a pandas DataFrame and return a *copy*\n",
    "with additional engineered columns so we avoid side effects.\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "# Helper functions\n",
    "###############################################################################\n",
    "def _classify_hitters_by_season(\n",
    "    df: pd.DataFrame,\n",
    "    batter_col: str = \"batter_id\",\n",
    "    season_col: str = \"season\",\n",
    "    outcome_col: str = \"outcome\",\n",
    "    power_pct: float = 0.8,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Season-by-season classification: top `power_pct` hitters by triple+HR rate\n",
    "    are 'POWER', all others 'CONTACT'. Batters with no hits default to CONTACT.\n",
    "    \"\"\"\n",
    "    # 1) restrict to actual base hits\n",
    "    hits = df[df[outcome_col].isin([\"SINGLE\",\"DOUBLE\",\"TRIPLE\",\"HOME RUN\"])].copy()\n",
    "\n",
    "    # 2) count per (season, batter)\n",
    "    counts = (\n",
    "        hits\n",
    "        .assign(\n",
    "            contact = lambda d: d[outcome_col].isin([\"SINGLE\",\"DOUBLE\"]).astype(int),\n",
    "            power   = lambda d: d[outcome_col].isin([\"TRIPLE\",\"HOME RUN\"]).astype(int),\n",
    "        )\n",
    "        .groupby([season_col, batter_col])\n",
    "        .agg(\n",
    "            total_hits   = (outcome_col, \"size\"),\n",
    "            contact_hits = (\"contact\",    \"sum\"),\n",
    "            power_hits   = (\"power\",      \"sum\"),\n",
    "        )\n",
    "    )\n",
    "    counts[\"power_rate\"]   = counts[\"power_hits\"]   / counts[\"total_hits\"]\n",
    "    counts[\"contact_rate\"] = counts[\"contact_hits\"] / counts[\"total_hits\"]\n",
    "\n",
    "    # 3) find the season‐level 80th percentile of power_rate\n",
    "    pct80 = counts.groupby(level=0)[\"power_rate\"].quantile(power_pct)\n",
    "    # map it back onto each row\n",
    "    counts[\"season_power_80\"] = counts.index.get_level_values(season_col).map(pct80)\n",
    "\n",
    "    # 4) label\n",
    "    def _label(row):\n",
    "        return \"POWER\" if row.power_rate >= row.season_power_80 else \"CONTACT\"\n",
    "\n",
    "    labels = counts.apply(_label, axis=1)\n",
    "    labels.name = \"hitter_type\"\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _rolling_stat_lagged(\n",
    "    df: pd.DataFrame,\n",
    "    group_cols: list[str],\n",
    "    target: str,\n",
    "    stat: str = \"mean\",\n",
    "    window: int = 50,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Group-wise rolling statistic using only *previous* rows.\n",
    "    For each group defined by group_cols, shift the target by 1 row \n",
    "    then compute a rolling(window) agg(stat) with min_periods=10.\n",
    "    \"\"\"\n",
    "    # 1) Within each group, shift the target by one so we only use past data\n",
    "    def shifted_rolling(x: pd.Series) -> pd.Series:\n",
    "        return x.shift(1).rolling(window=window, min_periods=10).agg(stat)\n",
    "\n",
    "    rolled = (\n",
    "        df\n",
    "        .groupby(group_cols)[target]     # group by batter_id or pitcher_id\n",
    "        .apply(shifted_rolling)          # shift & roll inside each group\n",
    "        .reset_index(level=group_cols, drop=True)  # get back a plain Series\n",
    "    )\n",
    "    return rolled\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Public API\n",
    "###############################################################################\n",
    "\n",
    "def feature_engineer(df: pd.DataFrame, copy: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Return a DataFrame enriched with engineered features (no leakage).\"\"\"\n",
    "\n",
    "    if copy:\n",
    "        df = df.copy()\n",
    "\n",
    "    # 1) Uppercase strings\n",
    "    str_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    df[str_cols] = df[str_cols].apply(lambda col: col.str.upper())\n",
    "\n",
    "    # 2) Age\n",
    "    df[\"age_sq\"]   = df[\"age\"] ** 2\n",
    "    df[\"age_bin\"]  = pd.qcut(df[\"age\"], q=4, duplicates='drop')\n",
    "\n",
    "    # 3) Height\n",
    "    avg_height    = df[\"batter_height\"].mean()\n",
    "    df[\"height_diff\"] = df[\"batter_height\"] - avg_height\n",
    "\n",
    "    # 4) Launch & spray bins\n",
    "    df[\"la_bin\"]    = pd.qcut(df[\"launch_angle\"], q=5, duplicates='drop')\n",
    "    df[\"spray_bin\"] = pd.qcut(df[\"spray_angle\"], q=3, duplicates='drop')\n",
    "\n",
    "    # 5) Handedness & matchups\n",
    "    df[\"same_hand\"]        = (df[\"batter_hand\"] == df[\"pitcher_hand\"])\n",
    "    df[\"hand_match\"]       = df[\"batter_hand\"] + \"_VS_\" + df[\"pitcher_hand\"]\n",
    "    df[\"pitch_hand_match\"] = df[\"pitch_group\"] + \"_\" + df[\"hand_match\"]\n",
    "\n",
    "    # 6) Batter lagged rolling EV stats\n",
    "    df[\"player_ev_mean50\"] = _rolling_stat_lagged(df, [\"batter_id\"], \"exit_velo\", \"mean\", 50)\n",
    "    df[\"player_ev_std50\"]  = _rolling_stat_lagged(df, [\"batter_id\"], \"exit_velo\",  \"std\", 50)\n",
    "    gm = df[\"exit_velo\"].mean(); gs = df[\"exit_velo\"].std()\n",
    "    df[\"player_ev_mean50\"].fillna(gm)\n",
    "    df[\"player_ev_std50\"].fillna(gs)\n",
    "\n",
    "    # 7) Pitcher lagged mean\n",
    "    df[\"pitcher_ev_mean50\"] = _rolling_stat_lagged(df, [\"pitcher_id\"], \"exit_velo\", \"mean\", 50)\n",
    "    df[\"pitcher_ev_mean50\"].fillna(gm)\n",
    "\n",
    "    # 8) Center covariates\n",
    "    df[\"age_centered\"]    = df[\"age\"]    - df[\"age\"].median()\n",
    "    df[\"season_centered\"] = df[\"season\"] - df[\"season\"].median()\n",
    "    df[\"level_idx\"]       = df[\"level_abbr\"].map({\"AA\":0, \"AAA\":1, \"MLB\":2})\n",
    "\n",
    "    # --- 9) New season‐by‐season batter classification ---\n",
    "    labels = _classify_hitters_by_season(df)\n",
    "    # bring labels into df by season & batter_id\n",
    "    labels_df = labels.reset_index()  # columns: [season, batter_id, hitter_type]\n",
    "    df = df.merge(labels_df, on=[\"season\",\"batter_id\"], how=\"left\")\n",
    "\n",
    "    # 10) fill anyone still NaN → they had no base hits\n",
    "    df[\"hitter_type\"] = df[\"hitter_type\"].fillna(\"CONTACT\")\n",
    "    return df\n",
    "\n",
    "###############################################################################\n",
    "# CLI entry‑point (quick smoke test)\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path, debug = True)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    print(df_fe.columns)\n",
    "    \n",
    "            \n",
    "    # --- DEBUG: batters with no classification ---\n",
    "    missing_mask = df_fe[\"hitter_type\"].isna()\n",
    "    missing_df   = df_fe[missing_mask].copy()\n",
    "    unique_b     = missing_df[\"batter_id\"].nunique()\n",
    "    print(f\"[DEBUG] {unique_b} unique batters have no label (NaN in hitter_type).\")\n",
    "\n",
    "    # --- Fix sort_values key to map each array's length ---\n",
    "    outcome_by_batter = (\n",
    "        missing_df\n",
    "        .groupby(\"batter_id\")[\"outcome\"]\n",
    "        .unique()\n",
    "        .sort_values(\n",
    "            key=lambda s: s.map(len),      # for each entry, use len(array)\n",
    "            ascending=False\n",
    "        )\n",
    "    )\n",
    "    print(\"[DEBUG] Sample of missing batters → their unique outcomes:\")\n",
    "    print(outcome_by_batter.head(10).to_dict())\n",
    "\n",
    "    print(\"[DEBUG] Outcome value counts among missing batters:\")\n",
    "    print(missing_df[\"outcome\"].value_counts().to_dict())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColumnSchema: separates raw and engineered columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/data/ColumnSchema.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/data/ColumnSchema.py\n",
    "\"\"\" \n",
    "Column schema helper for exit‑velo project.\n",
    "\n",
    "Centralises every raw and engineered column name in one place and exposes\n",
    " type‑safe accessors so downstream code never hard‑codes strings.\n",
    "\n",
    "Usage\n",
    "-----\n",
    ">>> from src.features.columns import cols\n",
    ">>> num_cols = cols.numerical()\n",
    ">>> ord_cols = cols.ordinal()\n",
    ">>> all_for_model = cols.model_features()\n",
    "\"\"\"\n",
    "\n",
    "from functools import lru_cache\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "class _ColumnSchema:\n",
    "    \"\"\"Container for canonical column lists.\n",
    "\n",
    "    Keeping everything behind methods avoids accidental mutation and lets\n",
    "    IDEs offer autocompletion (because the return type is always `List[str]`).\n",
    "    \"\"\"\n",
    "\n",
    "    _ID_COLS: List[str] = [\n",
    "        \"season\", \"batter_id\", \"pitcher_id\",\n",
    "    ]\n",
    "\n",
    "    _ORDINAL_CAT_COLS: List[str] = [\n",
    "        \"level_abbr\",   # AA < AAA < MLB\n",
    "        \"age_bin\",      # 4 quantile bins of age\n",
    "        \"la_bin\",       # 5 quantile bins of launch angle\n",
    "        \"spray_bin\",    # 3 quantile bins of spray angle\n",
    "        # \"outcome\",      # categorical outcome (out, single, double, triple, home run, sacrifice fly, sacrifice bunt, fielders choice)\n",
    "    ]\n",
    "\n",
    "\n",
    "    _NOMINAL_CAT_COLS = [\n",
    "        \"hit_type\",\n",
    "        \"pitch_group\",\n",
    "        \"batter_hand\",\n",
    "        \"pitcher_hand\",\n",
    "        \"hand_match\",\n",
    "        \"pitch_hand_match\",\n",
    "        \"same_hand\",\n",
    "        \"hitter_type\", \n",
    "    ]\n",
    "\n",
    "    _NUMERICAL_COLS = [\n",
    "        \"launch_angle\",\n",
    "        \"spray_angle\",\n",
    "        \"hangtime\",\n",
    "        \"height_diff\",\n",
    "        \"age_sq\",\n",
    "        \"age_centered\",\n",
    "        \"season_centered\",\n",
    "        \"level_idx\",\n",
    "        \"player_ev_mean50\",\n",
    "        \"player_ev_std50\",\n",
    "        \"pitcher_ev_mean50\",\n",
    "        # \"power_rate\",\n",
    "        # \"season_power_80\", \n",
    "    ]\n",
    "\n",
    "\n",
    "    _TARGET_COL: str = \"exit_velo\"\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    # Public helpers\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    def id(self) -> List[str]:\n",
    "        return self._ID_COLS.copy()\n",
    "\n",
    "    def ordinal(self) -> List[str]:\n",
    "        return self._ORDINAL_CAT_COLS.copy()\n",
    "\n",
    "    def nominal(self) -> List[str]:\n",
    "        return self._NOMINAL_CAT_COLS.copy()\n",
    "\n",
    "    def categorical(self) -> List[str]:\n",
    "        \"\"\"All cat cols (ordinal + nominal).\"\"\"\n",
    "        return self._ORDINAL_CAT_COLS + self._NOMINAL_CAT_COLS\n",
    "\n",
    "    def numerical(self) -> List[str]:\n",
    "        return self._NUMERICAL_COLS.copy()\n",
    "\n",
    "\n",
    "    def target(self) -> str:\n",
    "        return self._TARGET_COL\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    @lru_cache(maxsize=1)\n",
    "    def model_features(self) -> List[str]:\n",
    "        \"\"\"Columns fed into the ML pipeline *after* preprocess.\n",
    "\n",
    "        Excludes the target but includes derived cols.\n",
    "        \"\"\"\n",
    "        phys_minus_target = [\n",
    "            c for c in self._NUMERICAL_COLS if c != self._TARGET_COL\n",
    "        ]\n",
    "\n",
    "        return (\n",
    "            phys_minus_target\n",
    "            + self._NOMINAL_CAT_COLS\n",
    "            + self._ORDINAL_CAT_COLS  # some algos want raw string order\n",
    "        )\n",
    "\n",
    "    def all_raw(self) -> List[str]:\n",
    "        \"\"\"Returns every column expected in raw input CSV (incl. engineered).\"\"\"\n",
    "        return (\n",
    "            self._ID_COLS\n",
    "            + self._ORDINAL_CAT_COLS\n",
    "            + self._NOMINAL_CAT_COLS\n",
    "            + self._NUMERICAL_COLS\n",
    "        )\n",
    "\n",
    "    def as_dict(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Dictionary form – handy for YAML/JSON dumps.\"\"\"\n",
    "        return {\n",
    "            \"id\": self.id(),\n",
    "            \"ordinal\": self.ordinal(),\n",
    "            \"nominal\": self.nominal(),\n",
    "            \"numerical\": self.numerical(),\n",
    "            \"target\": [self.target()],\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Target column:      \", cols.target())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    print(\"\\nAs dict (JSON):\")\n",
    "    print(json.dumps(cols.as_dict(), indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/features/eda.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/features/eda.py\n",
    "\n",
    "import pandas as pd  # type: ignore\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "from scipy.stats import f_oneway, ttest_ind\n",
    "\n",
    "# Optional imports with fallbacks for advanced statistics\n",
    "try:\n",
    "    import scipy.stats as stats  # type: ignore\n",
    "    from statsmodels.nonparametric.smoothers_lowess import (\n",
    "        lowess  # type: ignore\n",
    "    )\n",
    "    from statsmodels.stats.stattools import (\n",
    "        durbin_watson  # type: ignore\n",
    "    )\n",
    "    _HAS_STATS_LIBS = True\n",
    "except ImportError:\n",
    "    _HAS_STATS_LIBS = False\n",
    "    print(\"Warning: scipy or statsmodels not available. \"\n",
    "          \"Some diagnostics will be limited.\")\n",
    "\n",
    "\n",
    "\n",
    "def get_column_groups() -> dict:\n",
    "    \"\"\"\n",
    "    Return a mapping of column-type → list of columns,\n",
    "    based on the canonical schema in src.features.feature_selection.cols.\n",
    "    \"\"\"\n",
    "    return cols.as_dict()\n",
    "\n",
    "def check_nulls(df: pd.DataFrame):\n",
    "    # Identify columns with null values\n",
    "    null_columns = df.columns[df.isnull().any()].tolist()\n",
    "    \n",
    "    # Output the columns with null values\n",
    "    if null_columns:\n",
    "        print(\"Columns with null values:\", null_columns)\n",
    "    else:\n",
    "        print(\"No columns with null values.\")\n",
    "\n",
    "\n",
    "def quick_pulse_check(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    level_col: str = \"level_abbr\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Print a quick summary table:\n",
    "      - total rows\n",
    "      - unique batters\n",
    "      - overall median exit_velo\n",
    "      - median exit_velo by level\n",
    "      - distribution of events per batter (median, 25th pct)\n",
    "      - distribution of seasons per batter\n",
    "      - pearson correlations of velo with launch_angle & hangtime\n",
    "    Returns a pd.DataFrame with those metrics.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    total_rows = len(df)\n",
    "    n_batters = df[group_col].nunique()\n",
    "    overall_med = df[velo_col].median()\n",
    "\n",
    "    # median by level\n",
    "    med_by_level = df.groupby(level_col)[velo_col].median()\n",
    "\n",
    "    # events per batter\n",
    "    ev_per = df[group_col].value_counts()\n",
    "    ev_stats = ev_per.quantile([0.25, 0.5]).to_dict()\n",
    "\n",
    "    # seasons per batter\n",
    "    seasons_per = df.groupby(group_col)[\"season\"].nunique()\n",
    "    seasons_stats = seasons_per.value_counts().sort_index().to_dict()\n",
    "\n",
    "    # basic correlations\n",
    "    corr = df[[velo_col, \"launch_angle\", \"hangtime\"]].corr()[velo_col].drop(velo_col)\n",
    "\n",
    "    # Build a summary table\n",
    "    metrics = [\n",
    "        \"Total rows\",\n",
    "        \"Unique batters\",\n",
    "        \"Overall median EV\",\n",
    "    ]\n",
    "    values = [\n",
    "        total_rows,\n",
    "        n_batters,\n",
    "        overall_med,\n",
    "    ]\n",
    "    \n",
    "    # Add level-specific metrics\n",
    "    for lvl in med_by_level.index:\n",
    "        metrics.append(f\"Median EV @ {lvl}\")\n",
    "        values.append(med_by_level[lvl])\n",
    "    \n",
    "    # Add batter event metrics\n",
    "    metrics.extend([\n",
    "        \"Events per batter (25th pct)\",\n",
    "        \"Events per batter (median)\",\n",
    "    ])\n",
    "    values.extend([\n",
    "        ev_stats.get(0.25, \"N/A\"),\n",
    "        ev_stats.get(0.5, \"N/A\"),\n",
    "    ])\n",
    "    \n",
    "    # Add season distribution\n",
    "    for season_count, count in seasons_stats.items():\n",
    "        metrics.append(f\"Batters with {season_count} season(s)\")\n",
    "        values.append(count)\n",
    "    \n",
    "    # Add correlations\n",
    "    metrics.extend([\n",
    "        \"ρ(exit_velo, launch_angle)\",\n",
    "        \"ρ(exit_velo, hangtime)\",\n",
    "    ])\n",
    "    values.extend([\n",
    "        corr.get(\"launch_angle\", \"N/A\"),\n",
    "        corr.get(\"hangtime\", \"N/A\"),\n",
    "    ])\n",
    "\n",
    "    table = pd.DataFrame({\n",
    "        \"Metric\": metrics,\n",
    "        \"Value\": values\n",
    "    })\n",
    "    \n",
    "    print(table.to_string(index=False))\n",
    "    return table\n",
    "\n",
    "\n",
    "def red_flag_small_samples(df: pd.DataFrame,\n",
    "                           group_col: str = \"batter_id\",\n",
    "                           threshold: int = 15) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Identify batters with fewer than `threshold` events.\n",
    "    Returns a Series of counts indexed by batter_id.\n",
    "    \"\"\"\n",
    "    counts = df[group_col].value_counts()\n",
    "    small = counts[counts < threshold]\n",
    "    print(f\"> Batters with fewer than {threshold} events: {len(small)}\")\n",
    "    if len(small) > 0:\n",
    "        print(f\"  First few: {', '.join(map(str, small.index[:5]))}\")\n",
    "    return small\n",
    "\n",
    "\n",
    "def red_flag_level_effect(df: pd.DataFrame,\n",
    "                          level_col: str = \"level_abbr\",\n",
    "                          velo_col: str = \"exit_velo\") -> tuple:\n",
    "    \"\"\"\n",
    "    One-way ANOVA of exit_velo across levels.\n",
    "    Returns (F-statistic, p-value) or (None, None) if scipy is not available.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> ANOVA on exit_velo by level: scipy not available\")\n",
    "        print(\"> Basic level summary instead:\")\n",
    "        summary = df.groupby(level_col)[velo_col].agg(['mean', 'std', 'count'])\n",
    "        print(summary)\n",
    "        return None, None\n",
    "    \n",
    "    groups = [\n",
    "        df[df[level_col] == lvl][velo_col].dropna()\n",
    "        for lvl in df[level_col].unique()\n",
    "    ]\n",
    "    F, p = stats.f_oneway(*groups)\n",
    "    print(f\"> ANOVA on {velo_col} by {level_col}: F={F:.3f}, p={p:.3e}\")\n",
    "    return F, p\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  REPLACE old red_flag_level_effect  → clearer name & doc\n",
    "# ------------------------------------------------------------------\n",
    "def league_level_effect(\n",
    "    df: pd.DataFrame,\n",
    "    level_col: str = \"level_abbr\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    ") -> tuple[float | None, float | None]:\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – confirms MLB vs Triple‑A (etc.) differences to\n",
    "      justify hierarchical level effects in the model.\n",
    "\n",
    "    One‑way ANOVA of `exit_velo` across `level_col`.\n",
    "    Returns (F, p) or (None, None) if SciPy unavailable.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> SciPy unavailable – falling back to group summary\")\n",
    "        print(df.groupby(level_col)[velo_col].describe())\n",
    "        return None, None\n",
    "\n",
    "    groups = [df[df[level_col] == lv][velo_col].dropna()\n",
    "              for lv in df[level_col].unique()]\n",
    "    f_val, p_val = stats.f_oneway(*groups)\n",
    "    print(f\"> Level effect ANOVA: F={f_val:.3f}, p={p_val:.3e}\")\n",
    "    return f_val, p_val\n",
    "\n",
    "\n",
    "\n",
    "def diag_age_effect(df: pd.DataFrame,\n",
    "                    age_col: str = \"age_centered\",\n",
    "                    velo_col: str = \"exit_velo\") -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    LOWESS smoothing of exit_velo vs. age_centered.\n",
    "    Returns the smoothed array or None if statsmodels is not available.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> Age effect analysis: statsmodels not available\")\n",
    "        print(\"> Basic correlation instead:\")\n",
    "        corr = df[[age_col, velo_col]].corr().iloc[0, 1]\n",
    "        print(f\"Correlation between {age_col} and {velo_col}: {corr:.3f}\")\n",
    "        return None\n",
    "    \n",
    "    # Run LOWESS smoothing\n",
    "    smooth_result = lowess(df[velo_col], df[age_col])\n",
    "    \n",
    "    # Plot the result\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.scatter(df[age_col], df[velo_col], alpha=0.1, s=1, color='gray')\n",
    "    plt.plot(\n",
    "        smooth_result[:, 0], \n",
    "        smooth_result[:, 1], \n",
    "        'r-', \n",
    "        linewidth=2, \n",
    "        label=\"LOWESS fit\"\n",
    "    )\n",
    "    plt.xlabel(age_col)\n",
    "    plt.ylabel(velo_col)\n",
    "    plt.title(\"Age effect (LOWESS)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return smooth_result\n",
    "\n",
    "\n",
    "def diag_time_series_dw(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"season\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    velo_col: str = \"exit_velo\"\n",
    ") -> pd.Series | None:\n",
    "    \"\"\"\n",
    "    Compute Durbin–Watson on each batter's time series of mean exit_velo.\n",
    "    Returns a Series of DW statistics or None if statsmodels is not available.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> Time series analysis: statsmodels not available\")\n",
    "        return None\n",
    "    \n",
    "    # Create pivot table of seasons (columns) by batters (rows)\n",
    "    pivot = (\n",
    "        df\n",
    "        .groupby([group_col, time_col])[velo_col]\n",
    "        .mean()\n",
    "        .unstack(fill_value=np.nan)\n",
    "    )\n",
    "    \n",
    "    # Only process batters with at least 3 seasons\n",
    "    valid_batters = pivot.dropna(thresh=3).index\n",
    "    if len(valid_batters) == 0:\n",
    "        print(\"> No batters with sufficient seasons for Durbin-Watson test\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate DW statistic for each valid batter\n",
    "    dw_stats = {}\n",
    "    for batter in valid_batters:\n",
    "        series = pivot.loc[batter].dropna()\n",
    "        if len(series) >= 3:  # Recheck after dropna\n",
    "            dw = durbin_watson(series)\n",
    "            dw_stats[batter] = dw\n",
    "    \n",
    "    dw_series = pd.Series(dw_stats)\n",
    "    print(\n",
    "        f\"> Mean Durbin–Watson across {len(dw_series)} batters: \"\n",
    "        f\"{dw_series.mean():.3f}\"\n",
    "    )\n",
    "    print(\"> DW < 1.5 suggests positive autocorrelation\")\n",
    "    print(\"> DW > 2.5 suggests negative autocorrelation\")\n",
    "    print(\"> DW ≈ 2.0 suggests no autocorrelation\")\n",
    "    \n",
    "    return dw_series\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  REPLACE old diag_time_series_dw WITH optional helper\n",
    "# ------------------------------------------------------------------\n",
    "def _optional_dw_check(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"season\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    ") -> pd.Series | None:\n",
    "    \"\"\"\n",
    "    (OPTIONAL) Durbin–Watson residual autocorrelation **per batter**.\n",
    "    Mostly irrelevant for cross‑sectional EV analysis but retained\n",
    "    behind a private name for power users.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        return None\n",
    "    pivot = (\n",
    "        df.groupby([group_col, time_col])[velo_col]\n",
    "          .mean().unstack()\n",
    "    )\n",
    "    stats_out = {}\n",
    "    for idx, row in pivot.dropna(thresh=3).iterrows():\n",
    "        if row.count() >= 3:\n",
    "            stats_out[idx] = durbin_watson(row.dropna())\n",
    "    if not stats_out:\n",
    "        print(\"> DW check: no eligible batters\")\n",
    "        return None\n",
    "    s = pd.Series(stats_out)\n",
    "    print(f\"DW mean={s.mean():.2f} (1.5<→pos autocorr, >2.5→neg)\")\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_red_flags(df: pd.DataFrame, \n",
    "                    sample_threshold: int = 15) -> dict:\n",
    "    \"\"\"\n",
    "    Run all red flag checks and return the results in a dictionary.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Check for small sample sizes\n",
    "    small_samples = red_flag_small_samples(df, threshold=sample_threshold)\n",
    "    results['small_samples'] = small_samples\n",
    "    \n",
    "    # Check for level effects\n",
    "    f_stat, p_val = red_flag_level_effect(df)\n",
    "    results['level_effect'] = {\n",
    "        'f_statistic': f_stat,\n",
    "        'p_value': p_val\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_distributions(df: pd.DataFrame,\n",
    "                       velo_col: str = \"exit_velo\",\n",
    "                       by: str = \"level_abbr\"):\n",
    "    \"\"\"\n",
    "    Histogram of `velo_col` faceted by `by`.\n",
    "    Returns the Matplotlib figure so callers can save or show it.\n",
    "    \"\"\"\n",
    "    groups = df[by].unique()\n",
    "    fig, axes = plt.subplots(len(groups), 1,\n",
    "                             figsize=(6, 2.8 * len(groups)),\n",
    "                             sharex=True)\n",
    "    for ax, grp in zip(axes, groups):\n",
    "        ax.hist(df[df[by] == grp][velo_col], bins=30, alpha=0.75)\n",
    "        ax.set_title(f\"{by} = {grp} (n={len(df[df[by] == grp])})\")\n",
    "        ax.set_xlabel(velo_col)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_correlations(df: pd.DataFrame, cols: list[str]):\n",
    "    \"\"\"\n",
    "    Heat-map of Pearson correlations for `cols`.\n",
    "    \"\"\"\n",
    "    corr = df[cols].corr()\n",
    "    fig, ax = plt.subplots(figsize=(0.6 * len(cols) + 2,\n",
    "                                    0.6 * len(cols) + 2))\n",
    "    im = ax.imshow(corr, vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "    ax.set_xticks(range(len(cols)), cols, rotation=90)\n",
    "    ax.set_yticks(range(len(cols)), cols)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_time_trends(df: pd.DataFrame,\n",
    "                     time_col: str = \"season\",\n",
    "                     group_col: str = \"batter_id\",\n",
    "                     velo_col: str = \"exit_velo\",\n",
    "                     sample: int = 50):\n",
    "    \"\"\"\n",
    "    Plot mean exit-velo over time for a random sample of batters.\n",
    "    \"\"\"\n",
    "    batters = df[group_col].unique()\n",
    "    chosen = np.random.choice(batters,\n",
    "                              min(sample, len(batters)),\n",
    "                              replace=False)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    for b in chosen:\n",
    "        series = (\n",
    "            df[df[group_col] == b]\n",
    "            .groupby(time_col)[velo_col]\n",
    "            .mean()\n",
    "        )\n",
    "        ax.plot(series.index, series.values, alpha=0.3)\n",
    "    ax.set_xlabel(time_col)\n",
    "    ax.set_ylabel(velo_col)\n",
    "    ax.set_title(\"Sample batter exit-velo over time\")\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def summarize_numeric_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    numeric_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarise each numeric predictor against the target.\n",
    "\n",
    "    Returns a DataFrame indexed by feature with:\n",
    "      n          – number of non‑null pairs\n",
    "      pearson_r  – Pearson correlation coefficient\n",
    "    \"\"\"\n",
    "    # --- Pull fresh lists from the schema every time -----------------\n",
    "    groups = cols.as_dict()\n",
    "\n",
    "    if numeric_cols is None:\n",
    "        numeric_cols = groups.get(\"numerical\", [])\n",
    "\n",
    "    # --- Clean the list ---------------------------------------------\n",
    "    numeric_cols = [\n",
    "        c for c in numeric_cols\n",
    "        if c != target_col and c in df.columns      # ❶ exclude target, ❷ guard\n",
    "    ]\n",
    "\n",
    "    records = []\n",
    "    for col in numeric_cols:\n",
    "        sub = df[[col, target_col]].dropna()\n",
    "        if sub.empty:               # skip columns that are all‑NA\n",
    "            continue\n",
    "        r = sub[col].corr(sub[target_col])\n",
    "        records.append({\"feature\": col, \"n\": len(sub), \"pearson_r\": r})\n",
    "\n",
    "    result = (\n",
    "        pd.DataFrame.from_records(records)\n",
    "        .set_index(\"feature\")\n",
    "        .sort_values(\"pearson_r\", ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Numeric vs target correlations ===\")\n",
    "    print(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_numeric_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    numeric_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter plots of each numeric predictor vs the target with r‑value in title.\n",
    "    \"\"\"\n",
    "    summary = summarize_numeric_vs_target(df, numeric_cols, target_col)\n",
    "    for feature, row in summary.iterrows():\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.scatter(\n",
    "            df[feature], df[target_col],\n",
    "            alpha=0.3, s=5, edgecolors=\"none\"\n",
    "        )\n",
    "        plt.title(f\"{feature} vs {target_col}  (r = {row['pearson_r']:.2f})\")\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel(target_col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def summarize_categorical_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    cat_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\"\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each categorical feature, returns a DataFrame of:\n",
    "      count, mean, median, std of the target by category.\n",
    "    \"\"\"\n",
    "    groups = get_column_groups()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = groups.get(\"categorical\", [])\n",
    "\n",
    "    summaries: dict[str, pd.DataFrame] = {}\n",
    "    for col in cat_cols:\n",
    "        stats = (\n",
    "            df\n",
    "            .groupby(col)[target_col]\n",
    "            .agg(count=\"count\", mean=\"mean\", median=\"median\", std=\"std\")\n",
    "            .sort_values(\"count\", ascending=False)\n",
    "        )\n",
    "        print(f\"\\n=== {col} vs {target_col} summary ===\")\n",
    "        print(stats)\n",
    "        summaries[col] = stats\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def plot_categorical_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    cat_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\"\n",
    "):\n",
    "    \"\"\"\n",
    "    For each categorical feature, draw a box‑plot of the target by category.\n",
    "    \"\"\"\n",
    "    groups = get_column_groups()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = groups.get(\"categorical\", [])\n",
    "\n",
    "    for col in cat_cols:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        df.boxplot(column=target_col, by=col, vert=False,\n",
    "                   grid=False, patch_artist=True)\n",
    "        plt.title(f\"{target_col} by {col}\")\n",
    "        plt.suptitle(\"\")           # remove pandas' automatic suptitle\n",
    "        plt.xlabel(target_col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def examine_and_filter_by_sample_size(\n",
    "    df: pd.DataFrame,\n",
    "    count_col: str = \"exit_velo\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    season_col: str = \"season\",\n",
    "    percentile: float = 0.05,\n",
    "    min_count: int | None = None,\n",
    "    filter_df: bool = False,\n",
    ") -> tuple[dict[int, pd.DataFrame], pd.DataFrame | None]:\n",
    "    \"\"\"\n",
    "    For each season:\n",
    "      - compute per-batter count, mean, std of `count_col`\n",
    "      - pick cutoff: min_count if provided, else the `percentile` quantile\n",
    "      - print diagnostics\n",
    "      - plot histograms *safely* (drops NaNs first)\n",
    "    Returns:\n",
    "      - summaries: dict season → per-batter summary DataFrame\n",
    "      - filtered_df: if filter_df, the original df filtered to batters ≥ cutoff\n",
    "    \"\"\"\n",
    "    summaries: dict[int, pd.DataFrame] = {}\n",
    "    mask_keep: list[pd.Series] = []\n",
    "\n",
    "    for season, sub in df.groupby(season_col):\n",
    "        # 1) per-batter summary (count *non-NA* exit_velo)\n",
    "        summary = (\n",
    "            sub\n",
    "            .groupby(group_col)[count_col]\n",
    "            .agg(count=\"count\", mean=\"mean\", std=\"std\")\n",
    "            .sort_values(\"count\")\n",
    "        )\n",
    "        summaries[season] = summary\n",
    "\n",
    "        # 2) determine cutoff\n",
    "        cutoff = min_count if min_count is not None else int(summary[\"count\"].quantile(percentile))\n",
    "        small = summary[summary[\"count\"] < cutoff]\n",
    "        large = summary[summary[\"count\"] >= cutoff]\n",
    "\n",
    "        # 3) diagnostics\n",
    "        print(f\"\\n=== Season {season} (cutoff = {cutoff}) ===\")\n",
    "        print(f\"  small (<{cutoff} events): {len(small)} batters\")\n",
    "        print(small[[\"count\",\"mean\",\"std\"]].describe(), \"\\n\")\n",
    "        print(f\"  large (≥{cutoff} events): {len(large)} batters\")\n",
    "        print(large[[\"count\",\"mean\",\"std\"]].describe())\n",
    "\n",
    "        # 4) **safe plotting**: drop NaNs, skip if nothing to plot\n",
    "        small_means = small[\"mean\"].dropna()\n",
    "        large_means = large[\"mean\"].dropna()\n",
    "\n",
    "        if small_means.empty and large_means.empty:\n",
    "            print(f\"  ⚠️  Season {season}: no valid per-batter means to plot\")\n",
    "        else:\n",
    "            plt.figure(figsize=(8, 3))\n",
    "            if not small_means.empty:\n",
    "                plt.hist(small_means, bins=30, alpha=0.6, label=f\"n<{cutoff}\")\n",
    "            if not large_means.empty:\n",
    "                plt.hist(large_means, bins=30, alpha=0.6, label=f\"n≥{cutoff}\")\n",
    "            plt.title(f\"Season {season}: per-batter EV means\")\n",
    "            plt.xlabel(\"Mean exit_velo\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # 5) build mask to keep only large-sample batters\n",
    "        if filter_df:\n",
    "            keep_ids = large.index\n",
    "            mask_keep.append(\n",
    "                (df[season_col] == season) &\n",
    "                (df[group_col].isin(keep_ids))\n",
    "            )\n",
    "\n",
    "    # 6) combine masks and filter\n",
    "    filtered_df = None\n",
    "    if filter_df and mask_keep:\n",
    "        combined = pd.concat(mask_keep, axis=1).any(axis=1)\n",
    "        filtered_df = df[combined].copy()\n",
    "\n",
    "    return summaries, filtered_df\n",
    "\n",
    "\n",
    "\n",
    "def hypothesis_test(df, feature, target=\"exit_velo\", test_type=\"anova\"):\n",
    "    \"\"\"\n",
    "    Perform hypothesis tests for feature significance.\n",
    "    \"\"\"\n",
    "    if test_type == \"anova\":\n",
    "        groups = [df[df[feature] == cat][target] for cat in df[feature].unique()]\n",
    "        F, p = f_oneway(*groups)\n",
    "        print(f\"ANOVA: F={F:.3f}, p={p:.3e}\")\n",
    "        return F, p\n",
    "    elif test_type == \"ttest\":\n",
    "        group1 = df[df[feature] == 0][target]\n",
    "        group2 = df[df[feature] == 1][target]\n",
    "        t, p = ttest_ind(group1, group2)\n",
    "        print(f\"T-test: t={t:.3f}, p={p:.3e}\")\n",
    "        return t, p\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  NEW: robust outlier flagging\n",
    "# ------------------------------------------------------------------\n",
    "def flag_outliers_iqr(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    iqr_mult: float = 1.5,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – extreme EVs (>120 mph or <40 mph) can distort\n",
    "      skew / variance estimates used in hierarchical priors.\n",
    "\n",
    "    Returns a boolean Series (True = *suspect* outlier) using the\n",
    "    classic IQR rule: value < Q1 − k·IQR  or  > Q3 + k·IQR.\n",
    "    \"\"\"\n",
    "    q1, q3 = df[velo_col].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - iqr_mult * iqr, q3 + iqr_mult * iqr\n",
    "    mask = (df[velo_col] < lower) | (df[velo_col] > upper)\n",
    "    n = int(mask.sum())\n",
    "    print(f\"> Outlier flag ({velo_col}): {n} rows outside [{lower:.1f}, {upper:.1f}]\")\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  NEW: EV distribution summary + QQ plot\n",
    "# ------------------------------------------------------------------\n",
    "def ev_distribution_summary(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    bins: int = 40,\n",
    "):\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – confirms right‑skew & heavy‑tail nature of EV\n",
    "      so you can choose a skew‑normal or Student‑t likelihood.\n",
    "\n",
    "    Prints skew/kurtosis, shows histogram, KDE, CDF & QQ (if scipy).\n",
    "    \"\"\"\n",
    "    data = df[velo_col].dropna()\n",
    "    print(\n",
    "        f\"Skewness = {stats.skew(data):.2f},  \"\n",
    "        f\"Kurtosis = {stats.kurtosis(data, fisher=False):.2f}\"\n",
    "    )\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "    ax[0].hist(data, bins=bins, density=True, alpha=0.7)\n",
    "    data.plot(kind=\"kde\", ax=ax[0], linewidth=2)\n",
    "    ax[0].set_title(\"Histogram & KDE\")\n",
    "\n",
    "    # empirical CDF\n",
    "    ecdf_x = np.sort(data)\n",
    "    ecdf_y = np.arange(1, len(ecdf_x) + 1) / len(ecdf_x)\n",
    "    ax[1].plot(ecdf_x, ecdf_y)\n",
    "    ax[1].set_title(\"Empirical CDF\")\n",
    "\n",
    "    # QQ vs normal\n",
    "    from scipy import stats as _st\n",
    "    _st.probplot(data, dist=\"norm\", plot=ax[2])\n",
    "    ax[2].set_title(\"QQ‑plot vs Normal\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  NEW: Year/era trend diagnostic\n",
    "# ------------------------------------------------------------------\n",
    "def year_trend_ev(\n",
    "    df: pd.DataFrame,\n",
    "    season_col: str = \"season\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    ci: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – detects ball‑era shifts (e.g. 2019 “juiced”,\n",
    "      2021 “deadened”) so forecasts for 2024 use correct baseline.\n",
    "\n",
    "    Produces a table & line plot of mean/median EV per season.\n",
    "    \"\"\"\n",
    "    g = df.groupby(season_col)[velo_col]\n",
    "    stats_df = g.agg(mean=\"mean\", median=\"median\", n=\"count\")\n",
    "    print(\"\\n=== Exit‑velo by season ===\")\n",
    "    print(stats_df)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 3))\n",
    "    stats_df[\"mean\"].plot(ax=ax, marker=\"o\", label=\"Mean EV\")\n",
    "    stats_df[\"median\"].plot(ax=ax, marker=\"s\", label=\"Median EV\")\n",
    "    if ci:\n",
    "        sem = g.sem()\n",
    "        ax.fill_between(\n",
    "            stats_df.index,\n",
    "            stats_df[\"mean\"] - 1.96 * sem,\n",
    "            stats_df[\"mean\"] + 1.96 * sem,\n",
    "            alpha=0.2,\n",
    "            label=\"95% CI (mean)\"\n",
    "        )\n",
    "    ax.set_ylabel(velo_col)\n",
    "    ax.set_title(\"Seasonal trend in exit velocity\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    return stats_df, fig\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  REPLACE old red_flag_level_effect  → clearer name & doc\n",
    "# ------------------------------------------------------------------\n",
    "def league_level_effect(\n",
    "    df: pd.DataFrame,\n",
    "    level_col: str = \"level_abbr\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    ") -> tuple[float | None, float | None]:\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – confirms MLB vs Triple‑A (etc.) differences to\n",
    "      justify hierarchical level effects in the model.\n",
    "\n",
    "    One‑way ANOVA of `exit_velo` across `level_col`.\n",
    "    Returns (F, p) or (None, None) if SciPy unavailable.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> SciPy unavailable – falling back to group summary\")\n",
    "        print(df.groupby(level_col)[velo_col].describe())\n",
    "        return None, None\n",
    "\n",
    "    groups = [df[df[level_col] == lv][velo_col].dropna()\n",
    "              for lv in df[level_col].unique()]\n",
    "    f_val, p_val = stats.f_oneway(*groups)\n",
    "    print(f\"> Level effect ANOVA: F={f_val:.3f}, p={p_val:.3e}\")\n",
    "    return f_val, p_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# debugs:\n",
    "def summarize_categorical_missingness(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each categorical column (ordinal + nominal), compute:\n",
    "      - original_null_count / pct\n",
    "      - imputed_missing_count / pct\n",
    "    Safely handles pandas.Categorical by first adding 'MISSING' to its categories.\n",
    "    \"\"\"\n",
    "    cols    = _ColumnSchema()\n",
    "    cat_cols = cols.ordinal() + cols.nominal()\n",
    "    summary = []\n",
    "    n = len(df)\n",
    "\n",
    "    for col in cat_cols:\n",
    "        ser = df[col]\n",
    "        orig_null = ser.isna().sum()\n",
    "\n",
    "        # If it's a Categorical, add 'MISSING' as a valid category\n",
    "        if is_categorical_dtype(ser):\n",
    "            ser = ser.cat.add_categories(['MISSING'])\n",
    "\n",
    "        # Count rows that would become 'MISSING'\n",
    "        imputed_missing = ser.fillna('MISSING').eq('MISSING').sum()\n",
    "\n",
    "        summary.append({\n",
    "            'column': col,\n",
    "            'original_null_count':   orig_null,\n",
    "            'original_null_pct':     orig_null / n,\n",
    "            'imputed_missing_count': imputed_missing,\n",
    "            'imputed_missing_pct':   imputed_missing / n,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "\n",
    "    # --- inspect nulls in the raw data ---\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values in raw data.\")\n",
    "    else:\n",
    "        print(\"=== Raw data null counts ===\")\n",
    "        for col, cnt in null_counts.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "\n",
    "    \n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:     \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    # use list‐comprehension to drop target(s) from numerical features\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "\n",
    "    # Filter out rows with nulls in the target column(s)\n",
    "    target_cols = cols.target()\n",
    "    df_filtered = df.dropna(subset=target_cols)\n",
    "\n",
    "    # Check for nulls in the filtered data\n",
    "    null_counts_filtered = df_filtered.isnull().sum()\n",
    "    null_counts_filtered = null_counts_filtered[null_counts_filtered > 0]\n",
    "    if null_counts_filtered.empty:\n",
    "        print(\"✅  No missing values in filtered data.\")\n",
    "    else:\n",
    "        print(\"=== Filtered data null counts ===\")\n",
    "        for col, cnt in null_counts_filtered.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "\n",
    "    print(\"\\n===== check on small samples =====\")\n",
    "    summaries, _ = examine_and_filter_by_sample_size(df_filtered, percentile=0.05)\n",
    "    summaries, df_filtered = examine_and_filter_by_sample_size(\n",
    "        df_filtered, percentile=0.05, min_count=15, filter_df=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Example usage\n",
    "    print(\"\\n===== NULLS CHECK =====\")\n",
    "    check_nulls(df_fe)\n",
    "    \n",
    "    print(\"\\n===== QUICK PULSE CHECK =====\")\n",
    "    quick_pulse_check(df_fe)\n",
    "    \n",
    "    print(\"\\n===== RED FLAGS CHECK =====\")\n",
    "    check_red_flags(df_fe)\n",
    "    \n",
    "    print(\"\\n===== AGE EFFECT ANALYSIS =====\")\n",
    "    diag_age_effect(df_fe, age_col=\"age\")\n",
    "    \n",
    "    print(\"\\n===== TIME SERIES ANALYSIS =====\")\n",
    "    diag_time_series_dw(df_fe)\n",
    "    \n",
    "    print(\"\\n===== PLOTTING =====\")\n",
    "    fig1 = plot_distributions(df_fe, by=\"hit_type\")\n",
    "    fig2 = plot_correlations(df_fe, numericals)  # Using cols schema\n",
    "    fig3 = plot_time_trends(df_fe, sample=20)\n",
    "\n",
    "\n",
    "    # — Numeric features —\n",
    "    num_summary = summarize_numeric_vs_target(df_fe)\n",
    "    plot_numeric_vs_target(df_fe)\n",
    "\n",
    "    # — Categorical features —\n",
    "    cat_summary = summarize_categorical_vs_target(df_fe)\n",
    "    plot_categorical_vs_target(df_fe)\n",
    "\n",
    "    # Example: Test if age has significant effect\n",
    "    hypothesis_test(df_fe, feature=\"age_bin\", test_type=\"anova\")\n",
    "    \n",
    "    \n",
    "    league_level_effect(df_fe)\n",
    "    year_trend_ev(df_fe)\n",
    "    flag_outliers_iqr(df_fe)\n",
    "    ev_distribution_summary(df_fe)\n",
    "    # _optional_dw_check(df_fe)   # only if you still care\n",
    "\n",
    "\n",
    "    summary_df = summarize_categorical_missingness(df_fe)\n",
    "    print(summary_df.to_markdown(index=False))\n",
    "    \n",
    "    # uniques of outcome\n",
    "    print(\"outcome uniques:=========================\")\n",
    "    print(df_fe[\"outcome\"].unique())\n",
    "    # uniques of hit_type\n",
    "    print(\"hit_type uniques:=========================\")\n",
    "    print(df_fe[\"hit_type\"].unique())\n",
    "\n",
    "\n",
    "    # assume df is your feature-engineered DataFrame\n",
    "    df = df.copy()\n",
    "    # 1) Flag missing hangtime\n",
    "    df['hangtime_missing'] = df['hangtime'].isna()\n",
    "\n",
    "    # 2) Contingency of hit_type vs missingness\n",
    "    hit_type_ct = pd.crosstab(\n",
    "        df['hit_type'].fillna('UNKNOWN'),\n",
    "        df['hangtime_missing'],\n",
    "        margins=True,\n",
    "        normalize='columns'\n",
    "    )\n",
    "    print(\"\\nProportion of hit_type when hangtime is missing:\")\n",
    "    print(hit_type_ct)\n",
    "\n",
    "    # 3) Contingency of outcome vs missingness\n",
    "    outcome_ct = pd.crosstab(\n",
    "        df['outcome'].fillna('UNKNOWN'),\n",
    "        df['hangtime_missing'],\n",
    "        margins=True,\n",
    "        normalize='columns'\n",
    "    )\n",
    "    print(\"\\nProportion of outcome when hangtime is missing:\")\n",
    "    print(outcome_ct)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/features/data_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/features/data_prep.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def compute_clip_bounds(\n",
    "    series: pd.Series,\n",
    "    *,\n",
    "    method: str = \"quantile\",\n",
    "    quantiles: tuple[float,float] = (0.01,0.99),\n",
    "    std_multiplier: float = 3.0,\n",
    ") -> tuple[float,float]:\n",
    "    \"\"\"\n",
    "    Compute (lower, upper) but do not apply them.\n",
    "    \"\"\"\n",
    "    s = series.dropna()\n",
    "    if method == \"quantile\":\n",
    "        return tuple(s.quantile(list(quantiles)).to_list())\n",
    "    if method == \"mean_std\":\n",
    "        mu, sigma = s.mean(), s.std()\n",
    "        return mu - std_multiplier*sigma, mu + std_multiplier*sigma\n",
    "    if method == \"iqr\":\n",
    "        q1, q3 = s.quantile([0.25,0.75])\n",
    "        iqr = q3 - q1\n",
    "        return q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    raise ValueError(f\"Unknown method {method}\")\n",
    "\n",
    "\n",
    "def clip_extreme_ev(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    lower: float | None = None,\n",
    "    upper: float | None = None,\n",
    "    *,\n",
    "    method: str = \"quantile\",\n",
    "    quantiles: tuple[float, float] = (0.01, 0.99),\n",
    "    std_multiplier: float = 3.0,\n",
    "    debug: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clip exit velocities to [lower, upper].\n",
    "\n",
    "    If lower/upper are None, compute them from the data using one of:\n",
    "      - method=\"quantile\": use df[velo_col].quantile(quantiles)\n",
    "      - method=\"mean_std\":  use mean ± std_multiplier * std\n",
    "      - method=\"iqr\":       use [Q1 - 1.5*IQR, Q3 + 1.5*IQR]\n",
    "\n",
    "    When debug=True, prints counts & percentages of values that will be clipped.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    series = df[velo_col].dropna()\n",
    "\n",
    "    # 1) infer bounds if not given\n",
    "    if lower is None or upper is None:\n",
    "        if method == \"quantile\":\n",
    "            low_q, high_q = quantiles\n",
    "            lower_, upper_ = series.quantile([low_q, high_q]).to_list()\n",
    "        elif method == \"mean_std\":\n",
    "            mu, sigma = series.mean(), series.std()\n",
    "            lower_, upper_ = mu - std_multiplier * sigma, mu + std_multiplier * sigma\n",
    "        elif method == \"iqr\":\n",
    "            q1, q3 = series.quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            lower_, upper_ = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method '{method}' for clip_extreme_ev\")\n",
    "        lower  = lower  if lower  is not None else lower_\n",
    "        upper  = upper  if upper  is not None else upper_\n",
    "\n",
    "    # 2) debug: count how many will be clipped\n",
    "    if debug:\n",
    "        total   = len(series)\n",
    "        n_low   = (series < lower).sum()\n",
    "        n_high  = (series > upper).sum()\n",
    "        print(f\"[clip_extreme_ev] lower={lower:.2f}, upper={upper:.2f}\")\n",
    "        print(f\"  → {n_low:,} / {total:,} ({n_low/total:.2%}) below lower\")\n",
    "        print(f\"  → {n_high:,} / {total:,} ({n_high/total:.2%}) above upper\")\n",
    "\n",
    "    # 3) actually clip\n",
    "    df[velo_col] = df[velo_col].clip(lower, upper)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def filter_bunts_and_popups(\n",
    "    df: pd.DataFrame,\n",
    "    hit_col: str = \"hit_type\",\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop bunts and pop-ups from the DataFrame.\n",
    "    If debug=True, prints how many rows were removed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    initial = len(df)\n",
    "    mask = ~df[hit_col].str.upper().isin([\"BUNT\", \"POP_UP\"])\n",
    "    filtered = df[mask]\n",
    "    if debug:\n",
    "        removed = initial - len(filtered)\n",
    "        print(f\"[filter_bunts_and_popups] dropped {removed:,} rows \"\n",
    "              f\"({removed/initial:.2%}) bunts/pop-ups\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def filter_low_event_batters(\n",
    "    df: pd.DataFrame,\n",
    "    batter_col: str = \"batter_id\",\n",
    "    min_events: int = 15,\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop all rows for batters with fewer than min_events total events.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    counts = df[batter_col].value_counts()\n",
    "    keep_batters = counts[counts >= min_events].index\n",
    "    initial = len(df)\n",
    "    filtered = df[df[batter_col].isin(keep_batters)]\n",
    "    if debug:\n",
    "        removed = initial - len(filtered)\n",
    "        print(f\"[filter_low_event_batters] dropped {removed:,} rows ({removed/initial:.2%}) for batters with <{min_events} events\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "def filter_physical_implausibles(\n",
    "    df: pd.DataFrame,\n",
    "    hang_col: str = \"hangtime\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    min_hang: float = 0.5,\n",
    "    max_velo: float = 115.0,\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop rows where hangtime < min_hang AND exit_velo > max_velo,\n",
    "    as these are likely sensor glitches (e.g., foul tips).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    initial = len(df)\n",
    "    mask = ~((df[hang_col] < min_hang) & (df[velo_col] > max_velo))\n",
    "    filtered = df[mask]\n",
    "    if debug:\n",
    "        removed = initial - len(filtered)\n",
    "        print(f\"[filter_physical_implausibles] dropped {removed:,} rows ({removed/initial:.2%}) for hangtime<{min_hang} & exit_velo>{max_velo}\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Smoke test / CLI entry\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.data.ColumnSchema import _ColumnSchema\n",
    "    from src.features.eda import summarize_categorical_missingness\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    # from src.features.data_prep import filter_and_clip\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    \n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    # use list‐comprehension to drop target(s) from numerical features\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "    \n",
    "\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    summary_df = summarize_categorical_missingness(df_fe)\n",
    "    print(summary_df.to_markdown(index=False))\n",
    "    \n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    \n",
    "    # filter out bunts and popups\n",
    "    df_fe = filter_bunts_and_popups(df_fe)\n",
    "    # chekc on bunts and popups\n",
    "    print(df_fe[\"hit_type\"].unique())\n",
    "    print(df_fe[\"outcome\"].unique())\n",
    "\n",
    "    debug = True\n",
    "    TARGET = cols.target()\n",
    "    lower, upper = compute_clip_bounds(\n",
    "        df[TARGET],\n",
    "        method=\"quantile\",            # default: 1st/99th percentile\n",
    "        quantiles=(0.01, 0.99),\n",
    "    )\n",
    "    if debug:\n",
    "        total = len(df)\n",
    "        n_low = (df[TARGET] < lower).sum()\n",
    "        n_high= (df[TARGET] > upper).sum()\n",
    "        print(f\"[fit_preprocessor] clipping train EV to [{lower:.2f}, {upper:.2f}]\")\n",
    "        print(f\"  → {n_low:,}/{total:,} ({n_low/total:.2%}) below\")\n",
    "        print(f\"  → {n_high:,}/{total:,} ({n_high/total:.2%}) above\")\n",
    "    df_clipped = clip_extreme_ev(df, lower=lower, upper=upper)\n",
    "\n",
    "    print(\"Final rows after filter & clip:\", len(df_clipped))\n",
    "\n",
    "\n",
    "    # 1) drop batters with too few events\n",
    "    df_fe = filter_low_event_batters(df_fe, batter_col=\"batter_id\", min_events=15, debug=debug)\n",
    "\n",
    "    # 2) drop physical implausibles\n",
    "    df_fe = filter_physical_implausibles(\n",
    "        df_fe,\n",
    "        hang_col=\"hangtime\",\n",
    "        velo_col=\"exit_velo\",\n",
    "        debug=debug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/features/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/features/preprocess.py\n",
    "\"\"\"\n",
    "Preprocessing module for exit velocity pipeline.\n",
    "Supports multiple model types (linear, XGBoost, PyMC, etc.) with\n",
    "automatic ordinal-category detection from the data.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer, MissingIndicator\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.features.data_prep import (filter_bunts_and_popups\n",
    "                                    , compute_clip_bounds\n",
    "                                    , clip_extreme_ev\n",
    "                                    , filter_low_event_batters\n",
    "                                    , filter_physical_implausibles)\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# Numeric & nominal pipelines (unchanged)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "numeric_linear = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median', add_indicator=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "numeric_iterative = Pipeline([\n",
    "    ('impute', IterativeImputer(random_state=0, add_indicator=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "nominal_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='MISSING')),\n",
    "    ('encode', OneHotEncoder(drop='first', handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# Helper function for domain cleaning to reduce duplication\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def filter_and_clip(\n",
    "    df: pd.DataFrame,\n",
    "    lower: float = None,\n",
    "    upper: float = None,\n",
    "    quantiles: tuple[float, float] = (0.01, 0.99),\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the dataset by:\n",
    "    1. Filtering out bunts and popups\n",
    "    2. Clipping extreme exit velocity values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame to clean\n",
    "    lower : float, optional\n",
    "        Lower bound for clipping, computed from data if None\n",
    "    upper : float, optional\n",
    "        Upper bound for clipping, computed from data if None\n",
    "    quantiles : tuple[float, float], optional\n",
    "        Quantiles to use if computing bounds from data\n",
    "    debug : bool, optional\n",
    "        Whether to print diagnostic information\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cleaned DataFrame (copy of input)\n",
    "    tuple[float, float]\n",
    "        The (lower, upper) bounds used for clipping\n",
    "    \"\"\"\n",
    "    cols = _ColumnSchema()\n",
    "    TARGET = cols.target()\n",
    "    \n",
    "    # 1. Filter out bunts and popups\n",
    "    df = filter_bunts_and_popups(df, debug=debug)\n",
    "    \n",
    "    # 2. Compute bounds if not provided\n",
    "    if lower is None or upper is None:\n",
    "        lower_computed, upper_computed = compute_clip_bounds(\n",
    "            df[TARGET], method=\"quantile\", quantiles=quantiles\n",
    "        )\n",
    "        lower = lower if lower is not None else lower_computed\n",
    "        upper = upper if upper is not None else upper_computed\n",
    "    \n",
    "    # 3. Clip extreme values\n",
    "    df = clip_extreme_ev(df, lower=lower, upper=upper, debug=debug)\n",
    "\n",
    "    # 1) drop batters with too few events\n",
    "    df = filter_low_event_batters(df, batter_col=\"batter_id\", min_events=15, debug=debug)\n",
    "\n",
    "    # 2) drop physical implausibles\n",
    "    df = filter_physical_implausibles(\n",
    "        df,\n",
    "        hang_col=\"hangtime\",\n",
    "        velo_col=\"exit_velo\",\n",
    "        debug=debug)\n",
    "    \n",
    "    return df, (lower, upper)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# Dynamic preprocess functions\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) fit_preprocessor (with train‐set‐only bound computation + clip)  \n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def fit_preprocessor(\n",
    "    df: pd.DataFrame,\n",
    "    model_type: str = \"linear\",\n",
    "    debug: bool = False,\n",
    "    quantiles: tuple[float, float] = (0.01, 0.99),\n",
    "    max_safe_rows: int = 200000\n",
    ") -> tuple[np.ndarray, pd.Series, ColumnTransformer]:\n",
    "    \"\"\"\n",
    "    Fit a preprocessing pipeline to transform raw data into model-ready features,\n",
    "    adding a MissingIndicator for ordinal features.\n",
    "\n",
    "    IMPORTANT: Call this on TRAINING data only.\n",
    "    \"\"\"\n",
    "    if len(df) > max_safe_rows:\n",
    "        warnings.warn(\n",
    "            f\"Dataset has {len(df)} rows (>{max_safe_rows}); \"\n",
    "            \"ensure you’re only fitting on TRAINING data.\",\n",
    "            UserWarning\n",
    "        )\n",
    "\n",
    "    # 0) Domain cleaning and clipping\n",
    "    df, (lower, upper) = filter_and_clip(df, quantiles=quantiles, debug=debug)\n",
    "\n",
    "    cols = _ColumnSchema()\n",
    "    TARGET = cols.target()\n",
    "\n",
    "    # 1) Split out features + coerce numerics\n",
    "    num_feats = [c for c in cols.numerical() if c != TARGET]\n",
    "    ord_feats = cols.ordinal()\n",
    "    nom_feats = cols.nominal()\n",
    "\n",
    "    df[num_feats] = df[num_feats].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X = df[num_feats + ord_feats + nom_feats].copy()\n",
    "    y = df[TARGET]\n",
    "\n",
    "    # 2) Prepare ordinal columns\n",
    "    X[ord_feats] = X[ord_feats].astype(\"string\")\n",
    "    X.loc[:, ord_feats] = X.loc[:, ord_feats].mask(X[ord_feats].isna(), np.nan)\n",
    "    ordinal_categories = [[*X[c].dropna().unique(), \"MISSING\"] for c in ord_feats]\n",
    "    ordinal_pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"MISSING\")),\n",
    "        (\"encode\", OrdinalEncoder(\n",
    "            categories=ordinal_categories,\n",
    "            handle_unknown=\"use_encoded_value\",\n",
    "            unknown_value=-1,\n",
    "            dtype=\"int32\"\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    # 3) Numeric pipeline selection\n",
    "    if model_type == \"linear\":\n",
    "        num_imputer = SimpleImputer(strategy=\"median\", add_indicator=True)\n",
    "    else:\n",
    "        num_imputer = IterativeImputer(\n",
    "            random_state=0,\n",
    "            add_indicator=True\n",
    "        )\n",
    "\n",
    "    numeric_pipe = Pipeline([\n",
    "        (\"impute\", num_imputer),\n",
    "        (\"scale\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    # 4) ColumnTransformer with an extra MissingIndicator for ordinals\n",
    "    ct = ColumnTransformer(\n",
    "        [\n",
    "            (\"num\", numeric_pipe, num_feats),\n",
    "            (\"ord_ind\", MissingIndicator(missing_values=np.nan), ord_feats),\n",
    "            (\"ord\", ordinal_pipe, ord_feats),\n",
    "            (\"nom\", nominal_pipe, nom_feats),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    # preserve clipping bounds for transform\n",
    "    ct.lower_, ct.upper_ = lower, upper\n",
    "\n",
    "    X_mat = ct.fit_transform(X, y)\n",
    "    return X_mat, y, ct\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def transform_preprocessor(\n",
    "    df: pd.DataFrame,\n",
    "    transformer: ColumnTransformer,\n",
    ") -> tuple[np.ndarray, pd.Series]:\n",
    "    \"\"\"\n",
    "    Transform new data using a fitted preprocessor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        New data to transform\n",
    "    transformer : ColumnTransformer\n",
    "        Fitted transformer from fit_preprocessor\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_mat : np.ndarray\n",
    "        Transformed feature matrix\n",
    "    y : pd.Series\n",
    "        Target values\n",
    "    \"\"\"\n",
    "    cols, TARGET = _ColumnSchema(), _ColumnSchema().target()\n",
    "\n",
    "    # Domain filter & clip using the bounds from the fitted transformer\n",
    "    df, _ = filter_and_clip(df, lower=transformer.lower_, upper=transformer.upper_)\n",
    "\n",
    "    # rebuild lists & coerce numerics\n",
    "    num_feats = [c for c in cols.numerical() if c != TARGET]\n",
    "    ord_feats = cols.ordinal()\n",
    "    nom_feats = cols.nominal()\n",
    "    df[num_feats] = df[num_feats].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    X = df[num_feats + ord_feats + nom_feats].copy()\n",
    "\n",
    "    # **NEW** – ensure ordinals are strings, then fill unseen→\"MISSING\"\n",
    "    X[ord_feats] = X[ord_feats].astype(\"string\")\n",
    "    X.loc[:, ord_feats] = X.loc[:, ord_feats].where(X.loc[:, ord_feats].notna(), \"MISSING\")\n",
    "\n",
    "    y = df[TARGET]\n",
    "    X_mat = transformer.transform(X)\n",
    "    return X_mat, y\n",
    "\n",
    "\n",
    "def inverse_transform_preprocessor(\n",
    "    X_trans: np.ndarray,\n",
    "    transformer: ColumnTransformer\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Invert each block of a ColumnTransformer back to its original features,\n",
    "    skipping transformers that lack inverse_transform (e.g., MissingIndicator).\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import warnings\n",
    "    from sklearn.impute import MissingIndicator  # for isinstance check\n",
    "\n",
    "    # 1) Recover original feature order\n",
    "    orig_features: list[str] = []\n",
    "    for name, _, cols in transformer.transformers_:\n",
    "        if cols == 'drop': continue\n",
    "        orig_features.extend(cols)\n",
    "\n",
    "    parts = []\n",
    "    start = 0\n",
    "    n_rows = X_trans.shape[0]\n",
    "\n",
    "    # 2) Loop through each transformer block\n",
    "    for name, trans, cols in transformer.transformers_:\n",
    "        if cols == 'drop':\n",
    "            continue\n",
    "\n",
    "        fitted = transformer.named_transformers_[name]\n",
    "\n",
    "        # 2a) Determine number of output columns\n",
    "        dummy = np.zeros((1, len(cols)))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "            try:\n",
    "                out = fitted.transform(dummy)\n",
    "            except Exception:\n",
    "                out = dummy\n",
    "        n_out = out.shape[1]\n",
    "\n",
    "        # 2b) Slice the real transformed data block\n",
    "        block = X_trans[:, start : start + n_out]\n",
    "        start += n_out\n",
    "\n",
    "        # 2c) Handle each transformer type\n",
    "        if isinstance(fitted, MissingIndicator):\n",
    "            # Skip MissingIndicator: it has no inverse_transform\n",
    "            continue  # :contentReference[oaicite:2]{index=2}\n",
    "        elif trans == 'passthrough':\n",
    "            inv = block\n",
    "        elif name == 'num':\n",
    "            scaler = fitted.named_steps['scale']\n",
    "            inv_full = scaler.inverse_transform(block)\n",
    "            inv = inv_full[:, :len(cols)]\n",
    "        else:\n",
    "            # Ordinal or nominal pipelines\n",
    "            if isinstance(fitted, Pipeline):\n",
    "                last = list(fitted.named_steps.values())[-1]\n",
    "                inv = last.inverse_transform(block)\n",
    "            else:\n",
    "                inv = fitted.inverse_transform(block)\n",
    "\n",
    "        parts.append(pd.DataFrame(inv, columns=cols, index=range(n_rows)))\n",
    "\n",
    "    # 3) Concatenate & reorder to match original features\n",
    "    df_orig = pd.concat(parts, axis=1)\n",
    "    return df_orig[orig_features]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prepare_for_mixed_and_hierarchical(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the rows *and* adds convenience covariates expected by the\n",
    "    hierarchical and mixed-effects models.\n",
    "    \"\"\"\n",
    "    cols = _ColumnSchema()\n",
    "    TARGET = cols.target()\n",
    "\n",
    "    # Apply domain cleaning with default quantiles\n",
    "    df, _ = filter_and_clip(df.copy())\n",
    "\n",
    "    # Category coding for batter\n",
    "    df[\"batter_id\"] = df[\"batter_id\"].astype(\"category\")\n",
    "\n",
    "    # New: Category coding for season\n",
    "    df[\"season_cat\"] = df[\"season\"].astype(\"category\")\n",
    "    df[\"season_idx\"] = df[\"season_cat\"].cat.codes\n",
    "\n",
    "    # New: Category coding for pitcher\n",
    "    df[\"pitcher_cat\"] = df[\"pitcher_id\"].astype(\"category\")\n",
    "    df[\"pitcher_idx\"] = df[\"pitcher_cat\"].cat.codes\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    df_fe = feature_engineer(df)\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "\n",
    "\n",
    "    train_df, test_df = train_test_split(df_fe, test_size=0.2, random_state=42)\n",
    "    # run with debug prints\n",
    "    X_train, y_train, tf = fit_preprocessor(train_df, model_type='linear', debug=True)\n",
    "    X_test,  y_test      = transform_preprocessor(test_df, tf)\n",
    "\n",
    "    print(\"Processed shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "    # Example of inverse transform: \n",
    "    print(\"==========Example of inverse transform:==========\")\n",
    "    df_back = inverse_transform_preprocessor(X_train, tf)\n",
    "    print(\"\\n✅ Inverse‐transformed head (should mirror your original X_train):\")\n",
    "    print(df_back.head())\n",
    "    print(\"Shape:\", df_back.shape, \"→ original X_train shape before transform:\", X_train.shape)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/features/feature_selection.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/features/feature_selection.py\n",
    "import pandas as pd\n",
    "\n",
    "# ── NEW: model and importance imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "\n",
    "from pathlib import Path\n",
    "from src.data.load_data import load_and_clean_data\n",
    "from src.features.feature_engineering import feature_engineer\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "# ── NEW: shapash and shapiq imports\n",
    "from shapash import SmartExplainer\n",
    "import shapiq\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def train_baseline_model(X, y):\n",
    "    \"\"\"\n",
    "    Fit a RandomForestRegressor on X, y.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    # You can adjust hyperparameters as needed\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def compute_permutation_importance(\n",
    "    model,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    n_repeats: int = 10,\n",
    "    n_jobs: int = 1,\n",
    "    max_samples: float | int = None,\n",
    "    random_state: int = 42,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute permutation importances with controlled resource usage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : estimator\n",
    "        Fitted model implementing .predict and .score.\n",
    "    X : pd.DataFrame\n",
    "        Features.\n",
    "    y : pd.Series or array\n",
    "        Target.\n",
    "    n_repeats : int\n",
    "        Number of shuffles per feature.\n",
    "    n_jobs : int\n",
    "        Number of parallel jobs (avoid -1 on Windows).\n",
    "    max_samples : float or int, optional\n",
    "        If float in (0,1], fraction of rows to sample.\n",
    "        If int, absolute number of rows to sample.\n",
    "    random_state : int\n",
    "        Seed for reproducibility.\n",
    "    verbose : bool\n",
    "        Print debug info if True.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: feature, importance_mean, importance_std.\n",
    "        Sorted descending by importance_mean.\n",
    "    \"\"\"\n",
    "    # Debug info\n",
    "    if verbose:\n",
    "        print(f\"⏳ Computing permutation importances on {X.shape[0]} rows × {X.shape[1]} features\")\n",
    "        print(f\"   n_repeats={n_repeats}, n_jobs={n_jobs}, max_samples={max_samples}\")\n",
    "\n",
    "    # Subsample if requested\n",
    "    X_sel, y_sel = X, y\n",
    "    if max_samples is not None:\n",
    "        if isinstance(max_samples, float):\n",
    "            nsamp = int(len(X) * max_samples)\n",
    "        else:\n",
    "            nsamp = int(max_samples)\n",
    "        if verbose:\n",
    "            print(f\"   Subsampling to {nsamp} rows for speed\")\n",
    "        X_sel, y_sel = resample(X, y, replace=False, n_samples=nsamp, random_state=random_state)\n",
    "\n",
    "    try:\n",
    "        result = permutation_importance(\n",
    "            model,\n",
    "            X_sel, y_sel,\n",
    "            n_repeats=n_repeats,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "    except OSError as e:\n",
    "        # Graceful fallback to single job\n",
    "        if verbose:\n",
    "            print(f\"⚠️  OSError ({e}). Retrying with n_jobs=1\")\n",
    "        result = permutation_importance(\n",
    "            model,\n",
    "            X_sel, y_sel,\n",
    "            n_repeats=n_repeats,\n",
    "            random_state=random_state,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "    # Build and sort DataFrame\n",
    "    importance_df = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": X.columns,\n",
    "            \"importance_mean\": result.importances_mean,\n",
    "            \"importance_std\": result.importances_std,\n",
    "        })\n",
    "        .sort_values(\"importance_mean\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"✅ Permutation importances computed.\")\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def compute_shap_importance(model, X, nsamples=100):\n",
    "    \"\"\"\n",
    "    Compute mean absolute SHAP values per feature.\n",
    "    Returns a DataFrame sorted by importance.\n",
    "    \"\"\"\n",
    "    # DeepExplainer or TreeExplainer for tree-based models\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    # sample for speed\n",
    "    X_sample = X.sample(n=min(nsamples, len(X)), random_state=42)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    # For regression, shap_values is a 2D array\n",
    "    mean_abs_shap = pd.DataFrame({\n",
    "        \"feature\": X_sample.columns,\n",
    "        \"shap_importance\": np.abs(shap_values).mean(axis=0),\n",
    "    })\n",
    "    mean_abs_shap = mean_abs_shap.sort_values(\"shap_importance\", ascending=False).reset_index(drop=True)\n",
    "    return mean_abs_shap\n",
    "\n",
    "\n",
    "\n",
    "def filter_permutation_features(\n",
    "    importance_df: pd.DataFrame,\n",
    "    threshold: float\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Return features whose permutation importance_mean exceeds threshold.\n",
    "    \"\"\"\n",
    "    kept = importance_df.loc[\n",
    "        importance_df[\"importance_mean\"] > threshold, \"feature\"\n",
    "    ]\n",
    "    return kept.tolist()\n",
    "\n",
    "\n",
    "def filter_shap_features(\n",
    "    importance_df: pd.DataFrame,\n",
    "    threshold: float\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Return features whose shap_importance exceeds threshold.\n",
    "    \"\"\"\n",
    "    kept = importance_df.loc[\n",
    "        importance_df[\"shap_importance\"] > threshold, \"feature\"\n",
    "    ]\n",
    "    return kept.tolist()\n",
    "\n",
    "\n",
    "def select_final_features(\n",
    "    perm_feats: list[str],\n",
    "    shap_feats: list[str],\n",
    "    mode: str = \"intersection\"\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Combine permutation and SHAP feature lists.\n",
    "    mode=\"intersection\" for features in both lists,\n",
    "    mode=\"union\" for features in either list.\n",
    "    \"\"\"\n",
    "    set_perm = set(perm_feats)\n",
    "    set_shap = set(shap_feats)\n",
    "    if mode == \"union\":\n",
    "        final = set_perm | set_shap\n",
    "    else:\n",
    "        final = set_perm & set_shap\n",
    "    # return sorted for reproducibility\n",
    "    return sorted(final)\n",
    "\n",
    "\n",
    "\n",
    "def load_final_features(\n",
    "    file_path: str = \"data/models/features/final_features.txt\"\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Read the newline-delimited feature names file and return as a list.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as fp:\n",
    "        return [line.strip() for line in fp if line.strip()]\n",
    "\n",
    "\n",
    "def filter_to_final_features(\n",
    "    df: pd.DataFrame,\n",
    "    file_path: str = \"data/models/features/final_features.txt\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a feature-engineered DataFrame, load the final feature list,\n",
    "    then return df[ ID_cols + final_features + [target] ].\n",
    "    \"\"\"\n",
    "    # load the feature names\n",
    "    final_feats = load_final_features(file_path)\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    keep = cols.id() + final_feats + [cols.target()]\n",
    "    missing = set(keep) - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Cannot filter: missing columns {missing}\")\n",
    "    return df[keep].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- existing loading & schema logic ---\n",
    "    raw_path = Path(\"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\")\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values in raw data.\")\n",
    "    else:\n",
    "        print(\"=== Raw data null counts ===\")\n",
    "        for col, cnt in null_counts.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "    df_fe = feature_engineer(df)\n",
    "    print(\"Raw →\", df.shape, \"//  Feature-engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "\n",
    "    cols = _ColumnSchema()\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:     \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "\n",
    "    # ── STEP 1: fully preprocess the engineered DataFrame ──\n",
    "    from src.features.preprocess import fit_preprocessor, inverse_transform_preprocessor\n",
    "\n",
    "    # fit_preprocessor returns (X_matrix, y, fitted_transformer)\n",
    "    X_np, y, preproc = fit_preprocessor(df_fe, model_type='linear', debug=False)\n",
    "\n",
    "    # Use the same index that y carries (only non-bunt, non-NA rows)\n",
    "    idx = y.index\n",
    "    \n",
    "    # turn that into a DataFrame with the same column names:\n",
    "    feat_names = preproc.get_feature_names_out()\n",
    "    X = pd.DataFrame(X_np, columns=feat_names, index=idx)\n",
    "    print(f\"✅ Preprocessed feature matrix: {X.shape[0]} rows × {X.shape[1]} cols\")\n",
    "\n",
    "    # (optional) confirm inverse transform lines up:\n",
    "    df_back = inverse_transform_preprocessor(X_np, preproc)\n",
    "    df_back.index = idx\n",
    "    print(\"✅ inverse_transform round-trip (head):\")\n",
    "    print(df_back.head())\n",
    "\n",
    "    # ── STEP 2: train & compute importances on *that* X ──\n",
    "    print(\"\\nTraining baseline model…\")\n",
    "    model = train_baseline_model(X, y)\n",
    "\n",
    "    print(\"\\n🔍 Permutation Importances:\")\n",
    "    perm_imp = compute_permutation_importance(\n",
    "        model, X, y,\n",
    "        n_repeats=10,\n",
    "        n_jobs=2,            # test small parallelism\n",
    "        max_samples=0.5,     # test subsampling\n",
    "        verbose=True\n",
    "    )\n",
    "    print(perm_imp)\n",
    "\n",
    "\n",
    "    print(\"\\n🔍 SHAP Importances:\")\n",
    "    shap_imp = compute_shap_importance(model, X)\n",
    "    print(shap_imp)\n",
    "\n",
    "    # ── STEP 3: threshold & select your final features ──\n",
    "    perm_thresh = 0.01\n",
    "    shap_thresh = 0.01\n",
    "    perm_feats = filter_permutation_features(perm_imp, perm_thresh)\n",
    "    shap_feats = filter_shap_features(shap_imp, shap_thresh)\n",
    "    final_feats = select_final_features(perm_feats, shap_feats, mode=\"intersection\")\n",
    "    print(f\"\\nFinal preprocessed feature list ({len(final_feats)}):\")\n",
    "    print(final_feats)\n",
    "\n",
    "    # ── STEP 4: build & save a dataset with just those features + target + IDs ──\n",
    "    df_final = pd.concat([\n",
    "        df_fe[cols.id()],\n",
    "        df_fe[[cols.target()]],\n",
    "        X[final_feats]\n",
    "    ], axis=1)\n",
    "    print(\"Final dataset shape:\", df_final.shape)\n",
    "\n",
    "    Path(\"data/models/features/final_features.txt\").write_text(\"\\n\".join(final_feats))\n",
    "    print(\"✔️ Saved feature list to final_features.txt\")\n",
    "\n",
    "\n",
    "    # Demo: filter the full df_fe back to just those features\n",
    "    df_filtered = filter_to_final_features(df_fe)\n",
    "    print(\"Filtered to final features shape:\", df_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model choices\n",
    "\n",
    "see modelling_choices.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/models/linear.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/linear.py\n",
    "\n",
    "\"\"\"\n",
    "Fast linear baselines (OLS and Ridge).\n",
    "\n",
    "Usage\n",
    "-----\n",
    ">>> from src.models.linear import fit_ridge\n",
    ">>> fitted, rmse = fit_ridge(train_df, val_df)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def _split_xy(df: pd.DataFrame):\n",
    "    X = df.drop(columns=[\"exit_velo\"])\n",
    "    y = df[\"exit_velo\"]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fit_ridge(X_tr: pd.DataFrame,\n",
    "              y_tr: pd.DataFrame,\n",
    "              X_te: pd.DataFrame,\n",
    "              y_te: pd.DataFrame,\n",
    "              alpha: float = 1.0):\n",
    "    \"\"\"\n",
    "    Returns (sklearn Pipeline, RMSE on test set).\n",
    "    \"\"\"\n",
    "\n",
    "    model = Pipeline(\n",
    "        [(\"reg\" , Ridge(alpha=alpha, random_state=0))]\n",
    "    ).fit(X_tr, y_tr)\n",
    "\n",
    "    pred = model.predict(X_te)\n",
    "    rmse = np.sqrt(np.mean((pred - y_te) ** 2))\n",
    "    return model, rmse\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.data.ColumnSchema import _ColumnSchema\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from src.features.preprocess import summarize_categorical_missingness\n",
    "    from src.features.preprocess import fit_preprocessor, transform_preprocessor, inverse_transform_preprocessor\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "\n",
    "    # --- inspect nulls in the raw data ---\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values in raw data.\")\n",
    "    else:\n",
    "        print(\"=== Raw data null counts ===\")\n",
    "        for col, cnt in null_counts.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    # use list‐comprehension to drop target(s) from numerical features\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "\n",
    "    summary_df = summarize_categorical_missingness(df_fe)\n",
    "    print(summary_df.to_markdown(index=False))\n",
    "\n",
    "\n",
    "    # check nulls\n",
    "    print(\"🛠️  Nulls in X before fit_transform:\")\n",
    "    null_counts = df_fe.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values after feature engineering.\")\n",
    "    else:\n",
    "        print(\"=== Null counts post-engineering ===\")\n",
    "        print(null_counts)\n",
    "\n",
    "    train_df, test_df = train_test_split(df_fe, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # only on training data for linear/XGB\n",
    "    train_df = clip_extreme_ev(train_df)\n",
    "    #valid_df = clip_extreme_ev(valid_df)\n",
    "    \n",
    "    # run with debug prints\n",
    "    X_train, y_train, tf = fit_preprocessor(train_df, model_type='linear', debug=True)\n",
    "    X_test,  y_test      = transform_preprocessor(test_df, tf)\n",
    "\n",
    "        \n",
    "    print(\"Processed shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "    # Example of inverse transform: \n",
    "    print(\"==========Example of inverse transform:==========\")\n",
    "    df_back = inverse_transform_preprocessor(X_train, tf)\n",
    "    print(\"\\n✅ Inverse‐transformed head (should mirror your original X_train):\")\n",
    "    print(df_back.head())\n",
    "    print(\"Shape:\", df_back.shape, \"→ original X_train shape before transform:\", X_train.shape)\n",
    "    \n",
    "\n",
    "    # === NEW: Train & evaluate Ridge regression ===\n",
    "    model_ridge, rmse_ridge = fit_ridge(X_train, y_train, X_test,  y_test)\n",
    "    print(f\"Ridge regression RMSE: {rmse_ridge:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/gbm_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/gbm_utils.py\n",
    "\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def save_pipeline(model, preprocessor, path: str = \"data/models/saved_models/gbm_pipeline.joblib\"):\n",
    "    \"\"\"\n",
    "    Save both model and preprocessor together to a single file.\n",
    "    \"\"\"\n",
    "    out_path = Path(path)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    joblib.dump(\n",
    "        {\"model\": model, \"preprocessor\": preprocessor},\n",
    "        out_path\n",
    "    )\n",
    "    print(f\"✅ Pipeline saved to {out_path.resolve()}\")\n",
    "\n",
    "def load_pipeline(path: str = \"data/models/saved_models/gbm_pipeline.joblib\"):\n",
    "    \"\"\"\n",
    "    Load the model+preprocessor dict back into memory.\n",
    "    Returns a tuple: (model, preprocessor)\n",
    "    \"\"\"\n",
    "    saved = joblib.load(path)\n",
    "    return saved[\"model\"], saved[\"preprocessor\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/models/gbm.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/gbm.py\n",
    "\n",
    "\"\"\"\n",
    "Gradient‑boosting baseline (XGBoost regressor).\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost.callback import EarlyStopping     # ① import the new callback\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import optuna\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from xgboost.core import XGBoostError\n",
    "from src.utils.gbm_utils import save_pipeline, load_pipeline\n",
    "\n",
    "# ————— Detect GPU support using modern API —————\n",
    "try:\n",
    "    XGBRegressor(tree_method=\"hist\", device=\"cuda\")\n",
    "    GPU_SUPPORT = True\n",
    "except XGBoostError:\n",
    "    GPU_SUPPORT = False\n",
    "\n",
    "def _split_xy(df: pd.DataFrame):\n",
    "    X = df.drop(columns=[\"exit_velo\"])\n",
    "    y = df[\"exit_velo\"]\n",
    "    return X, y\n",
    "\n",
    "def tune_gbm(X, y, n_trials: int = 50):\n",
    "    \"\"\"\n",
    "    Run an Optuna study to minimize CV RMSE of an XGBRegressor.\n",
    "    Uses device=cuda if available, else CPU.\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"random_state\": 0,\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "        params.update({\"tree_method\": \"hist\"})\n",
    "        model = XGBRegressor(**params)\n",
    "        scores = cross_val_score(\n",
    "            model, X, y,\n",
    "            scoring=\"neg_root_mean_squared_error\",\n",
    "            cv=3, n_jobs=-1\n",
    "        )\n",
    "        return -scores.mean()\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "\n",
    "\n",
    "def fit_gbm(X_tr, y_tr, X_te, y_te, **gbm_kw):\n",
    "    \"\"\"\n",
    "    Train an XGBRegressor with optional hyperparameters and early stopping.\n",
    "    Returns (fitted_model, rmse_on_test).\n",
    "    \"\"\"\n",
    "\n",
    "    # A) Default constructor args\n",
    "    constructor_defaults = dict(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        random_state=0,\n",
    "        n_jobs=-1,\n",
    "        tree_method=\"hist\",\n",
    "    )\n",
    "    constructor_defaults.update(gbm_kw)\n",
    "\n",
    "    # B) Extract early_stopping_rounds (pop it out)\n",
    "    early_stopping = constructor_defaults.pop(\"early_stopping_rounds\", None)\n",
    "\n",
    "    # C) Instantiate model\n",
    "    model = XGBRegressor(**constructor_defaults)\n",
    "\n",
    "    # D) Prepare fit kwargs\n",
    "    fit_kwargs = {}\n",
    "    # If an early_stopping value was provided, use the callback interface\n",
    "    if early_stopping:\n",
    "        fit_kwargs[\"callbacks\"] = [EarlyStopping(rounds=early_stopping)]\n",
    "        # pass eval_set so callback can work\n",
    "        fit_kwargs[\"eval_set\"] = [(X_te, y_te)]\n",
    "    # You can still pass verbose if desired\n",
    "    fit_kwargs[\"verbose\"] = False\n",
    "\n",
    "    # Fit the model with or without early stopping\n",
    "    model.fit(X_tr, y_tr, **fit_kwargs)\n",
    "\n",
    "    # E) Compute RMSE on test\n",
    "    preds = model.predict(X_te)\n",
    "    rmse  = mean_squared_error(y_te, preds, squared=False)\n",
    "\n",
    "    return model, rmse\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.features.preprocess import (fit_preprocessor\n",
    "                                        ,transform_preprocessor\n",
    "                                        ,inverse_transform_preprocessor)\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    df_fe = feature_engineer(df)\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "\n",
    "\n",
    "    train_df, test_df = train_test_split(df_fe, test_size=0.2, random_state=42)\n",
    "    # run with debug prints\n",
    "    # Record original feature count before preprocessing\n",
    "    X_orig = train_df.drop(columns=[\"exit_velo\"])\n",
    "    orig_shape = X_orig.shape\n",
    "\n",
    "    # Run preprocessing\n",
    "    X_train, y_train, tf = fit_preprocessor(train_df, model_type='linear', debug=True)\n",
    "    X_test,  y_test      = transform_preprocessor(test_df, tf)\n",
    "\n",
    "    print(\"Processed shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "    # Example of inverse transform: \n",
    "    print(\"==========Example of inverse transform:==========\")\n",
    "    df_back = inverse_transform_preprocessor(X_train, tf)\n",
    "    print(\"\\n✅ Inverse‐transformed head (should mirror your original X_train):\")\n",
    "    print(df_back.head())\n",
    "    print(f\"Shape: {df_back.shape} → original X_train shape before transform: {orig_shape}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    # === Hyperparameter tuning ===\n",
    "    best_params = tune_gbm(X_train, y_train, n_trials=5)\n",
    "    print(\"Tuned params:\", best_params)\n",
    "\n",
    "    # === Train & evaluate ===\n",
    "    gbm_model, rmse = fit_gbm(\n",
    "        X_train, y_train, X_test, y_test, **best_params\n",
    "    )\n",
    "    print(f\"Tuned XGBoost RMSE: {rmse:.4f}\")\n",
    "\n",
    "\n",
    "    # 3) save both at once\n",
    "    save_pipeline(gbm_model, tf, path=\"models/gbm_pipeline.joblib\")\n",
    "    \n",
    "    model, preprocessor = load_pipeline(\"models/gbm_pipeline.joblib\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/models/mixed.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/mixed.py\n",
    "\n",
    "\"\"\"\n",
    "Frequentist mixed‑effects model using statsmodels MixedLM.\n",
    "\n",
    "Formula implemented:\n",
    "    exit_velo ~ 1 + level_ord + age_centered\n",
    "              + (1 | batter_id)\n",
    "\n",
    "We rely on columns already produced by preprocess():\n",
    "    • level_idx  (0,1,2)   – ordinal\n",
    "    • age_centered\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "def fit_mixed(train: pd.DataFrame,\n",
    "              test: pd.DataFrame):\n",
    "    \"\"\"Return (fitted model, RMSE on test).\"\"\"\n",
    "    # statsmodels wants a *single* DataFrame with all cols\n",
    "    # so we concatenates and keep row positions for slicing\n",
    "    combined = pd.concat([train, test], axis=0)\n",
    "    # ensure categorical dtype\n",
    "    combined[\"level_ord\"] = combined[\"level_idx\"].astype(int)\n",
    "\n",
    "    mdl = smf.mixedlm(\n",
    "        formula=\"exit_velo ~ 1 + level_ord + age_centered\",\n",
    "        data=combined.iloc[: len(train)],\n",
    "        groups=combined.iloc[: len(train)][\"batter_id\"]\n",
    "    ).fit(reml=False)\n",
    "\n",
    "    # predict on test set\n",
    "    pred = mdl.predict(exog=combined.iloc[len(train):])\n",
    "    true = test[\"exit_velo\"].values\n",
    "    rmse = np.sqrt(np.mean((pred - true) ** 2))\n",
    "    return mdl, rmse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.features.preprocess import prepare_for_mixed_and_hierarchical\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    # Prepare and split\n",
    "    df_model = prepare_for_mixed_and_hierarchical(df_fe)\n",
    "    train_df, test_df = train_test_split(df_model, test_size=0.2, random_state=42)\n",
    " \n",
    "    # Fit mixed-effects\n",
    "    mixed_model, rmse_mixed = fit_mixed(train_df, test_df)\n",
    "    print(f\"Mixed-effects model RMSE: {rmse_mixed:.4f}\")\n",
    "\n",
    "    # In the smoke test section: P-Value Checks for Mixed-Effects Models\n",
    "    print(\"Mixed-effects model summary:\\n\", mixed_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/bayesian_explainability.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/bayesian_explainability.py\n",
    "import arviz as az\n",
    "import shap, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- Posterior summaries -----------------\n",
    "def plot_parameter_forest(idata, var_names=None, hdi_prob=0.95):\n",
    "    \"\"\"Caterpillar/forest plot of posterior estimates.\"\"\"\n",
    "    return az.plot_forest(\n",
    "        idata,\n",
    "        var_names=var_names,\n",
    "        combined=True,\n",
    "        hdi_prob=hdi_prob,\n",
    "        kind=\"forest\",\n",
    "        figsize=(6, len(var_names or idata.posterior.data_vars) * 0.4),\n",
    "    )\n",
    "\n",
    "def posterior_table(idata, round_to=2):\n",
    "    \"\"\"\n",
    "    Return a nicely rounded HDI/mean table with significance.\n",
    "    \"\"\"\n",
    "    summary = az.summary(idata, hdi_prob=0.95).round(round_to)\n",
    "    summary[\"significant\"] = (summary[\"hdi_2.5%\"] > 0) | (summary[\"hdi_97.5%\"] < 0)\n",
    "    return summary\n",
    "\n",
    "# ---------------- Posterior‑predictive checks ---------\n",
    "def plot_ppc(idata, kind=\"overlay\"):\n",
    "    \"\"\"Visual PPC (over‑laid densities by default).\"\"\"\n",
    "    return az.plot_ppc(idata, kind=kind, alpha=0.1)\n",
    "\n",
    "# ---------------- SHAP-based feature importances ------\n",
    "def shap_explain(predict_fn, background_df, sample_df):\n",
    "    \"\"\"\n",
    "    Model‑agnostic Kernel SHAP on the *posterior mean predictor*.\n",
    "\n",
    "    predict_fn(df) must return a 1‑D numpy array of predictions.\n",
    "    \"\"\"\n",
    "    explainer = shap.KernelExplainer(predict_fn, background_df)\n",
    "    shap_values = explainer.shap_values(sample_df, nsamples=200)\n",
    "    shap.summary_plot(shap_values, sample_df, show=False)\n",
    "    plt.tight_layout()\n",
    "    return shap_values\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/bayesian_metrics.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/bayesian_metrics.py\n",
    "import arviz as az\n",
    "from sklearn.metrics import mean_squared_error,root_mean_squared_error, mean_absolute_error, r2_score\n",
    "import numpy as np\n",
    "\n",
    "def compute_classical_metrics(idata, y_true):\n",
    "    \"\"\"Compute MSE, RMSE, MAE & R² from the posterior predictive distribution.\"\"\"\n",
    "    # Extract posterior predictive draws and compute mean prediction\n",
    "    y_ppc = (\n",
    "        idata.posterior_predictive['y_obs']\n",
    "        .stack(samples=('chain', 'draw'))\n",
    "        .values\n",
    "    )\n",
    "    y_pred = y_ppc.mean(axis=1)\n",
    "\n",
    "    # Classical regression metrics\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = root_mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"▶ Classical MSE : {mse:.2f}\")\n",
    "    print(f\"▶ Classical RMSE: {rmse:.2f}\")\n",
    "    print(f\"▶ Classical MAE : {mae:.2f}\")\n",
    "    print(f\"▶ Classical R²  : {r2:.3f}\")\n",
    "\n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n",
    "\n",
    "\n",
    "def compute_bayesian_metrics(idata):\n",
    "    \"\"\"Compute PSIS-LOO, WAIC and Pareto k diagnostics for model comparison.\"\"\"\n",
    "    # 1) Guard missing log_likelihood\n",
    "    if 'log_likelihood' not in idata.groups():\n",
    "        print(\"⚠️ InferenceData has no log_likelihood; skipping LOO/WAIC.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        loo  = az.loo(idata, pointwise=True)\n",
    "        waic = az.waic(idata, pointwise=True)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ LOO/WAIC computation failed: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 2) Extract scalars by indexing the ELPDData (a pandas Series)\n",
    "    #    LOOIC = -2 * elpd_loo\n",
    "    looic    = -2 * loo['elpd_loo']\n",
    "    p_loo    = loo['p_loo']\n",
    "\n",
    "    #    WAIC  = -2 * elpd_waic\n",
    "    waic_val = -2 * waic['elpd_waic']\n",
    "    p_waic   = waic['p_waic']\n",
    "\n",
    "    #    Pareto k diagnostic: proportion of observations with k > 0.7\n",
    "    prop_bad_k = np.mean(loo['pareto_k'].values > 0.7)\n",
    "\n",
    "    # 3) Print results\n",
    "    print(f\"▶ LOOIC       : {looic:.1f}, p_loo: {p_loo:.1f}\")\n",
    "    print(f\"▶ WAIC        : {waic_val:.1f}, p_waic: {p_waic:.1f}\")\n",
    "    print(f\"▶ Pareto k>0.7: {prop_bad_k:.2%} of observations\")\n",
    "\n",
    "    return {'loo': loo, 'waic': waic, 'prop_bad_k': prop_bad_k}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def compute_convergence_diagnostics(idata):\n",
    "    \"\"\"Print R̂ and ESS bulk/tail for key parameters.\"\"\"\n",
    "    if 'posterior' not in idata.groups():\n",
    "        print(\"No posterior group in InferenceData; skipping convergence diagnostics.\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        summary = az.summary(\n",
    "            idata,\n",
    "            var_names=[\"mu\", \"beta\", \"sigma_e\"],\n",
    "            kind=\"diagnostics\",\n",
    "            round_to=2\n",
    "        )\n",
    "    except KeyError as e:\n",
    "        print(f\"Convergence diagnostics skipped: missing vars {e}\")\n",
    "        return None\n",
    "\n",
    "    print(\"▶ Convergence diagnostics (R̂, ESS):\")\n",
    "    print(summary[[\"r_hat\", \"ess_bulk\", \"ess_tail\"]])\n",
    "    return summary\n",
    "\n",
    "\n",
    "def compute_calibration(idata, y_true, hdi_prob=0.95):\n",
    "    \"\"\"\n",
    "    Compute calibration: the fraction of true observations that lie within the posterior predictive HDI.\n",
    "    \"\"\"\n",
    "    # Extract posterior predictive draws\n",
    "    y_ppc = (\n",
    "        idata.posterior_predictive['y_obs']\n",
    "        .stack(samples=('chain','draw'))\n",
    "        .values\n",
    "    )\n",
    "    # Compute lower/upper quantiles per observation\n",
    "    lower = np.percentile(y_ppc, (1-hdi_prob)/2*100, axis=1)\n",
    "    upper = np.percentile(y_ppc, (1+(hdi_prob))/2*100, axis=1)\n",
    "    within = ((y_true >= lower) & (y_true <= upper)).mean()\n",
    "    print(f\"▶ Calibration: {within:.2%} of true values within {int(hdi_prob*100)}% HDI\")\n",
    "    return within\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    import arviz as az\n",
    "\n",
    "    # 1) True values\n",
    "    np.random.seed(42)\n",
    "    y_true = np.random.normal(0, 1, size=10)\n",
    "\n",
    "    # 2) Posterior predictive draws (2 chains × 5 draws × 10 obs)\n",
    "    y_obs = np.random.normal(loc=y_true, scale=0.5, size=(2,5,10))\n",
    "\n",
    "    # 3) Posterior parameters μ, β, σₑ (2 chains × 5 draws)\n",
    "    posterior = {\n",
    "        'mu':      np.random.normal(0, 1, size=(2,5)),\n",
    "        'beta':    np.random.normal(0, 1, size=(2,5)),\n",
    "        'sigma_e': np.abs(np.random.normal(1, 0.2, size=(2,5))),\n",
    "    }\n",
    "\n",
    "    # 4) Log‑likelihood (for each chain, draw, obs)\n",
    "    log_lik = np.random.normal(-1, 0.5, size=(2,5,10))\n",
    "\n",
    "    # 5) Build InferenceData\n",
    "    idata = az.from_dict(\n",
    "        posterior=posterior,\n",
    "        posterior_predictive={'y_obs': y_obs},\n",
    "        log_likelihood     ={'y_obs': log_lik},\n",
    "        dims={\n",
    "            'y_obs': ['chain','draw','obs'],\n",
    "            'mu':    ['chain','draw'],\n",
    "            'beta':  ['chain','draw'],\n",
    "            'sigma_e':['chain','draw'],\n",
    "        },\n",
    "        coords={\n",
    "            'chain': [0,1],\n",
    "            'draw':  list(range(5)),\n",
    "            'obs':   list(range(10))\n",
    "        }\n",
    "    )\n",
    "\n",
    "    print(\"=== Classical Metrics ===\")\n",
    "    compute_classical_metrics(idata, y_true)\n",
    "\n",
    "    print(\"\\n=== Calibration ===\")\n",
    "    compute_calibration(idata, y_true)\n",
    "\n",
    "    print(\"\\n=== Bayesian Metrics ===\")\n",
    "    compute_bayesian_metrics(idata)\n",
    "\n",
    "    print(\"\\n=== Convergence Diagnostics ===\")\n",
    "    compute_convergence_diagnostics(idata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/hierarchical_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/hierarchical_utils.py\n",
    "\n",
    "import arviz as az\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(idata, file_path: str, overwrite: bool = True):\n",
    "    \"\"\"Save ArviZ InferenceData to NetCDF.\"\"\"\n",
    "    idata.to_netcdf(file_path, engine=\"h5netcdf\", overwrite_existing=overwrite)\n",
    "    print(f\"✔︎ saved model → {file_path}\")\n",
    "\n",
    "def load_model(file_path: str):\n",
    "    \"\"\"Load ArviZ InferenceData from NetCDF.\"\"\"\n",
    "    idata = az.from_netcdf(file_path, engine=\"h5netcdf\")\n",
    "    print(f\"✔︎ loaded model ← {file_path}\")\n",
    "    return idata\n",
    "\n",
    "def save_preprocessor(transformer, file_path: str):\n",
    "    \"\"\"Save the scikit-learn transformer to a joblib file.\"\"\"\n",
    "    joblib.dump(transformer, file_path)\n",
    "    print(f\"✔︎ saved preprocessor → {file_path}\")\n",
    "\n",
    "def load_preprocessor(file_path: str):\n",
    "    \"\"\"Load the scikit-learn transformer from a joblib file.\"\"\"\n",
    "    transformer = joblib.load(file_path)\n",
    "    print(f\"✔︎ loaded preprocessor ← {file_path}\")\n",
    "    return transformer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # === Editable settings ===\n",
    "    # Path to the saved model (NetCDF format)\n",
    "    MODEL_PATH = \"data/models/saved_models/model.nc\"\n",
    "    # Path to the preprocessor\n",
    "    PREPROC_PATH = \"data/models/saved_models/preprocessor.joblib\"\n",
    "    # Input data for prediction (raw CSV with exit velocity data)\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    # Output predictions file (CSV) or set to None to print to console\n",
    "    OUTPUT_PREDS_2024 = \"data/predictions/predictions_2024.csv\"  # <-- EDITABLE: set output CSV path or None\n",
    "    \n",
    "    model = load_model(MODEL_PATH)\n",
    "    save_model(model, MODEL_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/posterior.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/posterior.py\n",
    "# src/utils/posterior.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "\n",
    "# ── REPLACEMENT: paste over the whole function ─────────────────────────\n",
    "import json, pathlib, numpy as np, pandas as pd, arviz as az\n",
    "\n",
    "JSON_GLOBAL = pathlib.Path(\"data/models/saved_models/global_effects.json\")\n",
    "\n",
    "def posterior_to_frame(idata: az.InferenceData) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a batter‑level summary **AND** writes global effects to JSON.\n",
    "\n",
    "    File written  ➜  data/models/saved_models/global_effects.json\n",
    "    \"\"\"\n",
    "    post = idata.posterior\n",
    "\n",
    "    # ------- per‑batter u summaries -----------------------------------\n",
    "    u   = post[\"u\"]                                         # (chain,draw,batter)\n",
    "    stats = {\n",
    "        \"u_mean\"  : u.mean((\"chain\",\"draw\")).values,\n",
    "        \"u_sd\"    : u.std ((\"chain\",\"draw\")).values,\n",
    "        \"u_q2.5\"  : np.percentile(u.values,  2.5, axis=(0,1)),\n",
    "        \"u_q50\"   : np.percentile(u.values, 50.0, axis=(0,1)),\n",
    "        \"u_q97.5\" : np.percentile(u.values,97.5, axis=(0,1)),\n",
    "    }\n",
    "    df = pd.DataFrame({\"batter_idx\": np.arange(u.shape[-1]), **stats})\n",
    "\n",
    "    # ------- global effects ------------------------------------------\n",
    "    mu_mean = post[\"mu\"].mean().item()\n",
    "\n",
    "    # β_age  ➜ last entry of beta vector (age_centered was added last)\n",
    "    beta  = post[\"beta\"]\n",
    "    feat_dim = [d for d in beta.dims if d not in (\"chain\",\"draw\")][0]\n",
    "    beta_age = beta.isel({feat_dim: -1}).mean().item()\n",
    "\n",
    "    beta_level = post[\"beta_level\"].mean((\"chain\",\"draw\")).values.tolist()\n",
    "    sigma_b    = post[\"sigma_b\"].mean().item()\n",
    "    sigma_e    = post[\"sigma_e\"].mean().item()\n",
    "\n",
    "    global_eff = dict(\n",
    "        mu_mean=mu_mean,\n",
    "        beta_age=beta_age,\n",
    "        beta_level=beta_level,\n",
    "        sigma_b=sigma_b,\n",
    "        sigma_e=sigma_e,\n",
    "        median_age=idata.attrs.get(\"median_age\", 26.0),\n",
    "        beta=post[\"beta\"].mean((\"chain\",\"draw\")).values.tolist(),  # Save all beta coefficients\n",
    "        feature_names=idata.attrs.get(\"feature_names\", [])  # Also save feature names if available\n",
    "    )\n",
    "\n",
    "    # ➜  write side‑car JSON (overwrite every run)\n",
    "    JSON_GLOBAL.write_text(json.dumps(global_eff, indent=2))\n",
    "    print(f\"✔︎ wrote global effects → {JSON_GLOBAL}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def align_batter_codes(df_roster: pd.DataFrame,\n",
    "                       train_cats: pd.Index) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert integer batter_ids in *roster* into the categorical codes\n",
    "    **identical** to what the model saw during training.\n",
    "\n",
    "    Any unseen batter gets code = -1 (handled later).\n",
    "    \"\"\"\n",
    "    cat = pd.Categorical(df_roster[\"batter_id\"], categories=train_cats)\n",
    "    return pd.Series(cat.codes, index=df_roster.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/validation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/validation.py\n",
    "\"\"\"\n",
    "Generic K‑fold validator.\n",
    "\n",
    "• Works for sklearn Pipelines *or* PyMC idata.\n",
    "• Decides how to extract predictions based on\n",
    "  the object returned by `fit_func`.\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from arviz import InferenceData\n",
    "import arviz as az\n",
    "from typing import Callable, List, Union\n",
    "import statsmodels as sm\n",
    "\n",
    "\n",
    "\n",
    "def _rmse(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    return np.sqrt(np.mean((a - b) ** 2))\n",
    "\n",
    "\n",
    "def run_kfold_cv(\n",
    "    fit_func: Callable[[pd.DataFrame, pd.DataFrame], tuple],\n",
    "    df: pd.DataFrame,\n",
    "    k: int = 5,\n",
    "    random_state: int = 0,\n",
    "    **fit_kw\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    Sklearn-style K-fold CV:\n",
    "      fit_func(train_df, test_df, **fit_kw) -> (model, rmse)\n",
    "    Returns a list of rmse scores.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    rmses: List[float] = []\n",
    "    for train_idx, test_idx in kf.split(df):\n",
    "        train, test = df.iloc[train_idx], df.iloc[test_idx]\n",
    "        _, rmse = fit_func(train, test, **fit_kw)\n",
    "        rmses.append(rmse)\n",
    "    return rmses\n",
    "\n",
    "\n",
    "\n",
    "def run_kfold_cv_pymc(\n",
    "    fit_func: Callable[[pd.DataFrame], InferenceData],\n",
    "    df: pd.DataFrame,\n",
    "    k: int = 5,\n",
    "    random_state: int = 0\n",
    ") -> list[float]:\n",
    "    \"\"\"\n",
    "    PyMC K-fold CV: \n",
    "      fit_func(train_df) -> ArviZ InferenceData with posterior a[group] & sigma.\n",
    "    Returns RMSE on each hold-out fold.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    rmses: list[float] = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(df):\n",
    "        train_df, test_df = df.iloc[train_idx], df.iloc[test_idx]\n",
    "        idata = fit_func(train_df)\n",
    "\n",
    "        # 1) grab posterior samples of the group intercepts and noise σ\n",
    "        #    stack chain+draw into one dim of length M\n",
    "        a_samples = (\n",
    "            idata.posterior[\"a\"]\n",
    "            .stack(sample=(\"chain\", \"draw\"))    # dims now (\"group\",\"sample\")\n",
    "            .transpose(\"sample\", \"group\")       # shape (M, n_groups)\n",
    "            .values\n",
    "        )\n",
    "        sigma_samples = (\n",
    "            idata.posterior[\"sigma\"]\n",
    "            .stack(sample=(\"chain\", \"draw\"))    # shape (M,)\n",
    "            .values\n",
    "        )\n",
    "\n",
    "        # 2) for each test row, pull the intercept for its group\n",
    "        groups = test_df[\"group\"].values       # shape (n_test,)\n",
    "        # a_samples[:, groups] → shape (M, n_test)\n",
    "        pred_samples = a_samples[:, groups]\n",
    "\n",
    "        # 3) point estimate = posterior mean for each test row\n",
    "        pred_mean = pred_samples.mean(axis=0)  # shape (n_test,)\n",
    "\n",
    "        # 4) compute RMSE against true exit_velo\n",
    "        true_vals = test_df[\"exit_velo\"].values\n",
    "        rmse = np.sqrt(np.mean((pred_mean - true_vals) ** 2))\n",
    "        rmses.append(rmse)\n",
    "\n",
    "    return rmses\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# helper to score a *single* train/test split for idata\n",
    "def rmse_pymc(idata: InferenceData, test_df: pd.DataFrame) -> float:\n",
    "    \"\"\"\n",
    "    Compute RMSE by comparing posterior-mean predictions\n",
    "    (using the group intercept `a`) against observed `exit_velo`.\n",
    "    \"\"\"\n",
    "    # 1) Stack (chain, draw) into a single \"sample\" dimension\n",
    "    a_samples = (\n",
    "        idata.posterior[\"a\"]\n",
    "        .stack(sample=(\"chain\", \"draw\"))    # dims → (\"group\",\"sample\")\n",
    "        .transpose(\"sample\", \"group\")       # shape → (M, n_groups)\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # 2) Gather the sampled intercepts for each test row\n",
    "    groups = test_df[\"group\"].values       # shape → (n_test,)\n",
    "    pred_samples = a_samples[:, groups]    # shape → (M, n_test)\n",
    "\n",
    "    # 3) Posterior mean prediction for each test instance\n",
    "    pred_mean = pred_samples.mean(axis=0)  # shape → (n_test,)\n",
    "\n",
    "    # 4) Compute and return RMSE versus the true values\n",
    "    true_vals = test_df[\"exit_velo\"].values\n",
    "    return np.sqrt(np.mean((pred_mean - true_vals) ** 2))\n",
    "\n",
    "\n",
    "def posterior_predictive_check(idata, df, batter_idx):\n",
    "    \"\"\"\n",
    "    Plot observed vs. simulated exit-velo histograms.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    obs = df[\"exit_velo\"].values\n",
    "    sim = (\n",
    "        idata.posterior_predictive[\"y_obs\"]\n",
    "        .stack(samples=(\"chain\", \"draw\"))\n",
    "        .values.flatten()\n",
    "    )\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(8, 3))\n",
    "    ax[0].hist(obs, bins=30)\n",
    "    ax[0].set_title(\"Observed\")\n",
    "    ax[1].hist(sim, bins=30)\n",
    "    ax[1].set_title(\"Simulated\")\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prediction_interval(model, X, alpha=0.05, method='linear'):\n",
    "    \"\"\"\n",
    "    Compute prediction intervals for a model.\n",
    "    \"\"\"\n",
    "    if method == 'linear':\n",
    "        # For OLS and Ridge\n",
    "        X_const = sm.add_constant(X)\n",
    "        preds = model.get_prediction(X_const)\n",
    "        pred_int = preds.conf_int(alpha=alpha)\n",
    "        return preds.predicted_mean, pred_int[:, 0], pred_int[:, 1]\n",
    "    elif method == 'bayesian':\n",
    "        # For Bayesian models\n",
    "        hdi = az.hdi(model, hdi=1 - alpha)\n",
    "        return (\n",
    "            hdi.posterior_predictive.y_obs.sel(hdi=f\"{alpha/2*100}%\"),\n",
    "            hdi.posterior_predictive.y_obs.sel(hdi=f\"{(1-alpha/2)*100}%\")\n",
    "        )\n",
    "    elif method == 'gbm':\n",
    "        # For XGBoost quantile regression\n",
    "        lower = model.predict(X, pred_contribs=False, iteration_range=(0, model.best_iteration))\n",
    "        upper = model.predict(X, pred_contribs=False, iteration_range=(0, model.best_iteration))\n",
    "        return lower, upper  # Replace with actual quantile regression\n",
    "    else:\n",
    "        raise ValueError(\"Method not supported\")\n",
    "\n",
    "# Example for bootstrapping GBM\n",
    "def bootstrap_prediction_interval(model, X, n_bootstraps=1000, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Compute nonparametric bootstrap confidence intervals for predictions.\n",
    "\n",
    "    Must support either:\n",
    "      - model.predict(X) -> array_like shape (n_obs,)\n",
    "      - model.get_prediction(X).predicted_mean -> array_like shape (n_obs,)\n",
    "\n",
    "    Returns lower, upper arrays of shape (n_obs,) at the given alpha level.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "\n",
    "    # Determine number of observations\n",
    "    try:\n",
    "        n_obs = X.shape[0]\n",
    "    except Exception:\n",
    "        raise ValueError(\"X must have a valid `.shape[0]`\")\n",
    "\n",
    "    # Allocate storage\n",
    "    all_preds = np.zeros((n_bootstraps, n_obs))\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        # Sample indices with replacement\n",
    "        idx = np.random.randint(0, n_obs, size=n_obs)\n",
    "        sampled = X.iloc[idx] if hasattr(X, \"iloc\") else X[idx]\n",
    "\n",
    "        # Dispatch to correct predict method\n",
    "        if hasattr(model, \"predict\"):\n",
    "            preds_i = model.predict(sampled)\n",
    "        elif hasattr(model, \"get_prediction\"):\n",
    "            result = model.get_prediction(sampled)\n",
    "            preds_i = getattr(result, \"predicted_mean\", None)\n",
    "            if preds_i is None:\n",
    "                raise AttributeError(\n",
    "                    \"get_prediction(...) did not return `predicted_mean`\"\n",
    "                )\n",
    "        else:\n",
    "            raise AttributeError(\n",
    "                f\"Model {model!r} has no `predict` or `get_prediction` method\"\n",
    "            )\n",
    "\n",
    "        all_preds[i, :] = preds_i\n",
    "\n",
    "    # Compute CI percentiles\n",
    "    lower = np.percentile(all_preds, 100 * (alpha / 2), axis=0)\n",
    "    upper = np.percentile(all_preds, 100 * (1 - alpha / 2), axis=0)\n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import arviz as az\n",
    "    import statsmodels.api as sm\n",
    "    import pymc as pm\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    # from src.utils.validation import (\n",
    "    # run_kfold_cv_pymc, prediction_interval, \n",
    "    # bootstrap_prediction_interval, rmse_pymc, \n",
    "    # posterior_predictive_check\n",
    "    # )\n",
    "\n",
    "    # 1) Synthetic data for sklearn\n",
    "    np.random.seed(42)\n",
    "    X = np.linspace(0, 10, 40)\n",
    "    y = 2 * X + 1 + np.random.normal(0, 1, size=X.shape)\n",
    "    df_skl = pd.DataFrame({\"feature\": X, \"exit_velo\": y})\n",
    "\n",
    "    def fit_sklearn(train, test):\n",
    "        X_tr, y_tr = train[[\"feature\"]], train[\"exit_velo\"]\n",
    "        X_te, y_te = test[[\"feature\"]], test[\"exit_velo\"]\n",
    "        m = LinearRegression().fit(X_tr, y_tr)\n",
    "        preds = m.predict(X_te)\n",
    "        rmse = np.sqrt(np.mean((preds - y_te) ** 2))\n",
    "        return m, rmse\n",
    "\n",
    "    print(\"\\n--- sklearn CV ---\")\n",
    "    print(\"RMSEs:\", run_kfold_cv(fit_sklearn, df_skl, k=4))\n",
    "\n",
    "    # 2) Synthetic hierarchical data for PyMC\n",
    "    #    (3 groups, group‐level intercepts)\n",
    "    n_per = 20\n",
    "    groups = np.repeat(np.arange(3), n_per)\n",
    "    true_alpha = np.array([1.0, 3.0, 5.0])\n",
    "    y_hier = true_alpha[groups] + np.random.normal(0, 1, size=groups.size)\n",
    "    df_hier = pd.DataFrame({\"exit_velo\": y_hier, \"group\": groups})\n",
    "\n",
    "    def fit_hierarchical(train_df: pd.DataFrame) -> az.InferenceData:\n",
    "        \"\"\"\n",
    "        Fit a hierarchical model and append posterior_predictive samples for y_obs.\n",
    "        Returns:\n",
    "            InferenceData with .posterior and .posterior_predictive[\"y_obs\"].\n",
    "        \"\"\"\n",
    "        coords = {\"group\": np.unique(train_df[\"group\"])}\n",
    "        with pm.Model(coords=coords) as model:\n",
    "            mu_a = pm.Normal(\"mu_a\", 0, 5)\n",
    "            sigma_a = pm.HalfNormal(\"sigma_a\", 5)\n",
    "            a = pm.Normal(\"a\", mu=mu_a, sigma=sigma_a, dims=\"group\")\n",
    "            sigma = pm.HalfNormal(\"sigma\", 1)\n",
    "            y_grp = a[train_df[\"group\"].values]\n",
    "            pm.Normal(\"y_obs\", mu=y_grp, sigma=sigma,\n",
    "                    observed=train_df[\"exit_velo\"].values)\n",
    "\n",
    "            # 1) Draw posterior samples\n",
    "            idata = pm.sample(\n",
    "                draws=500, tune=500,\n",
    "                chains=2, cores=1,\n",
    "                return_inferencedata=True,\n",
    "                progressbar=False\n",
    "            )\n",
    "\n",
    "            # 2) Generate posterior_predictive samples for 'y_obs'\n",
    "            pm.sample_posterior_predictive(\n",
    "                idata,\n",
    "                model=model,\n",
    "                var_names=[\"y_obs\"],\n",
    "                extend_inferencedata=True,\n",
    "                random_seed=42,\n",
    "                progressbar=False\n",
    "            )\n",
    "\n",
    "        # 3) Return the enriched InferenceData\n",
    "        return idata\n",
    "\n",
    "\n",
    "    print(\"\\n--- PyMC hierarchical CV ---\")\n",
    "    bayes_rmse = run_kfold_cv_pymc(fit_hierarchical, df_hier, k=3)\n",
    "    print(\"Bayesian RMSEs:\", bayes_rmse)\n",
    "\n",
    "    # 3) Test prediction_interval (linear OLS)\n",
    "    df_ols = df_skl.copy()\n",
    "    df_ols[\"const\"] = 1.0\n",
    "    ols = sm.OLS(df_ols[\"exit_velo\"], df_ols[[\"const\", \"feature\"]]).fit()\n",
    "    mean, lo, hi = prediction_interval(ols, df_skl[[\"feature\"]], method=\"linear\")\n",
    "    print(\"\\n--- OLS prediction_interval shapes ---\")\n",
    "    print(\"mean:\", mean.shape, \"lower:\", lo.shape, \"upper:\", hi.shape)\n",
    "\n",
    "    # 4) Test bootstrap_prediction_interval with LinearRegression\n",
    "    lr = LinearRegression().fit(df_skl[[\"feature\"]], df_skl[\"exit_velo\"])\n",
    "    lower, upper = bootstrap_prediction_interval(lr, df_skl[[\"feature\"]], n_bootstraps=200)\n",
    "    print(\"\\n--- bootstrap_prediction_interval shapes ---\")\n",
    "    print(\"lower:\", lower.shape, \"upper:\", upper.shape)\n",
    "\n",
    "    # 5) rmse_pymc + posterior_predictive_check on a single InferenceData\n",
    "    #    (we can re-use the last fold of the hierarchical example)\n",
    "    last_idata = fit_hierarchical(df_hier)\n",
    "    rmse_val = rmse_pymc(last_idata, df_hier)\n",
    "    print(\"\\n--- rmse_pymc on full data ---\", rmse_val)\n",
    "\n",
    "    # 6) posterior_predictive_check plot\n",
    "    print(\"\\n--- posterior_predictive_check (figure) ---\")\n",
    "    fig = posterior_predictive_check(last_idata, df_hier, batter_idx=None)\n",
    "    fig.savefig(\"ppc_hist.png\")\n",
    "    print(\"Saved histogram to ppc_hist.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/jax_gpu_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/jax_gpu_utils.py\n",
    "\n",
    "import os, subprocess, json, logging, platform, jax\n",
    "from jax.lib import xla_bridge\n",
    "\n",
    "def gpu_diagnostics():\n",
    "    info = {\n",
    "        \"backend\":      xla_bridge.get_backend().platform,\n",
    "        \"devices\":      [str(d) for d in jax.devices()],\n",
    "        \"python\":       platform.python_version(),\n",
    "        \"ld_library_path\": os.getenv(\"LD_LIBRARY_PATH\",\"<unset>\"),\n",
    "    }\n",
    "    if shutil.which(\"nvidia-smi\"):\n",
    "        try:\n",
    "            smi = subprocess.check_output(\n",
    "                [\"nvidia-smi\", \"--query-gpu=name,driver_version,memory.total\",\n",
    "                 \"--format=csv,noheader,nounits\"]\n",
    "            )\n",
    "            info[\"nvidia-smi\"] = smi.decode().strip()\n",
    "        except Exception as exc:\n",
    "            info[\"nvidia-smi-error\"] = repr(exc)\n",
    "    return info\n",
    "\n",
    "def log_gpu_diagnostics(level=logging.INFO):\n",
    "    logging.getLogger(__name__).log(\n",
    "        level,\n",
    "        \"GPU-diag: %s\",\n",
    "        json.dumps(gpu_diagnostics(), indent=2)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/models/bayesian_alternatives.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/bayesian_alternatives.py\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# src/models/bayesian_alternatives.py\n",
    "# Implements additional Bayesian engines that all return\n",
    "# arviz.InferenceData so downstream metrics remain unchanged.\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "from __future__ import annotations\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import tempfile\n",
    "import os\n",
    "from cmdstanpy import CmdStanModel  # Stan program wrapper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "import pyjags   \n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp  \n",
    "# 1)  CmdStanPy  ---------------------------------------------------------\n",
    "def fit_bayesian_cmdstanpy(\n",
    "    stan_code: str,\n",
    "    stan_data: dict,\n",
    "    *,\n",
    "    draws: int = 1000,\n",
    "    warmup: int = 500,\n",
    "    chains: int = 4,\n",
    "    seed: int = 42,\n",
    ") -> az.InferenceData:\n",
    "    \"\"\"\n",
    "    Compile a Stan model from source code string and return ArviZ InferenceData.\n",
    "    This writes the code to a temporary file, compiles it, samples, and cleans up.\n",
    "    \"\"\"\n",
    "    # 1) Write the Stan code to a temp file\n",
    "    with tempfile.NamedTemporaryFile(\n",
    "        mode=\"w\", suffix=\".stan\", delete=False\n",
    "    ) as tmp:\n",
    "        tmp.write(stan_code)\n",
    "        stan_file = tmp.name  # Path to pass to CmdStanModel\n",
    "\n",
    "    try:\n",
    "        # 2) Instantiate and compile the model from the .stan file\n",
    "        model = CmdStanModel(\n",
    "            stan_file=stan_file,\n",
    "            force_compile=True\n",
    "        )\n",
    "\n",
    "        # 3) Draw samples using HMC/NUTS\n",
    "        fit = model.sample(\n",
    "            data=stan_data,\n",
    "            iter_sampling=draws,\n",
    "            iter_warmup=warmup,\n",
    "            chains=chains,\n",
    "            seed=seed\n",
    "        )\n",
    "\n",
    "        # 4) Convert to ArviZ InferenceData\n",
    "        idata = az.from_cmdstanpy(posterior=fit)\n",
    "        return idata\n",
    "\n",
    "    finally:\n",
    "        # 5) Cleanup temp files to prevent clutter\n",
    "        try:\n",
    "            os.remove(stan_file)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "\n",
    "\n",
    "# 2)  PyJAGS (Gibbs)  ----------------------------------------------------\n",
    "def fit_bayesian_pyjags(\n",
    "    jags_model: str,\n",
    "    jags_data: dict,\n",
    "    *,\n",
    "    draws: int = 5000,\n",
    "    burn: int = 1000,\n",
    "    thin: int = 1,\n",
    "    chains: int = 4,\n",
    "    seed: int = 42,\n",
    ") -> az.InferenceData:\n",
    "    \"\"\"\n",
    "    Fit a JAGS model via PyJAGS and convert to InferenceData.\n",
    "    \"\"\"                                               # PyJAGS interface :contentReference[oaicite:10]{index=10}:contentReference[oaicite:11]{index=11}\n",
    "    model = pyjags.Model(code=jags_model,\n",
    "                         data=jags_data,\n",
    "                         chains=chains,\n",
    "                         adapt=burn,\n",
    "                         init=[{\".RNG.seed\": seed + c * 10} for c in range(chains)])\n",
    "    samples = model.sample(draws, vars=None, thin=thin)\n",
    "    idata   = az.from_pyjags(posterior=samples)\n",
    "    return idata\n",
    "\n",
    "\n",
    "# 3)  NumPyro (NUTS)  ----------------------------------------------------\n",
    "def fit_bayesian_numpyro(\n",
    "    numpyro_model,\n",
    "    rng_key,\n",
    "    *,\n",
    "    draws: int = 1000,\n",
    "    warmup: int = 500,\n",
    "    chains: int = 4,\n",
    ") -> az.InferenceData:\n",
    "    \"\"\"\n",
    "    Run NUTS in NumPyro; return InferenceData (auto-vectorises across chains).\n",
    "    \"\"\"\n",
    "    import jax\n",
    "    from numpyro.infer import MCMC, NUTS                           # NumPyro HMC/NUTS :contentReference[oaicite:12]{index=12}:contentReference[oaicite:13]{index=13}\n",
    "    kernel = NUTS(numpyro_model)\n",
    "    mcmc   = MCMC(kernel,\n",
    "                  num_warmup=warmup,\n",
    "                  num_samples=draws,\n",
    "                  num_chains=chains,\n",
    "                  progress_bar=False)\n",
    "    mcmc.run(rng_key)\n",
    "    idata  = az.from_numpyro(mcmc)\n",
    "    return idata\n",
    "\n",
    "\n",
    "# 4)  TensorFlow Probability HMC  ---------------------------------------\n",
    "def fit_bayesian_tfp_hmc(\n",
    "    target_log_prob_fn,\n",
    "    init_state,\n",
    "    *,\n",
    "    step_size: float = 0.05,\n",
    "    leapfrog_steps: int = 5,\n",
    "    draws: int = 1000,\n",
    "    burnin: int = 500,\n",
    "    seed: int = 42,\n",
    ") -> az.InferenceData:\n",
    "    \"\"\"\n",
    "    Vanilla single-chain HMC in TensorFlow Probability.\n",
    "    \"\"\"                         # TFP HMC kernel :contentReference[oaicite:14]{index=14}:contentReference[oaicite:15]{index=15}\n",
    "    tfd, tfmcmc = tfp.distributions, tfp.mcmc\n",
    "\n",
    "    hmc = tfmcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=target_log_prob_fn,\n",
    "        step_size=step_size,\n",
    "        num_leapfrog_steps=leapfrog_steps)\n",
    "\n",
    "    adaptive_hmc = tfmcmc.SimpleStepSizeAdaptation(\n",
    "        inner_kernel=hmc,\n",
    "        num_adaptation_steps=int(0.8 * burnin))\n",
    "\n",
    "    @tf.function(autograph=False, jit_compile=True)\n",
    "    def _run_chain():\n",
    "        return tfmcmc.sample_chain(\n",
    "            num_results=draws,\n",
    "            current_state=init_state,\n",
    "            kernel=adaptive_hmc,\n",
    "            num_burnin_steps=burnin,\n",
    "            seed=seed)\n",
    "\n",
    "    samples, _ = _run_chain()\n",
    "    # Wrap in dict so ArviZ can coerce\n",
    "    posterior = {f\"param_{i}\": s.numpy() for i, s in enumerate(samples)}\n",
    "    idata     = az.from_dict(posterior=posterior)\n",
    "    return idata\n",
    "\n",
    "\n",
    "# 5)  PyMC ADVI (fast VI baseline)  --------------------------------------\n",
    "def fit_bayesian_pymc_advi(\n",
    "    pymc_model,\n",
    "    *,\n",
    "    draws: int = 1000,\n",
    "    tune: int = 10000,\n",
    ") -> az.InferenceData:\n",
    "    \"\"\"\n",
    "    Run Automatic Differentiation VI (ADVI) in PyMC —> InferenceData.\n",
    "    \"\"\"\n",
    "    import pymc as pm                                              # PyMC ADVI API :contentReference[oaicite:16]{index=16}:contentReference[oaicite:17]{index=17}\n",
    "    with pymc_model:\n",
    "        approx = pm.fit(n=tune, method=\"advi\", progressbar=False)  # returns Approximation\n",
    "        idata  = approx.sample(draws)\n",
    "    return idata\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ----------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    import jax.random as jr\n",
    "    import tensorflow_probability as tfp\n",
    "    import tensorflow as tf\n",
    "    import pymc as pm\n",
    "    from cmdstanpy import CmdStanModel\n",
    "    import arviz as az\n",
    "    import matplotlib.pyplot as plt\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "\n",
    "    # ─── User-configurable sampling parameters ────────────────────────\n",
    "    N_CHAINS   = 4      # number of MCMC chains (PyMC, Stan, NumPyro, TFP)\n",
    "    N_DRAWS    = 500   # number of samples to keep per chain\n",
    "    N_TUNE     = 500   # number of tuning/adaptation steps per chain\n",
    "    PP_DRAWS   = 250    # draws for posterior_predictive in PyMC-ADVI & attach_ppc\n",
    "    JAGS_DRAWS = 500   # total samples for PyJAGS\n",
    "    JAGS_BURN  = 500   # adaptation steps for PyJAGS\n",
    "    SEED       = 42     # global RNG seed\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # ─── Synthetic data ────────────────────────────────────────────────\n",
    "    N       = 50\n",
    "    rng     = np.random.default_rng(SEED)\n",
    "    x_data  = rng.uniform(-2, 2, size=N)\n",
    "    y_true  = 1.0 + 2.5 * x_data\n",
    "    y_obs   = y_true + rng.normal(0, 0.8, size=N)\n",
    "\n",
    "    # ─── Helper: ensure posterior_predictive[\"y_obs\"] exists ───────────\n",
    "    def _attach_ppc(idata: az.InferenceData,\n",
    "                    x: np.ndarray,\n",
    "                    alpha: str = \"alpha\",\n",
    "                    beta: str = \"beta\",\n",
    "                    sigma: str = \"sigma\",\n",
    "                    draws: int | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Unified, robust method to add posterior predictive samples to idata.\n",
    "        Ensures a uniform (chain, draw, obs) array is passed to ArviZ.\n",
    "        \"\"\"\n",
    "        post = idata.posterior\n",
    "\n",
    "        # 1) Extract posterior arrays via stacking\n",
    "        a = post[alpha].stack(samples=(\"chain\", \"draw\")).values  # shape: (n_chains*n_draws,)\n",
    "        b = post[beta].stack(samples=(\"chain\", \"draw\")).values\n",
    "        s = post[sigma].stack(samples=(\"chain\", \"draw\")).values\n",
    "\n",
    "        # 2) Determine dimensions\n",
    "        n_chains = post.sizes.get(\"chain\", 1)                  # single‐chain ADVI fallback\n",
    "        n_draws  = post.sizes[\"draw\"]\n",
    "        n_obs    = x.shape[0]\n",
    "        M        = min(a.size, draws) if draws is not None else a.size\n",
    "\n",
    "        # 3) Build predictive draws (shape: M × n_obs)\n",
    "        mu  = a[:M, None] + b[:M, None] * x[None, :]\n",
    "        ypp = rng.normal(loc=mu, scale=s[:M, None])\n",
    "\n",
    "        # 4) Reshape to (chain, draw, obs)\n",
    "        arr = ypp.reshape(n_chains, n_draws, n_obs)\n",
    "\n",
    "        # 5) Attach to InferenceData, letting ArviZ handle chain/draw dims\n",
    "        idata.add_groups(\n",
    "            posterior_predictive={\"y_obs\": arr},\n",
    "            dims={\"y_obs\": [\"obs\"]},\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    # ─── 1) PyMC HMC (NUTS) ───────────────────────────────────────────\n",
    "    coords = {\"obs\": np.arange(N)}\n",
    "    with pm.Model(coords=coords) as pymc_model:\n",
    "        alpha = pm.Normal(\"alpha\", 0, 5)\n",
    "        beta  = pm.Normal(\"beta\",  0, 5)\n",
    "        sigma = pm.HalfNormal(\"sigma\", 1)\n",
    "        mu    = alpha + beta * x_data\n",
    "        pm.Normal(\"y\", mu, sigma, observed=y_obs, dims=\"obs\")\n",
    "\n",
    "    idata_pymc_hmc = pm.sample(\n",
    "        N_DRAWS,\n",
    "        tune=N_TUNE,\n",
    "        chains=N_CHAINS,\n",
    "        random_seed=SEED,\n",
    "        progressbar=False,\n",
    "        return_inferencedata=True,\n",
    "        model=pymc_model\n",
    "    )\n",
    "    pm.sample_posterior_predictive(\n",
    "        idata_pymc_hmc,\n",
    "        var_names=[\"y\"],\n",
    "        extend_inferencedata=True,\n",
    "        model=pymc_model\n",
    "    )\n",
    "    idata_pymc_hmc.posterior_predictive = \\\n",
    "        idata_pymc_hmc.posterior_predictive.rename({\"y\": \"y_obs\"})\n",
    "\n",
    "    # ─── 2) PyMC ADVI ─────────────────────────────────────────────────\n",
    "    idata_pymc_advi = fit_bayesian_pymc_advi(pymc_model,\n",
    "                                             draws=PP_DRAWS,\n",
    "                                             tune=N_TUNE*5)\n",
    "    _attach_ppc(idata_pymc_advi, x_data, draws=PP_DRAWS)\n",
    "\n",
    "    # ─── 3) CmdStanPy (Stan HMC) ──────────────────────────────────────\n",
    "    stan_code = \"\"\"\n",
    "    data { int<lower=0> N; vector[N] x; vector[N] y; }\n",
    "    parameters { real alpha; real beta; real<lower=0> sigma; }\n",
    "    model { y ~ normal(alpha+beta*x, sigma); }\n",
    "    generated quantities {\n",
    "      vector[N] y_obs;\n",
    "      for (n in 1:N)\n",
    "        y_obs[n] = normal_rng(alpha+beta*x[n], sigma);\n",
    "    }\n",
    "    \"\"\"\n",
    "    stan_data = {\"N\": N, \"x\": x_data, \"y\": y_obs}\n",
    "    idata_stan = fit_bayesian_cmdstanpy(stan_code,\n",
    "                                        stan_data,\n",
    "                                        draws=N_DRAWS,\n",
    "                                        warmup=N_TUNE,\n",
    "                                        chains=N_CHAINS,\n",
    "                                        seed=SEED)\n",
    "\n",
    "    # ─── 4) PyJAGS (Gibbs Sampling) ───────────────────────────────────\n",
    "    jags_model = \"\"\"\n",
    "    model {\n",
    "      for (i in 1:N) {\n",
    "        y[i] ~ dnorm(alpha+beta*x[i], tau);\n",
    "        y_obs[i] <- alpha + beta*x[i] + sigma*randn();\n",
    "      }\n",
    "      alpha ~ dnorm(0, .001);\n",
    "      beta  ~ dnorm(0, .001);\n",
    "      sigma ~ dunif(0, 10);\n",
    "      tau   <- pow(sigma, -2);\n",
    "    }\n",
    "    \"\"\"\n",
    "    jags_data = {\"N\": N, \"x\": x_data, \"y\": y_obs}\n",
    "    idata_jags = fit_bayesian_pyjags(jags_model,\n",
    "                                     jags_data,\n",
    "                                     draws=JAGS_DRAWS,\n",
    "                                     burn=JAGS_BURN,\n",
    "                                     chains=N_CHAINS,\n",
    "                                     seed=SEED)\n",
    "    _attach_ppc(idata_jags, x_data, draws=PP_DRAWS)\n",
    "\n",
    "    # ─── 5) NumPyro (NUTS) ─────────────────────────────────────────────\n",
    "    import numpyro, numpyro.distributions as dist\n",
    "    def numpyro_model(x, y=None):\n",
    "        a = numpyro.sample(\"alpha\", dist.Normal(0,5))\n",
    "        b = numpyro.sample(\"beta\",  dist.Normal(0,5))\n",
    "        s = numpyro.sample(\"sigma\", dist.HalfNormal(1))\n",
    "        mu = a + b * x\n",
    "        numpyro.sample(\"y\", dist.Normal(mu, s), obs=y)\n",
    "    rng_key = jr.PRNGKey(SEED)\n",
    "    idata_numpyro = fit_bayesian_numpyro(numpyro_model,\n",
    "                                         rng_key,\n",
    "                                         draws=N_DRAWS,\n",
    "                                         warmup=N_TUNE,\n",
    "                                         chains=N_CHAINS)\n",
    "    _attach_ppc(idata_numpyro, x_data, draws=PP_DRAWS)\n",
    "\n",
    "    # ─── 6) TFP HMC ───────────────────────────────────────────────────\n",
    "    tfd, tfmcmc = tfp.distributions, tfp.mcmc\n",
    "    def target_log_prob_fn(alpha, beta, log_sigma):\n",
    "        sigma = tf.exp(log_sigma)\n",
    "        yhat  = alpha + beta * tf.constant(x_data, tf.float32)\n",
    "        return tf.reduce_sum(tfd.Normal(yhat, sigma).log_prob(y_obs)) + \\\n",
    "               tfd.Normal(0,5).log_prob(alpha) + \\\n",
    "               tfd.Normal(0,5).log_prob(beta) + \\\n",
    "               tfd.Normal(0,1).log_prob(log_sigma)\n",
    "\n",
    "    init_state = [tf.zeros([]), tf.zeros([]), tf.zeros([])]\n",
    "    idata_tfp  = fit_bayesian_tfp_hmc(target_log_prob_fn,\n",
    "                                      init_state,\n",
    "                                      draws=N_DRAWS,\n",
    "                                      burnin=N_TUNE,\n",
    "                                      seed=SEED)\n",
    "    _attach_ppc(idata_tfp, x_data, sigma=\"param_2\", draws=PP_DRAWS)\n",
    "\n",
    "    # ─── Compare & Plot ───────────────────────────────────────────────\n",
    "    from src.utils.bayesian_metrics import compute_classical_metrics\n",
    "    engines = {\n",
    "      \"PyMC-HMC\":  idata_pymc_hmc,\n",
    "      \"PyMC-ADVI\": idata_pymc_advi,\n",
    "      \"Stan\":      idata_stan,\n",
    "      \"JAGS\":      idata_jags,\n",
    "      \"NumPyro\":   idata_numpyro,\n",
    "      \"TFP-HMC\":   idata_tfp,\n",
    "    }\n",
    "    for name,idata in engines.items():\n",
    "        print(f\"\\n▼▼ {name}\")\n",
    "        compute_classical_metrics(idata, y_obs)\n",
    "\n",
    "    # quick visual check (posterior mean lines)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.scatter(x_data, y_obs, label=\"obs\", alpha=.6)\n",
    "    line_x = np.linspace(-2, 2, 100)\n",
    "    colors = plt.cm.tab10(np.linspace(0,1,len(engines)))\n",
    "    for c,(name,id_) in zip(colors, engines.items()):\n",
    "        a = id_.posterior[\"alpha\"].mean().item()\n",
    "        b = id_.posterior[\"beta\"].mean().item()\n",
    "        plt.plot(line_x, a + b*line_x, color=c, label=name, lw=1.2)\n",
    "    plt.legend(); plt.title(\"Posterior mean fits – smoke test\")\n",
    "    plt.tight_layout()\n",
    "    Path(\"smoke_test_fits.png\").write_bytes(plt.savefig(\"/tmp/_smoke.png\") or b\"\")\n",
    "    print(\"\\n✔︎ Smoke-test figure saved → smoke_test_fits.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/models/hierarchical.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/hierarchical.py\n",
    "\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import json  # Add json import for allocation result\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from src.features.preprocess import transform_preprocessor\n",
    "\n",
    "# ─── Apply our JAX GPU-memory flags first ───────────────────────────────\n",
    "# this must run before any `import jax`\n",
    "from src.utils.jax_memory_fix_module import apply_jax_memory_fix\n",
    "# Configure JAX to use 90% of GPU memory with preallocation\n",
    "apply_jax_memory_fix(fraction=0.9, preallocate=True)\n",
    "# ─── Now it's safe to pull in JAX (with your flags applied) ─────────────\n",
    "import jax, jaxlib\n",
    "\n",
    "import logging\n",
    "import pymc.sampling.jax as pmjax\n",
    "\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from tqdm.auto import tqdm\n",
    "from jax.lib import xla_bridge\n",
    "from src.utils.jax_gpu_utils import log_gpu_diagnostics\n",
    "\n",
    "# Import memory monitoring utilities\n",
    "from src.utils.jax_memory_monitor import (\n",
    "    monitor_memory_usage,\n",
    "    take_memory_snapshot,\n",
    "    print_memory_snapshot,\n",
    "    force_allocation_if_needed,\n",
    "    generate_memory_report\n",
    ")\n",
    "\n",
    "# Log GPU diagnostics at startup\n",
    "log_gpu_diagnostics()\n",
    "\n",
    "# ── Context manager for timing ─────────────────────────────────────────\n",
    "@contextmanager\n",
    "def _timed_section(label: str):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f\"[{label}] finished in {time.time() - t0:,.1f} s\")\n",
    "\n",
    "\n",
    "# Attempt to import and configure JAX\n",
    "USE_JAX = True\n",
    "try:\n",
    "    # Debug: confirm module and path\n",
    "    print(f\"🔍 JAX module: {jax!r}\")\n",
    "    print(f\"🔍 JAX path:   {getattr(jax, '__file__', 'builtin')}\")\n",
    "    if not hasattr(jax, \"__version__\"):\n",
    "        raise ImportError(\"jax.__version__ missing—possible circular import\")\n",
    "    print(f\"✅ JAX version: {jax.__version__}\")\n",
    "    jax.config.update(\"jax_enable_x64\", True)\n",
    "    log_gpu_diagnostics() \n",
    "    # ── NEW: Verify PyMC sees GPU backend ─────────────────────────────\n",
    "    print(\"🔍 PyMC version:\", pm.__version__)\n",
    "    print(\"🔍 JAX backend:\", xla_bridge.get_backend().platform)\n",
    "\n",
    "except Exception as e:\n",
    "    USE_JAX = False\n",
    "    print(f\"⚠️  Warning: could not import/configure JAX ({e}). Falling back to CPU sampling.\")\n",
    "\n",
    "\n",
    "def fit_bayesian_hierarchical(\n",
    "    df_raw,\n",
    "    transformer: ColumnTransformer,\n",
    "    batter_idx: np.ndarray,\n",
    "    level_idx: np.ndarray,\n",
    "    season_idx: np.ndarray,\n",
    "    pitcher_idx: np.ndarray,\n",
    "    *,\n",
    "    feature_list: list[str] | None = None,\n",
    "    mu_mean: float  = 88.0,\n",
    "    mu_sd:   float  = 30.0,\n",
    "    sigma_prior: float = 10.0,\n",
    "    draws: int      = 200,\n",
    "    tune:  int      = 200,\n",
    "    target_accept: float = 0.9,\n",
    "    verbose: bool   = True,\n",
    "    sampler: str    = \"jax\",\n",
    "    chains: int     = 1,\n",
    "    monitor_memory: bool = True,\n",
    "    force_memory_allocation: bool = True,\n",
    "    allocation_target: float = 0.8,\n",
    "    direct_feature_input: tuple = None,  # (X, y, feature_names) for testing\n",
    "):\n",
    "    cols = _ColumnSchema()\n",
    "    if feature_list is None:\n",
    "        feature_list = cols.model_features()\n",
    "\n",
    "    # Take initial memory snapshot if monitoring enabled\n",
    "    if monitor_memory:\n",
    "        take_memory_snapshot(\"Before model setup\")\n",
    "        \n",
    "    # Force memory allocation if requested\n",
    "    if force_memory_allocation and monitor_memory:\n",
    "        if verbose:\n",
    "            print(\"\\n=== Pre-training Memory Allocation ===\")\n",
    "        allocation_result = force_allocation_if_needed(\n",
    "            target_fraction=allocation_target,\n",
    "            current_usage_threshold=0.4,\n",
    "            step_size_mb=1000,\n",
    "            max_steps=8,\n",
    "            verbose=verbose\n",
    "        )\n",
    "        if verbose:\n",
    "            print(f\"Memory allocation result: {json.dumps(allocation_result, indent=2)}\")\n",
    "\n",
    "    # 1) design matrix - either from transformer or direct input\n",
    "    if direct_feature_input is not None:\n",
    "        # Use direct input (for testing)\n",
    "        X, y, feature_names = direct_feature_input\n",
    "        if feature_list is not None:\n",
    "            # Filter features if list provided\n",
    "            selected_indices = [i for i, name in enumerate(feature_names) if name in feature_list]\n",
    "            X = X[:, selected_indices]\n",
    "            feature_names = [feature_names[i] for i in selected_indices]\n",
    "    else:\n",
    "        # Use transformer\n",
    "        X_all, y_ser = transform_preprocessor(df_raw, transformer)\n",
    "        names = transformer.get_feature_names_out()\n",
    "        X = X_all[:, np.isin(names, feature_list)]\n",
    "        y = y_ser.values\n",
    "        feature_names = [fn for fn in names if fn in feature_list]\n",
    "\n",
    "    # Cast indices to Python ints\n",
    "    n_bat  = int(batter_idx.max()) + 1\n",
    "    n_lvl  = int(level_idx.max()) + 1\n",
    "    n_seas = int(season_idx.max()) + 1\n",
    "    n_pit  = int(pitcher_idx.max()) + 1\n",
    "    n_feat = int(X.shape[1])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"🔍 Data dims: n_bat={n_bat}, n_lvl={n_lvl}, n_seas={n_seas}, n_pit={n_pit}, n_feat={n_feat}\")\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # Priors\n",
    "        mu         = pm.Normal(\"mu\", mu_mean, mu_sd)\n",
    "        beta_level = pm.Normal(\"beta_level\", 0, 5, shape=n_lvl)\n",
    "        beta       = pm.Normal(\"beta\",       0, 5, shape=n_feat)\n",
    "\n",
    "        sigma_b    = pm.HalfNormal(\"sigma_b\", sigma_prior)\n",
    "        u_raw      = pm.Normal(\"u_raw\", 0, 1, shape=n_bat)\n",
    "        u          = pm.Deterministic(\"u\", u_raw * sigma_b)\n",
    "\n",
    "        sigma_seas = pm.HalfNormal(\"sigma_seas\", sigma_prior)\n",
    "        v_raw      = pm.Normal(\"v_raw\", 0, 1, shape=n_seas)\n",
    "        v          = pm.Deterministic(\"v\", v_raw * sigma_seas)\n",
    "\n",
    "        sigma_pit  = pm.HalfNormal(\"sigma_pit\", sigma_prior)\n",
    "        p_raw      = pm.Normal(\"p_raw\", 0, 1, shape=n_pit)\n",
    "        p          = pm.Deterministic(\"p\", p_raw * sigma_pit)\n",
    "\n",
    "        # Likelihood\n",
    "        theta = (\n",
    "            mu\n",
    "            + beta_level[level_idx]\n",
    "            + pm.math.dot(X, beta)\n",
    "            + u[batter_idx]\n",
    "            + v[season_idx]\n",
    "            + p[pitcher_idx]\n",
    "        )\n",
    "        sigma_e  = pm.HalfNormal(\"sigma_e\", sigma_prior)\n",
    "        pm.Normal(\"y_obs\", theta, sigma_e, observed=y)\n",
    "\n",
    "        # Memory snapshot before sampling if monitoring enabled\n",
    "        if monitor_memory:\n",
    "            take_memory_snapshot(\"Before sampling\")\n",
    "            # Take another memory snapshot after a small delay to catch any lazy allocation\n",
    "            time.sleep(1)\n",
    "            take_memory_snapshot(\"Before sampling (after delay)\")\n",
    "\n",
    "        # Sampling with timing and memory monitoring\n",
    "        sampling_context = (\n",
    "            monitor_memory_usage(\"MCMC Sampling\", verbose=verbose) if monitor_memory \n",
    "            else _timed_section(\"compile+sample\")\n",
    "        )\n",
    "        \n",
    "        with sampling_context:\n",
    "            idata = pm.sample(\n",
    "                draws=draws,\n",
    "                tune=tune,\n",
    "                chains=chains,\n",
    "                target_accept=target_accept,\n",
    "                nuts_sampler=\"numpyro\" if (sampler==\"jax\" and USE_JAX) else \"nuts\",\n",
    "                **(\n",
    "                    {\"nuts_sampler_kwargs\": {\"chain_method\": \"vectorized\"}}\n",
    "                    if (sampler == \"jax\" and USE_JAX)\n",
    "                    else {}\n",
    "                ),\n",
    "                progressbar=verbose,\n",
    "                idata_kwargs={\"log_likelihood\": [\"y_obs\"]},\n",
    "            )\n",
    "\n",
    "        # Memory snapshot after sampling if monitoring enabled\n",
    "        if monitor_memory:\n",
    "            take_memory_snapshot(\"After sampling\")\n",
    "\n",
    "        # Posterior predictive timing with memory monitoring\n",
    "        posterior_context = (\n",
    "            monitor_memory_usage(\"Posterior Predictive\", verbose=verbose) if monitor_memory \n",
    "            else _timed_section(\"posterior_predictive\")\n",
    "        )\n",
    "        \n",
    "        with posterior_context:\n",
    "            idata.extend(pm.sample_posterior_predictive(idata, var_names=[\"y_obs\"]))\n",
    "\n",
    "    # Store feature names\n",
    "    idata.attrs[\"feature_names\"] = feature_names\n",
    "\n",
    "    if verbose:\n",
    "        print(\"⚡ Posterior predictive samples (first 5):\",\n",
    "              idata.posterior_predictive[\"y_obs\"]\n",
    "                   .stack(s=(\"chain\", \"draw\")).values[:5])\n",
    "\n",
    "    # Generate memory report if monitoring enabled\n",
    "    if monitor_memory:\n",
    "        report = generate_memory_report(\"hierarchical_memory_report.json\")\n",
    "        if \"summary\" in report and \"gpu_utilization\" in report[\"summary\"]:\n",
    "            util = report[\"summary\"][\"gpu_utilization\"]\n",
    "            print(\"\\n=== Memory Usage Summary ===\")\n",
    "            print(f\"GPU Utilization - Min: {util['min']:.2f}% Max: {util['max']:.2f}% Avg: {util['avg']:.2f}%\")\n",
    "            print(f\"Detailed report saved to hierarchical_memory_report.json\")\n",
    "\n",
    "    return idata\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    from pathlib import Path\n",
    "    import pandas as pd, numpy as np, arviz as az\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.features.preprocess import (fit_preprocessor,\n",
    "                                        prepare_for_mixed_and_hierarchical)\n",
    "    # from src.models.hierarchical import fit_bayesian_hierarchical\n",
    "    from src.utils.hierarchical_utils import save_model, save_preprocessor\n",
    "    from src.utils.posterior import posterior_to_frame\n",
    "    from src.utils.bayesian_metrics import (compute_classical_metrics,\n",
    "                                            compute_bayesian_metrics,\n",
    "                                            compute_convergence_diagnostics,\n",
    "                                            compute_calibration)\n",
    "\n",
    "    from src.utils.validation import (\n",
    "        run_kfold_cv_pymc,\n",
    "        rmse_pymc,\n",
    "        posterior_predictive_check,\n",
    "    )\n",
    "    import json  # Added import for JSON operations\n",
    "    \n",
    "    RAW   = Path(\"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\")\n",
    "    OUT_NC = Path(\"data/models/saved_models/exitvelo_hmc.nc\")\n",
    "    OUT_POST = Path(\"data/models/saved_models/posterior_summary.parquet\")\n",
    "    OUT_PREPROC = Path(\"data/models/saved_models/preprocessor.joblib\")\n",
    "\n",
    "    # 1 · prep\n",
    "    df = load_and_clean_data(RAW)\n",
    "    df_fe = feature_engineer(df)\n",
    "    df_model = prepare_for_mixed_and_hierarchical(df_fe)\n",
    "\n",
    "    _, _, tf = fit_preprocessor(df_model, model_type=\"linear\", debug=False)\n",
    "\n",
    "    b_idx = df_model[\"batter_id\"].cat.codes.values\n",
    "    l_idx = df_model[\"level_idx\"].values\n",
    "    s_idx = df_model[\"season_idx\"].values\n",
    "    p_idx = df_model[\"pitcher_idx\"].values\n",
    "    draws_and_tune = 50\n",
    "    target_accept=0.95\n",
    "    chains=4\n",
    "    # 2 · fit\n",
    "    idata = fit_bayesian_hierarchical(\n",
    "        df_model,\n",
    "        tf,\n",
    "        b_idx,            # batter codes\n",
    "        l_idx,            # level codes\n",
    "        s_idx,            # season codes\n",
    "        p_idx,            # pitcher codes\n",
    "        sampler=\"jax\",\n",
    "        draws=draws_and_tune,\n",
    "        tune=draws_and_tune,\n",
    "        target_accept=target_accept,\n",
    "        chains=chains,\n",
    "        monitor_memory=True,  # Enable memory monitoring\n",
    "        force_memory_allocation=True,  # Force memory allocation\n",
    "        allocation_target=0.8,  # Target 80% memory utilization\n",
    "        direct_feature_input=None  # No direct feature input for this example\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    idata.attrs[\"median_age\"] = df_model[\"age\"].median()   # ← NEW\n",
    "\n",
    "    # 3 · persist\n",
    "    save_model(idata, OUT_NC)\n",
    "    save_preprocessor(tf, OUT_PREPROC)\n",
    "    posterior_to_frame(idata).to_parquet(OUT_POST)\n",
    "    print(\"✅ training complete – artefacts written:\")\n",
    "    print(\"   •\", OUT_NC)\n",
    "    print(\"   •\", OUT_POST)\n",
    "    print(\"   •\", OUT_PREPROC)\n",
    "\n",
    "\n",
    "    # # ─── NEW: IMPORT VALIDATION UTILITIES ─────────────────────────────\n",
    "    # # ─── 7) Sanity‐check CV on your real data ───────────────────────────\n",
    "    # print(\"\\n--- PyMC hierarchical CV on real data ---\")\n",
    "    # cv_scores = run_kfold_cv_pymc(\n",
    "    #     lambda d: fit_bayesian_hierarchical(\n",
    "    #         d, tf, b_idx, l_idx, s_idx, p_idx,\n",
    "    #         sampler=\"jax\",\n",
    "    #         draws=draws_and_tune,\n",
    "    #         tune=draws_and_tune,\n",
    "    #         target_accept=target_accept,\n",
    "    #         verbose=False,  # silent inside CV\n",
    "    #         chains = chains\n",
    "    #     ),\n",
    "    #     df_model,\n",
    "    #     k=3\n",
    "    # )\n",
    "    # print(\"CV RMSEs:\", cv_scores)\n",
    "\n",
    "\n",
    "    # Extract the true target vector for classical metrics\n",
    "    _, y_full = transform_preprocessor(df_model, tf)\n",
    "    print(\"=== Classical Metrics ===\")\n",
    "    compute_classical_metrics(idata, y_full.values)\n",
    "\n",
    "    # print(\"\\n=== Calibration ===\")\n",
    "    # compute_calibration(idata, y_full.values)\n",
    "\n",
    "    print(\"\\n=== Bayesian Metrics ===\")\n",
    "    compute_bayesian_metrics(idata)\n",
    "\n",
    "    print(\"\\n=== Convergence Diagnostics ===\")\n",
    "    compute_convergence_diagnostics(idata)\n",
    "\n",
    "\n",
    "    # # ─── 8) RMSE & PPC on the full training set ─────────────────────────\n",
    "    # print(\"\\n--- rmse_pymc on full training data ---\")\n",
    "    # rmse_full = rmse_pymc(idata, df_model)\n",
    "    # print(\"RMSE (train):\", rmse_full)\n",
    "\n",
    "    # print(\"\\n--- posterior_predictive_check (real data) ---\")\n",
    "    # fig = posterior_predictive_check(idata, df_model, batter_idx=b_idx)\n",
    "    # fig.savefig(\"ppc_real_data.png\")\n",
    "    # print(\"Saved plot to ppc_real_data.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/models/hierarchical_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/hierarchical_predict.py\n",
    "\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.utils.posterior import align_batter_codes\n",
    "\n",
    "# ─── new helper at top of file ────────────────────────────────────────────\n",
    "def get_top_hitters(\n",
    "    df: pd.DataFrame,\n",
    "    hitter_col: str = \"hitter_type\",\n",
    "    n: int = 5,\n",
    "    verbose: bool = False\n",
    ") -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Given a DataFrame with predictions and a 'hitter_type' column,\n",
    "    return two DataFrames of the top-n POWER and CONTACT hitters by pred_mean.\n",
    "    If the column is missing, returns two empty DataFrames (and prints a warning).\n",
    "    \"\"\"\n",
    "    if hitter_col not in df.columns:\n",
    "        if verbose:\n",
    "            print(f\"[Warning] Column '{hitter_col}' not found; skipping top-hitter extraction.\")\n",
    "        empty = pd.DataFrame(columns=[ \"batter_id\", hitter_col, \"pred_mean\", \"pred_lo95\", \"pred_hi95\" ])\n",
    "        return empty, empty\n",
    "\n",
    "    # 1) Filter power vs. contact, case‐insensitive\n",
    "    power_mask   = df[hitter_col].str.contains(\"POWER\",   case=False, na=False)\n",
    "    contact_mask = df[hitter_col].str.contains(\"CONTACT\", case=False, na=False)\n",
    "\n",
    "    power_df   = df.loc[power_mask]\n",
    "    contact_df = df.loc[contact_mask]\n",
    "\n",
    "    # 2) Take the top-n by pred_mean\n",
    "    top_power   = power_df.nlargest(n, \"pred_mean\")[ [\"batter_id\", hitter_col, \"pred_mean\", \"pred_lo95\", \"pred_hi95\"] ]\n",
    "    top_contact = contact_df.nlargest(n, \"pred_mean\")[ [\"batter_id\", hitter_col, \"pred_mean\", \"pred_lo95\", \"pred_hi95\"] ]\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[Top {n}] power hitters:\\n{top_power}\")\n",
    "        print(f\"[Top {n}] contact hitters:\\n{top_contact}\")\n",
    "\n",
    "    return top_power, top_contact\n",
    "\n",
    "\n",
    "\n",
    "def simplified_prepare_validation(df_val: pd.DataFrame, median_age: float, verbose: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare validation dataset with simplified approach, handling missing columns gracefully.\n",
    "\n",
    "    Assumptions:\n",
    "    - All predictions are for MLB-level competition (level_idx = 2).\n",
    "    - Missing age defaults to median training age (age_centered = 0).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_val : pd.DataFrame\n",
    "        Raw validation dataframe (must include 'season', 'batter_id', optional 'age').\n",
    "    median_age : float\n",
    "        Median age computed from training data for centering.\n",
    "    verbose : bool\n",
    "        If True, prints debug information.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with added 'age_centered' and 'level_idx'.\n",
    "    \"\"\"\n",
    "    df_val = df_val.copy()\n",
    "\n",
    "    # Default to MLB level for all observations\n",
    "    df_val['level_idx'] = 2\n",
    "\n",
    "    # Center age\n",
    "    if 'age' in df_val.columns:\n",
    "        df_val['age_centered'] = df_val['age'] - median_age\n",
    "    else:\n",
    "        # Using median training age => zero effect\n",
    "        df_val['age_centered'] = 0.0\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"[Validation Prep] level_idx set to 2 for all rows, age_centered stats: min={df_val['age_centered'].min()}, max={df_val['age_centered'].max()}\")\n",
    "\n",
    "    return df_val\n",
    "\n",
    "\n",
    "def predict_from_summaries(\n",
    "    roster_csv: Path,\n",
    "    posterior_parquet: Path,\n",
    "    global_effects_json: Path,\n",
    "    output_csv: Path,\n",
    "    verbose: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load saved model summaries and raw roster, merge random effects,\n",
    "    compute point predictions and intervals for validation set,\n",
    "    write out predictions CSV, and return the DataFrame.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        DataFrame with columns:\n",
    "        ['season','batter_id','age','level_idx','age_centered','batter_idx',\n",
    "         'u_q50','u_q2.5','u_q97.5',\n",
    "         'contrib_age','contrib_level','contrib_u','contrib_features',\n",
    "         'pred_mean','pred_lo95','pred_hi95']\n",
    "    \"\"\"\n",
    "    # 1) Load posterior random effects (batter-level)\n",
    "    df_post = pd.read_parquet(posterior_parquet)\n",
    "    if verbose:\n",
    "        print(f\"[Load] posterior_summary: {df_post.shape} rows\")\n",
    "\n",
    "    # 2) Load validation roster\n",
    "    df_roster = pd.read_csv(roster_csv)\n",
    "    if verbose:\n",
    "        print(f\"[Load] validation roster: {df_roster.shape} rows, columns: {df_roster.columns.tolist()}\")\n",
    "\n",
    "    # 3) Load global effects\n",
    "    glob        = json.loads(global_effects_json.read_text())\n",
    "    post_mu     = glob['mu_mean']\n",
    "    beta_age    = glob['beta_age']\n",
    "    beta_level  = glob['beta_level'][2]  # MLB-level coefficient\n",
    "    med_age     = glob['median_age']\n",
    "    if verbose:\n",
    "        print(f\"[Global Effects] mu={post_mu}, beta_age={beta_age}, \"\n",
    "              f\"beta_level={beta_level}, median_age={med_age}\")\n",
    "\n",
    "    # 4) Prepare minimal validation features\n",
    "    df_val = simplified_prepare_validation(df_roster, med_age, verbose)\n",
    "\n",
    "    # 5) Align batter codes and merge random effects\n",
    "    df_val['batter_idx'] = align_batter_codes(df_val, df_post['batter_idx'])\n",
    "    df = df_val.merge(df_post, on='batter_idx', how='left')\n",
    "    if verbose:\n",
    "        print(f\"[Merge] merged validation with posterior: {df.shape}\")\n",
    "\n",
    "    # 6) Compute contributions & predictions\n",
    "    # 6a) Age contribution\n",
    "    df['contrib_age']   = beta_age * df['age_centered']\n",
    "    # 6b) Level (MLB) contribution\n",
    "    df['contrib_level'] = beta_level\n",
    "    # 6c) Batter random effect (median)\n",
    "    df['contrib_u']     = df['u_q50']\n",
    "\n",
    "    # 6d) Point‐estimate\n",
    "    df['pred_mean']  = post_mu + df['contrib_age'] + df['contrib_level'] + df['contrib_u']\n",
    "\n",
    "    # 6e) 95% credible‐interval bounds\n",
    "    df['pred_lo95']  = post_mu + df['contrib_age'] + df['contrib_level'] + df['u_q2.5']\n",
    "    df['pred_hi95']  = post_mu + df['contrib_age'] + df['contrib_level'] + df['u_q97.5']\n",
    "\n",
    "    # 6f) Placeholder for any other feature contributions\n",
    "    df['contrib_features'] = 0.0\n",
    "\n",
    "    # 7) Persist predictions CSV\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    if verbose:\n",
    "        print(f\"[Save] Predictions written to {output_csv}\")\n",
    "\n",
    "    # 8) Return for downstream inspection or metrics\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from src.utils.validation import (\n",
    "        prediction_interval,\n",
    "        bootstrap_prediction_interval,\n",
    "    )\n",
    "\n",
    "    BASE    = Path('data/models/saved_models')\n",
    "    SUMMARY = BASE / 'posterior_summary.parquet'\n",
    "    GLOBAL  = BASE / 'global_effects.json'\n",
    "    ROSTER  = Path('data/Research Data Project/Research Data Project/exit_velo_validate_data.csv')\n",
    "    OUT     = Path('data/predictions/exitvelo_predictions_2024.csv')\n",
    "\n",
    "    # 1) Generate predictions + CI columns\n",
    "    predict_df = predict_from_summaries(\n",
    "        roster_csv=ROSTER,\n",
    "        posterior_parquet=SUMMARY,\n",
    "        global_effects_json=GLOBAL,\n",
    "        output_csv=OUT,\n",
    "        verbose=True\n",
    "    )\n",
    "    print(predict_df.head())\n",
    "\n",
    "    # 2) Empirical RMSE if 'exit_velo' present\n",
    "    df_val = pd.read_csv(ROSTER)\n",
    "    if 'exit_velo' in df_val.columns:\n",
    "        y_true = df_val['exit_velo'].values\n",
    "        y_pred = predict_df['pred_mean'].values\n",
    "        rmse_val = np.sqrt(np.mean((y_pred - y_true)**2))\n",
    "        print(\"\\n--- empirical RMSE on validation set ---\", rmse_val)\n",
    "\n",
    "    # 3) Prepare a preds DataFrame for CI routines\n",
    "    preds = predict_df['pred_mean'].to_frame(name='pred')\n",
    "    lo95  = predict_df['pred_lo95'].values\n",
    "    hi95  = predict_df['pred_hi95'].values\n",
    "\n",
    "\n",
    "    # 4) Safely extract top hitters (requires a 'hitter_type' column)\n",
    "    top_power, top_contact = get_top_hitters(predict_df, hitter_col=\"hitter_type\", n=5, verbose=True)\n",
    "\n",
    "    print(\"\\nTop Power Hitters (if any):\\n\", top_power)\n",
    "    print(\"\\nTop Contact Hitters (if any):\\n\", top_contact)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/train.py\n",
    "\"\"\"\n",
    "Train / compare four families on a 70‑30 split.\n",
    "\n",
    "Run:\n",
    "    python -m src.train\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.data.load_data import load_and_clean_data\n",
    "from src.features.preprocess import preprocess\n",
    "\n",
    "from src.models.linear import fit_ridge\n",
    "from src.models.gbm   import fit_gbm\n",
    "from src.models.mixed import fit_mixed\n",
    "from src.models.hierarchical import fit_bayesian_hierarchical\n",
    "\n",
    "RAW_PATH = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    df_raw   = load_and_clean_data(RAW_PATH)\n",
    "    df_clean = preprocess(df_raw)\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_clean, test_size=0.30, random_state=42, stratify=df_clean[\"level_abbr\"]\n",
    "    )\n",
    "\n",
    "    # ––– A  Ridge  –––––––––––––––––––––––––––––––\n",
    "    _, rmse_ridge = fit_ridge(train_df, test_df)\n",
    "    print(f\"Ridge RMSE ……  {rmse_ridge:5.2f} mph\")\n",
    "\n",
    "    # ––– B  Gradient‑Boost  ––––––––––––––––––––––\n",
    "    _, rmse_gbm = fit_gbm(train_df, test_df)\n",
    "    print(f\"XGBoost RMSE … {rmse_gbm:5.2f} mph\")\n",
    "\n",
    "    # ––– C  Mixed‑Effects  –––––––––––––––––––––––\n",
    "    _, rmse_mixed = fit_mixed(train_df, test_df)\n",
    "    print(f\"Mixed‑LM RMSE  {rmse_mixed:5.2f} mph\")\n",
    "\n",
    "    # ––– D  Bayesian Hierarchical (quick sample) –\n",
    "    idata = fit_bayesian_hierarchical(\n",
    "        train_df,\n",
    "        batter_idx=train_df.batter_id.astype(\"category\").cat.codes.values,\n",
    "        level_idx=train_df.level_idx.values,\n",
    "        age_centered=train_df.age_centered.values,\n",
    "        mu_prior=90,\n",
    "        sigma_prior=5,\n",
    "        draws=500, tune=500   # short run for demo\n",
    "    )\n",
    "    from src.utils.validation import rmse_pymc\n",
    "    rmse_bayes = rmse_pymc(idata, test_df)\n",
    "    print(f\"PyMC RMSE ……  {rmse_bayes:5.2f} mph\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/dash_compat.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/dash_compat.py\n",
    "# src/utils/dash_compat.py\n",
    "import dash\n",
    "import functools\n",
    "import dash_bootstrap_components as dbc\n",
    "\n",
    "# ── 3.0 compat: if Dash.run_server() was removed, alias it to run()\n",
    "if not hasattr(dash.Dash, \"run_server\"):\n",
    "    dash.Dash.run_server = dash.Dash.run\n",
    "\n",
    "_DROPDOWN_PATCH_KEY = \"_ghadf_dropdown_patched\"\n",
    "\n",
    "def patch_dropdown_right_once() -> None:\n",
    "    \"\"\"\n",
    "    Back-compat shim: translate deprecated `right=` → `align_end=`,\n",
    "    and patch it exactly once per Python process.\n",
    "    \"\"\"\n",
    "    if getattr(dbc, _DROPDOWN_PATCH_KEY, False):\n",
    "        return\n",
    "\n",
    "    orig_init = dbc.DropdownMenu.__init__\n",
    "\n",
    "    @functools.wraps(orig_init)\n",
    "    def _init(self, *args, **kwargs):\n",
    "        if \"right\" in kwargs and \"align_end\" not in kwargs:\n",
    "            kwargs[\"align_end\"] = kwargs.pop(\"right\")\n",
    "        return orig_init(self, *args, **kwargs)\n",
    "\n",
    "    dbc.DropdownMenu.__init__ = _init\n",
    "    setattr(dbc, _DROPDOWN_PATCH_KEY, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/dash_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/dash_utils.py\n",
    "\n",
    "import dash\n",
    "from src.utils.dash_compat import patch_dropdown_right_once\n",
    "import pandas as pd\n",
    "import dash_bootstrap_components as dbc\n",
    "from explainerdashboard import RegressionExplainer, ExplainerDashboard\n",
    "from __future__ import annotations\n",
    "import socket, contextlib, time\n",
    "\n",
    "def _in_notebook() -> bool:\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "        ip = get_ipython()\n",
    "        return ip is not None and ip.has_trait(\"kernel\")\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Apply necessary shims at import time\n",
    "patch_dropdown_right_once()\n",
    "\n",
    "\n",
    "\n",
    "def _port_in_use(host: str, port: int) -> bool:\n",
    "    with contextlib.closing(socket.socket()) as s:\n",
    "        s.settimeout(0.001)\n",
    "        return s.connect_ex((host, port)) == 0\n",
    "\n",
    "def _get_free_port() -> int:\n",
    "    with contextlib.closing(socket.socket()) as s:\n",
    "        s.bind((\"\", 0))\n",
    "        return s.getsockname()[1]\n",
    "    \n",
    "def launch_explainer_dashboard(\n",
    "    pipeline,\n",
    "    preprocessor,\n",
    "    X_raw: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    *,\n",
    "    cats=None,\n",
    "    descriptions=None,\n",
    "    title: str = \"Model Explainer\",\n",
    "    bootstrap=dbc.themes.FLATLY,\n",
    "    inline: bool | None = None,\n",
    "    host: str = \"127.0.0.1\",\n",
    "    port: int = 8050,\n",
    "    **db_kwargs,\n",
    "):\n",
    "    \"\"\"Create & display an ExplainerDashboard.\n",
    "\n",
    "    • Inline in notebooks by default.  \n",
    "    • Auto-terminates any previous inline dashboard on the same port.  \n",
    "    • Falls back to a free port if the requested one is still busy.\n",
    "    \"\"\"\n",
    "    if inline is None:\n",
    "        inline = _in_notebook()\n",
    "\n",
    "    # ---------- tidy up any existing inline server ----------\n",
    "    if inline and _port_in_use(host, port):\n",
    "        try:\n",
    "            ExplainerDashboard.terminate(port)\n",
    "        except Exception:             # pragma: no cover\n",
    "            pass                      # nothing running or unsupported version\n",
    "        # give the OS a moment to release the socket\n",
    "        for _ in range(10):\n",
    "            if not _port_in_use(host, port):\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "        else:                          # still busy after 1 s\n",
    "            port = _get_free_port()\n",
    "\n",
    "    # ---------- prepare dataset ----------\n",
    "    X_proc     = preprocessor.transform(X_raw)\n",
    "    feat_names = preprocessor.get_feature_names_out()\n",
    "    X_df       = pd.DataFrame(X_proc, columns=feat_names, index=X_raw.index)\n",
    "\n",
    "    if cats:\n",
    "        cats = [c for c in cats if any(fn.startswith(f\"{c}_\") for fn in feat_names)]\n",
    "\n",
    "    explainer = RegressionExplainer(\n",
    "        pipeline,\n",
    "        X_df,\n",
    "        y,\n",
    "        cats=cats or [],\n",
    "        descriptions=descriptions or {},\n",
    "        precision=\"float32\",\n",
    "    )\n",
    "    explainer.merged_cols = explainer.merged_cols.intersection(X_df.columns)\n",
    "\n",
    "    # ---------- launch dashboard ----------\n",
    "    if inline:\n",
    "        ExplainerDashboard(\n",
    "            explainer,\n",
    "            title=title,\n",
    "            bootstrap=bootstrap,\n",
    "            mode=\"inline\",\n",
    "            **db_kwargs,\n",
    "        ).run(host=host, port=port)\n",
    "    else:\n",
    "        ExplainerDashboard(\n",
    "            explainer,\n",
    "            title=title,\n",
    "            bootstrap=bootstrap,\n",
    "            **db_kwargs,\n",
    "        ).run(host=host, port=port)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.features.preprocess import transform_preprocessor\n",
    "    from src.utils.gbm_utils import load_pipeline\n",
    "    from src.data.ColumnSchema import _ColumnSchema\n",
    "\n",
    "    # 1) load your trained pipeline + preprocessor\n",
    "    model_pipeline, preprocessor = load_pipeline(\"data/models/saved_models/gbm_pipeline.joblib\")\n",
    "\n",
    "    # 2) load & prepare a small sample\n",
    "    df_raw = load_and_clean_data(\n",
    "        \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    ).sample(200, random_state=42)\n",
    "    df_fe  = feature_engineer(df_raw)\n",
    "    X_raw = df_fe.drop(columns=[\"exit_velo\"])\n",
    "    y_raw = df_fe[\"exit_velo\"]\n",
    "\n",
    "    # 3) category grouping helper\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    # 4) launch the dashboard on port 8050\n",
    "    launch_explainer_dashboard(\n",
    "        pipeline      = model_pipeline,\n",
    "        preprocessor  = preprocessor,\n",
    "        X_raw         = X_raw,\n",
    "        y             = y_raw,\n",
    "        cats          = cols.nominal(),\n",
    "        descriptions  = {c: c for c in preprocessor.get_feature_names_out()},\n",
    "        whatif        = True,\n",
    "        shap_interaction = False,\n",
    "        hide_wizard     = True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
