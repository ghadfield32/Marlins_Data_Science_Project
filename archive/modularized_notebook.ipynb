{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/data/load_data.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(path='data/Research Data Project/Research Data Project/exit_velo_project_data.csv'):\n",
    "    \"\"\"\n",
    "    Load raw exit velocity data from the data directory.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Raw exit velocity data\n",
    "    \"\"\"\n",
    "    # This is a placeholder - in a real implementation, you would load actual data\n",
    "    # For example: df = pd.read_csv('data/raw/exit_velo.csv')\n",
    "\n",
    "    df = pd.read_csv(path)\n",
    "    return df \n",
    "\n",
    "\n",
    "\n",
    "# ──  utility ─────────────────────────────────────────────────────────\n",
    "def clean_raw(df: pd.DataFrame, \n",
    "              target: str = \"exit_velo\"\n",
    "              ,debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Central place to:\n",
    "      1. Drop rows with NaN in *target*.\n",
    "      2. Print a concise null‑summary afterwards (one line per column).\n",
    "    \n",
    "    Returns a *fresh copy* (never mutates in‑place).\n",
    "    \"\"\"\n",
    "    if debug:\n",
    "        print(f\"Dropping rows with NaN in {target}\")\n",
    "        print(df.isna().sum())\n",
    "        \n",
    "    drop_columns = [target, \"hit_type\", \"launch_angle\"]\n",
    "    # remove 125 hit_type rows (~0.01%) and 2 launch_angle rows (~0.00%) after the target was dropped because that's automatically done\n",
    "    out = df.dropna(subset=drop_columns).copy()   \n",
    "\n",
    "    if debug:\n",
    "        print(f\"after rows dropped with NaN in {target}\")\n",
    "        print(out.isna().sum())\n",
    "    # quick dashboard\n",
    "    nulls = out.isna().sum()\n",
    "    non_zero = nulls[nulls > 0]\n",
    "    if non_zero.empty:\n",
    "        print(f\"✅  After target‑filter, no other nulls (n={len(out):,}).\")\n",
    "    else:\n",
    "        print(\"⚠️  Nulls after target‑filter:\")\n",
    "        for col, cnt in non_zero.items():\n",
    "            pct = cnt / len(out)\n",
    "            print(f\"  • {col:<15} {cnt:>7,} ({pct:5.2%})\")\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_and_clean_data(path='data/Research Data Project/Research Data Project/exit_velo_project_data.csv'\n",
    "                        ,debug: bool = False):\n",
    "    df = load_data(path)\n",
    "    df = clean_raw(df, debug = True) #hangtime only one left to impute\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    path = 'data/Research Data Project/Research Data Project/exit_velo_project_data.csv'\n",
    "    # df = load_data(path)\n",
    "    # print(df.head())\n",
    "    # print(df.columns)\n",
    "    # df = clean_raw(df, debug = True) #hangtime only one left to impute\n",
    "\n",
    "    df = load_and_clean_data(path, debug = True)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/features/feature_engineering.py\n",
    "\"\"\"Feature engineering utilities for the exit‑velo project.\n",
    "\n",
    "All helpers are pure functions that take a pandas DataFrame and return a *copy*\n",
    "with additional engineered columns so we avoid side effects.\n",
    "\n",
    "Example\n",
    "-------\n",
    ">>> from src.data.load_data import load_and_clean_data\n",
    ">>> from src.features.feature_engineering import feature_engineer\n",
    ">>> df = load_and_clean_data()\n",
    ">>> df_fe = feature_engineer(df)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "###############################################################################\n",
    "# Helper functions\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def _rolling_stat_lagged(\n",
    "    df: pd.DataFrame,\n",
    "    group_cols: list[str],\n",
    "    target: str,\n",
    "    stat: str = \"mean\",\n",
    "    window: int = 50,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    Group-wise rolling statistic using only *previous* rows.\n",
    "    For each group defined by group_cols, shift the target by 1 row \n",
    "    then compute a rolling(window) agg(stat) with min_periods=10.\n",
    "    \"\"\"\n",
    "    # 1) Within each group, shift the target by one so we only use past data\n",
    "    def shifted_rolling(x: pd.Series) -> pd.Series:\n",
    "        return x.shift(1).rolling(window=window, min_periods=10).agg(stat)\n",
    "\n",
    "    rolled = (\n",
    "        df\n",
    "        .groupby(group_cols)[target]     # group by batter_id or pitcher_id\n",
    "        .apply(shifted_rolling)          # shift & roll inside each group\n",
    "        .reset_index(level=group_cols, drop=True)  # get back a plain Series\n",
    "    )\n",
    "    return rolled\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Public API\n",
    "###############################################################################\n",
    "\n",
    "def feature_engineer(df: pd.DataFrame, copy: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Return a DataFrame enriched with engineered features (no leakage).\"\"\"\n",
    "\n",
    "    if copy:\n",
    "        df = df.copy()\n",
    "\n",
    "    # 1) Uppercase strings\n",
    "    str_cols = df.select_dtypes(include=['object', 'string']).columns\n",
    "    df[str_cols] = df[str_cols].apply(lambda col: col.str.upper())\n",
    "\n",
    "    # 2) Age\n",
    "    df[\"age_sq\"]   = df[\"age\"] ** 2\n",
    "    df[\"age_bin\"]  = pd.qcut(df[\"age\"], q=4, duplicates='drop')\n",
    "\n",
    "    # 3) Height\n",
    "    avg_height    = df[\"batter_height\"].mean()\n",
    "    df[\"height_diff\"] = df[\"batter_height\"] - avg_height\n",
    "\n",
    "    # 4) Launch & spray bins\n",
    "    df[\"la_bin\"]    = pd.qcut(df[\"launch_angle\"], q=5, duplicates='drop')\n",
    "    df[\"spray_bin\"] = pd.qcut(df[\"spray_angle\"], q=3, duplicates='drop')\n",
    "\n",
    "    # 5) Handedness & matchups\n",
    "    df[\"same_hand\"]        = (df[\"batter_hand\"] == df[\"pitcher_hand\"])\n",
    "    df[\"hand_match\"]       = df[\"batter_hand\"] + \"_VS_\" + df[\"pitcher_hand\"]\n",
    "    df[\"pitch_hand_match\"] = df[\"pitch_group\"] + \"_\" + df[\"hand_match\"]\n",
    "\n",
    "    # 6) Batter lagged rolling EV stats\n",
    "    df[\"player_ev_mean50\"] = _rolling_stat_lagged(df, [\"batter_id\"], \"exit_velo\", \"mean\", 50)\n",
    "    df[\"player_ev_std50\"]  = _rolling_stat_lagged(df, [\"batter_id\"], \"exit_velo\",  \"std\", 50)\n",
    "    gm = df[\"exit_velo\"].mean(); gs = df[\"exit_velo\"].std()\n",
    "    df[\"player_ev_mean50\"].fillna(gm, inplace=True)\n",
    "    df[\"player_ev_std50\"].fillna(gs, inplace=True)\n",
    "\n",
    "    # 7) Pitcher lagged mean\n",
    "    df[\"pitcher_ev_mean50\"] = _rolling_stat_lagged(df, [\"pitcher_id\"], \"exit_velo\", \"mean\", 50)\n",
    "    df[\"pitcher_ev_mean50\"].fillna(gm, inplace=True)\n",
    "\n",
    "    # 8) Center covariates\n",
    "    df[\"age_centered\"]    = df[\"age\"]    - df[\"age\"].median()\n",
    "    df[\"season_centered\"] = df[\"season\"] - df[\"season\"].median()\n",
    "    df[\"level_idx\"]       = df[\"level_abbr\"].map({\"AA\":0, \"AAA\":1, \"MLB\":2})\n",
    "\n",
    "    return df\n",
    "\n",
    "###############################################################################\n",
    "# CLI entry‑point (quick smoke test)\n",
    "###############################################################################\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path, debug = True)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    print(df_fe.columns)\n",
    "    \n",
    "    # --- inspect nulls in the raw data ---\n",
    "    null_counts = df_fe.isnull().sum()\n",
    "    print(null_counts)\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values in raw data.\")\n",
    "    else:\n",
    "        print(\"=== Raw data null counts ===\")\n",
    "        for col, cnt in null_counts.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColumnSchema: separates raw and engineered columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/data/ColumnSchema.py\n",
    "\"\"\" \n",
    "Column schema helper for exit‑velo project.\n",
    "\n",
    "Centralises every raw and engineered column name in one place and exposes\n",
    " type‑safe accessors so downstream code never hard‑codes strings.\n",
    "\n",
    "Usage\n",
    "-----\n",
    ">>> from src.features.columns import cols\n",
    ">>> num_cols = cols.numerical()\n",
    ">>> ord_cols = cols.ordinal()\n",
    ">>> all_for_model = cols.model_features()\n",
    "\"\"\"\n",
    "\n",
    "from functools import lru_cache\n",
    "from typing import List, Dict\n",
    "import json\n",
    "\n",
    "class _ColumnSchema:\n",
    "    \"\"\"Container for canonical column lists.\n",
    "\n",
    "    Keeping everything behind methods avoids accidental mutation and lets\n",
    "    IDEs offer autocompletion (because the return type is always `List[str]`).\n",
    "    \"\"\"\n",
    "\n",
    "    _ID_COLS: List[str] = [\n",
    "        \"season\", \"batter_id\", \"pitcher_id\",\n",
    "    ]\n",
    "\n",
    "    _ORDINAL_CAT_COLS: List[str] = [\n",
    "        \"level_abbr\",   # AA < AAA < MLB\n",
    "        \"age_bin\",      # 4 quantile bins of age\n",
    "        \"la_bin\",       # 5 quantile bins of launch angle\n",
    "        \"spray_bin\",    # 3 quantile bins of spray angle\n",
    "        # \"outcome\",      # categorical outcome (out, single, double, triple, home run, sacrifice fly, sacrifice bunt, fielders choice)\n",
    "    ]\n",
    "\n",
    "\n",
    "    _NOMINAL_CAT_COLS = [\n",
    "        \"hit_type\",\n",
    "        \"pitch_group\",\n",
    "        \"batter_hand\",\n",
    "        \"pitcher_hand\",\n",
    "        \"hand_match\",\n",
    "        \"pitch_hand_match\",\n",
    "        \"same_hand\",\n",
    "    ]\n",
    "\n",
    "    _NUMERICAL_COLS = [\n",
    "        \"launch_angle\",\n",
    "        \"spray_angle\",\n",
    "        \"hangtime\",\n",
    "        \"height_diff\",\n",
    "        \"age_sq\",\n",
    "        \"age_centered\",\n",
    "        \"season_centered\",\n",
    "        \"level_idx\",\n",
    "        \"player_ev_mean50\",\n",
    "        \"player_ev_std50\",\n",
    "        \"pitcher_ev_mean50\",\n",
    "    ]\n",
    "\n",
    "\n",
    "    _TARGET_COL: str = \"exit_velo\"\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    # Public helpers\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    def id(self) -> List[str]:\n",
    "        return self._ID_COLS.copy()\n",
    "\n",
    "    def ordinal(self) -> List[str]:\n",
    "        return self._ORDINAL_CAT_COLS.copy()\n",
    "\n",
    "    def nominal(self) -> List[str]:\n",
    "        return self._NOMINAL_CAT_COLS.copy()\n",
    "\n",
    "    def categorical(self) -> List[str]:\n",
    "        \"\"\"All cat cols (ordinal + nominal).\"\"\"\n",
    "        return self._ORDINAL_CAT_COLS + self._NOMINAL_CAT_COLS\n",
    "\n",
    "    def numerical(self) -> List[str]:\n",
    "        return self._NUMERICAL_COLS.copy()\n",
    "\n",
    "\n",
    "    def target(self) -> str:\n",
    "        return self._TARGET_COL\n",
    "\n",
    "    # ------------------------------------------------------------------\n",
    "    @lru_cache(maxsize=1)\n",
    "    def model_features(self) -> List[str]:\n",
    "        \"\"\"Columns fed into the ML pipeline *after* preprocess.\n",
    "\n",
    "        Excludes the target but includes derived cols.\n",
    "        \"\"\"\n",
    "        phys_minus_target = [\n",
    "            c for c in self._NUMERICAL_COLS if c != self._TARGET_COL\n",
    "        ]\n",
    "\n",
    "        return (\n",
    "            phys_minus_target\n",
    "            + self._NOMINAL_CAT_COLS\n",
    "            + self._ORDINAL_CAT_COLS  # some algos want raw string order\n",
    "        )\n",
    "\n",
    "    def all_raw(self) -> List[str]:\n",
    "        \"\"\"Returns every column expected in raw input CSV (incl. engineered).\"\"\"\n",
    "        return (\n",
    "            self._ID_COLS\n",
    "            + self._ORDINAL_CAT_COLS\n",
    "            + self._NOMINAL_CAT_COLS\n",
    "            + self._NUMERICAL_COLS\n",
    "        )\n",
    "\n",
    "    def as_dict(self) -> Dict[str, List[str]]:\n",
    "        \"\"\"Dictionary form – handy for YAML/JSON dumps.\"\"\"\n",
    "        return {\n",
    "            \"id\": self.id(),\n",
    "            \"ordinal\": self.ordinal(),\n",
    "            \"nominal\": self.nominal(),\n",
    "            \"numerical\": self.numerical(),\n",
    "            \"target\": [self.target()],\n",
    "        }\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Target column:      \", cols.target())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    print(\"\\nAs dict (JSON):\")\n",
    "    print(json.dumps(cols.as_dict(), indent=2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/features/eda.py\n",
    "\n",
    "import pandas as pd  # type: ignore\n",
    "import matplotlib.pyplot as plt  # type: ignore\n",
    "import numpy as np  # type: ignore\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from pandas.api.types import is_categorical_dtype\n",
    "\n",
    "# Optional imports with fallbacks for advanced statistics\n",
    "try:\n",
    "    import scipy.stats as stats  # type: ignore\n",
    "    from statsmodels.nonparametric.smoothers_lowess import (\n",
    "        lowess  # type: ignore\n",
    "    )\n",
    "    from statsmodels.stats.stattools import (\n",
    "        durbin_watson  # type: ignore\n",
    "    )\n",
    "    _HAS_STATS_LIBS = True\n",
    "except ImportError:\n",
    "    _HAS_STATS_LIBS = False\n",
    "    print(\"Warning: scipy or statsmodels not available. \"\n",
    "          \"Some diagnostics will be limited.\")\n",
    "\n",
    "from scipy.stats import f_oneway, ttest_ind\n",
    "\n",
    "\n",
    "\n",
    "def get_column_groups() -> dict:\n",
    "    \"\"\"\n",
    "    Return a mapping of column-type → list of columns,\n",
    "    based on the canonical schema in src.features.feature_selection.cols.\n",
    "    \"\"\"\n",
    "    return cols.as_dict()\n",
    "\n",
    "def check_nulls(df: pd.DataFrame):\n",
    "    # Identify columns with null values\n",
    "    null_columns = df.columns[df.isnull().any()].tolist()\n",
    "    \n",
    "    # Output the columns with null values\n",
    "    if null_columns:\n",
    "        print(\"Columns with null values:\", null_columns)\n",
    "    else:\n",
    "        print(\"No columns with null values.\")\n",
    "\n",
    "\n",
    "def quick_pulse_check(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    level_col: str = \"level_abbr\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Print a quick summary table:\n",
    "      - total rows\n",
    "      - unique batters\n",
    "      - overall median exit_velo\n",
    "      - median exit_velo by level\n",
    "      - distribution of events per batter (median, 25th pct)\n",
    "      - distribution of seasons per batter\n",
    "      - pearson correlations of velo with launch_angle & hangtime\n",
    "    Returns a pd.DataFrame with those metrics.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    total_rows = len(df)\n",
    "    n_batters = df[group_col].nunique()\n",
    "    overall_med = df[velo_col].median()\n",
    "\n",
    "    # median by level\n",
    "    med_by_level = df.groupby(level_col)[velo_col].median()\n",
    "\n",
    "    # events per batter\n",
    "    ev_per = df[group_col].value_counts()\n",
    "    ev_stats = ev_per.quantile([0.25, 0.5]).to_dict()\n",
    "\n",
    "    # seasons per batter\n",
    "    seasons_per = df.groupby(group_col)[\"season\"].nunique()\n",
    "    seasons_stats = seasons_per.value_counts().sort_index().to_dict()\n",
    "\n",
    "    # basic correlations\n",
    "    corr = df[[velo_col, \"launch_angle\", \"hangtime\"]].corr()[velo_col].drop(velo_col)\n",
    "\n",
    "    # Build a summary table\n",
    "    metrics = [\n",
    "        \"Total rows\",\n",
    "        \"Unique batters\",\n",
    "        \"Overall median EV\",\n",
    "    ]\n",
    "    values = [\n",
    "        total_rows,\n",
    "        n_batters,\n",
    "        overall_med,\n",
    "    ]\n",
    "    \n",
    "    # Add level-specific metrics\n",
    "    for lvl in med_by_level.index:\n",
    "        metrics.append(f\"Median EV @ {lvl}\")\n",
    "        values.append(med_by_level[lvl])\n",
    "    \n",
    "    # Add batter event metrics\n",
    "    metrics.extend([\n",
    "        \"Events per batter (25th pct)\",\n",
    "        \"Events per batter (median)\",\n",
    "    ])\n",
    "    values.extend([\n",
    "        ev_stats.get(0.25, \"N/A\"),\n",
    "        ev_stats.get(0.5, \"N/A\"),\n",
    "    ])\n",
    "    \n",
    "    # Add season distribution\n",
    "    for season_count, count in seasons_stats.items():\n",
    "        metrics.append(f\"Batters with {season_count} season(s)\")\n",
    "        values.append(count)\n",
    "    \n",
    "    # Add correlations\n",
    "    metrics.extend([\n",
    "        \"ρ(exit_velo, launch_angle)\",\n",
    "        \"ρ(exit_velo, hangtime)\",\n",
    "    ])\n",
    "    values.extend([\n",
    "        corr.get(\"launch_angle\", \"N/A\"),\n",
    "        corr.get(\"hangtime\", \"N/A\"),\n",
    "    ])\n",
    "\n",
    "    table = pd.DataFrame({\n",
    "        \"Metric\": metrics,\n",
    "        \"Value\": values\n",
    "    })\n",
    "    \n",
    "    print(table.to_string(index=False))\n",
    "    return table\n",
    "\n",
    "\n",
    "def red_flag_small_samples(df: pd.DataFrame,\n",
    "                           group_col: str = \"batter_id\",\n",
    "                           threshold: int = 15) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Identify batters with fewer than `threshold` events.\n",
    "    Returns a Series of counts indexed by batter_id.\n",
    "    \"\"\"\n",
    "    counts = df[group_col].value_counts()\n",
    "    small = counts[counts < threshold]\n",
    "    print(f\"> Batters with fewer than {threshold} events: {len(small)}\")\n",
    "    if len(small) > 0:\n",
    "        print(f\"  First few: {', '.join(map(str, small.index[:5]))}\")\n",
    "    return small\n",
    "\n",
    "\n",
    "def red_flag_level_effect(df: pd.DataFrame,\n",
    "                          level_col: str = \"level_abbr\",\n",
    "                          velo_col: str = \"exit_velo\") -> tuple:\n",
    "    \"\"\"\n",
    "    One-way ANOVA of exit_velo across levels.\n",
    "    Returns (F-statistic, p-value) or (None, None) if scipy is not available.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> ANOVA on exit_velo by level: scipy not available\")\n",
    "        print(\"> Basic level summary instead:\")\n",
    "        summary = df.groupby(level_col)[velo_col].agg(['mean', 'std', 'count'])\n",
    "        print(summary)\n",
    "        return None, None\n",
    "    \n",
    "    groups = [\n",
    "        df[df[level_col] == lvl][velo_col].dropna()\n",
    "        for lvl in df[level_col].unique()\n",
    "    ]\n",
    "    F, p = stats.f_oneway(*groups)\n",
    "    print(f\"> ANOVA on {velo_col} by {level_col}: F={F:.3f}, p={p:.3e}\")\n",
    "    return F, p\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  REPLACE old red_flag_level_effect  → clearer name & doc\n",
    "# ------------------------------------------------------------------\n",
    "def league_level_effect(\n",
    "    df: pd.DataFrame,\n",
    "    level_col: str = \"level_abbr\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    ") -> tuple[float | None, float | None]:\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – confirms MLB vs Triple‑A (etc.) differences to\n",
    "      justify hierarchical level effects in the model.\n",
    "\n",
    "    One‑way ANOVA of `exit_velo` across `level_col`.\n",
    "    Returns (F, p) or (None, None) if SciPy unavailable.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> SciPy unavailable – falling back to group summary\")\n",
    "        print(df.groupby(level_col)[velo_col].describe())\n",
    "        return None, None\n",
    "\n",
    "    groups = [df[df[level_col] == lv][velo_col].dropna()\n",
    "              for lv in df[level_col].unique()]\n",
    "    f_val, p_val = stats.f_oneway(*groups)\n",
    "    print(f\"> Level effect ANOVA: F={f_val:.3f}, p={p_val:.3e}\")\n",
    "    return f_val, p_val\n",
    "\n",
    "\n",
    "\n",
    "def diag_age_effect(df: pd.DataFrame,\n",
    "                    age_col: str = \"age_centered\",\n",
    "                    velo_col: str = \"exit_velo\") -> np.ndarray | None:\n",
    "    \"\"\"\n",
    "    LOWESS smoothing of exit_velo vs. age_centered.\n",
    "    Returns the smoothed array or None if statsmodels is not available.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> Age effect analysis: statsmodels not available\")\n",
    "        print(\"> Basic correlation instead:\")\n",
    "        corr = df[[age_col, velo_col]].corr().iloc[0, 1]\n",
    "        print(f\"Correlation between {age_col} and {velo_col}: {corr:.3f}\")\n",
    "        return None\n",
    "    \n",
    "    # Run LOWESS smoothing\n",
    "    smooth_result = lowess(df[velo_col], df[age_col])\n",
    "    \n",
    "    # Plot the result\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.scatter(df[age_col], df[velo_col], alpha=0.1, s=1, color='gray')\n",
    "    plt.plot(\n",
    "        smooth_result[:, 0], \n",
    "        smooth_result[:, 1], \n",
    "        'r-', \n",
    "        linewidth=2, \n",
    "        label=\"LOWESS fit\"\n",
    "    )\n",
    "    plt.xlabel(age_col)\n",
    "    plt.ylabel(velo_col)\n",
    "    plt.title(\"Age effect (LOWESS)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    return smooth_result\n",
    "\n",
    "\n",
    "def diag_time_series_dw(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"season\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    velo_col: str = \"exit_velo\"\n",
    ") -> pd.Series | None:\n",
    "    \"\"\"\n",
    "    Compute Durbin–Watson on each batter's time series of mean exit_velo.\n",
    "    Returns a Series of DW statistics or None if statsmodels is not available.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> Time series analysis: statsmodels not available\")\n",
    "        return None\n",
    "    \n",
    "    # Create pivot table of seasons (columns) by batters (rows)\n",
    "    pivot = (\n",
    "        df\n",
    "        .groupby([group_col, time_col])[velo_col]\n",
    "        .mean()\n",
    "        .unstack(fill_value=np.nan)\n",
    "    )\n",
    "    \n",
    "    # Only process batters with at least 3 seasons\n",
    "    valid_batters = pivot.dropna(thresh=3).index\n",
    "    if len(valid_batters) == 0:\n",
    "        print(\"> No batters with sufficient seasons for Durbin-Watson test\")\n",
    "        return None\n",
    "    \n",
    "    # Calculate DW statistic for each valid batter\n",
    "    dw_stats = {}\n",
    "    for batter in valid_batters:\n",
    "        series = pivot.loc[batter].dropna()\n",
    "        if len(series) >= 3:  # Recheck after dropna\n",
    "            dw = durbin_watson(series)\n",
    "            dw_stats[batter] = dw\n",
    "    \n",
    "    dw_series = pd.Series(dw_stats)\n",
    "    print(\n",
    "        f\"> Mean Durbin–Watson across {len(dw_series)} batters: \"\n",
    "        f\"{dw_series.mean():.3f}\"\n",
    "    )\n",
    "    print(\"> DW < 1.5 suggests positive autocorrelation\")\n",
    "    print(\"> DW > 2.5 suggests negative autocorrelation\")\n",
    "    print(\"> DW ≈ 2.0 suggests no autocorrelation\")\n",
    "    \n",
    "    return dw_series\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  REPLACE old diag_time_series_dw WITH optional helper\n",
    "# ------------------------------------------------------------------\n",
    "def _optional_dw_check(\n",
    "    df: pd.DataFrame,\n",
    "    time_col: str = \"season\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    ") -> pd.Series | None:\n",
    "    \"\"\"\n",
    "    (OPTIONAL) Durbin–Watson residual autocorrelation **per batter**.\n",
    "    Mostly irrelevant for cross‑sectional EV analysis but retained\n",
    "    behind a private name for power users.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        return None\n",
    "    pivot = (\n",
    "        df.groupby([group_col, time_col])[velo_col]\n",
    "          .mean().unstack()\n",
    "    )\n",
    "    stats_out = {}\n",
    "    for idx, row in pivot.dropna(thresh=3).iterrows():\n",
    "        if row.count() >= 3:\n",
    "            stats_out[idx] = durbin_watson(row.dropna())\n",
    "    if not stats_out:\n",
    "        print(\"> DW check: no eligible batters\")\n",
    "        return None\n",
    "    s = pd.Series(stats_out)\n",
    "    print(f\"DW mean={s.mean():.2f} (1.5<→pos autocorr, >2.5→neg)\")\n",
    "    return s\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def check_red_flags(df: pd.DataFrame, \n",
    "                    sample_threshold: int = 15) -> dict:\n",
    "    \"\"\"\n",
    "    Run all red flag checks and return the results in a dictionary.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Check for small sample sizes\n",
    "    small_samples = red_flag_small_samples(df, threshold=sample_threshold)\n",
    "    results['small_samples'] = small_samples\n",
    "    \n",
    "    # Check for level effects\n",
    "    f_stat, p_val = red_flag_level_effect(df)\n",
    "    results['level_effect'] = {\n",
    "        'f_statistic': f_stat,\n",
    "        'p_value': p_val\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def plot_distributions(df: pd.DataFrame,\n",
    "                       velo_col: str = \"exit_velo\",\n",
    "                       by: str = \"level_abbr\"):\n",
    "    \"\"\"\n",
    "    Histogram of `velo_col` faceted by `by`.\n",
    "    Returns the Matplotlib figure so callers can save or show it.\n",
    "    \"\"\"\n",
    "    groups = df[by].unique()\n",
    "    fig, axes = plt.subplots(len(groups), 1,\n",
    "                             figsize=(6, 2.8 * len(groups)),\n",
    "                             sharex=True)\n",
    "    for ax, grp in zip(axes, groups):\n",
    "        ax.hist(df[df[by] == grp][velo_col], bins=30, alpha=0.75)\n",
    "        ax.set_title(f\"{by} = {grp} (n={len(df[df[by] == grp])})\")\n",
    "        ax.set_xlabel(velo_col)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_correlations(df: pd.DataFrame, cols: list[str]):\n",
    "    \"\"\"\n",
    "    Heat-map of Pearson correlations for `cols`.\n",
    "    \"\"\"\n",
    "    corr = df[cols].corr()\n",
    "    fig, ax = plt.subplots(figsize=(0.6 * len(cols) + 2,\n",
    "                                    0.6 * len(cols) + 2))\n",
    "    im = ax.imshow(corr, vmin=-1, vmax=1, cmap=\"coolwarm\")\n",
    "    fig.colorbar(im, ax=ax, shrink=0.8)\n",
    "    ax.set_xticks(range(len(cols)), cols, rotation=90)\n",
    "    ax.set_yticks(range(len(cols)), cols)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def plot_time_trends(df: pd.DataFrame,\n",
    "                     time_col: str = \"season\",\n",
    "                     group_col: str = \"batter_id\",\n",
    "                     velo_col: str = \"exit_velo\",\n",
    "                     sample: int = 50):\n",
    "    \"\"\"\n",
    "    Plot mean exit-velo over time for a random sample of batters.\n",
    "    \"\"\"\n",
    "    batters = df[group_col].unique()\n",
    "    chosen = np.random.choice(batters,\n",
    "                              min(sample, len(batters)),\n",
    "                              replace=False)\n",
    "    fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    for b in chosen:\n",
    "        series = (\n",
    "            df[df[group_col] == b]\n",
    "            .groupby(time_col)[velo_col]\n",
    "            .mean()\n",
    "        )\n",
    "        ax.plot(series.index, series.values, alpha=0.3)\n",
    "    ax.set_xlabel(time_col)\n",
    "    ax.set_ylabel(velo_col)\n",
    "    ax.set_title(\"Sample batter exit-velo over time\")\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "def summarize_numeric_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    numeric_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Summarise each numeric predictor against the target.\n",
    "\n",
    "    Returns a DataFrame indexed by feature with:\n",
    "      n          – number of non‑null pairs\n",
    "      pearson_r  – Pearson correlation coefficient\n",
    "    \"\"\"\n",
    "    # --- Pull fresh lists from the schema every time -----------------\n",
    "    groups = cols.as_dict()\n",
    "\n",
    "    if numeric_cols is None:\n",
    "        numeric_cols = groups.get(\"numerical\", [])\n",
    "\n",
    "    # --- Clean the list ---------------------------------------------\n",
    "    numeric_cols = [\n",
    "        c for c in numeric_cols\n",
    "        if c != target_col and c in df.columns      # ❶ exclude target, ❷ guard\n",
    "    ]\n",
    "\n",
    "    records = []\n",
    "    for col in numeric_cols:\n",
    "        sub = df[[col, target_col]].dropna()\n",
    "        if sub.empty:               # skip columns that are all‑NA\n",
    "            continue\n",
    "        r = sub[col].corr(sub[target_col])\n",
    "        records.append({\"feature\": col, \"n\": len(sub), \"pearson_r\": r})\n",
    "\n",
    "    result = (\n",
    "        pd.DataFrame.from_records(records)\n",
    "        .set_index(\"feature\")\n",
    "        .sort_values(\"pearson_r\", ascending=False)\n",
    "    )\n",
    "\n",
    "    print(\"\\n=== Numeric vs target correlations ===\")\n",
    "    print(result)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def plot_numeric_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    numeric_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Scatter plots of each numeric predictor vs the target with r‑value in title.\n",
    "    \"\"\"\n",
    "    summary = summarize_numeric_vs_target(df, numeric_cols, target_col)\n",
    "    for feature, row in summary.iterrows():\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.scatter(\n",
    "            df[feature], df[target_col],\n",
    "            alpha=0.3, s=5, edgecolors=\"none\"\n",
    "        )\n",
    "        plt.title(f\"{feature} vs {target_col}  (r = {row['pearson_r']:.2f})\")\n",
    "        plt.xlabel(feature)\n",
    "        plt.ylabel(target_col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def summarize_categorical_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    cat_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\"\n",
    ") -> dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    For each categorical feature, returns a DataFrame of:\n",
    "      count, mean, median, std of the target by category.\n",
    "    \"\"\"\n",
    "    groups = get_column_groups()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = groups.get(\"categorical\", [])\n",
    "\n",
    "    summaries: dict[str, pd.DataFrame] = {}\n",
    "    for col in cat_cols:\n",
    "        stats = (\n",
    "            df\n",
    "            .groupby(col)[target_col]\n",
    "            .agg(count=\"count\", mean=\"mean\", median=\"median\", std=\"std\")\n",
    "            .sort_values(\"count\", ascending=False)\n",
    "        )\n",
    "        print(f\"\\n=== {col} vs {target_col} summary ===\")\n",
    "        print(stats)\n",
    "        summaries[col] = stats\n",
    "    return summaries\n",
    "\n",
    "\n",
    "def plot_categorical_vs_target(\n",
    "    df: pd.DataFrame,\n",
    "    cat_cols: list[str] | None = None,\n",
    "    target_col: str = \"exit_velo\"\n",
    "):\n",
    "    \"\"\"\n",
    "    For each categorical feature, draw a box‑plot of the target by category.\n",
    "    \"\"\"\n",
    "    groups = get_column_groups()\n",
    "    if cat_cols is None:\n",
    "        cat_cols = groups.get(\"categorical\", [])\n",
    "\n",
    "    for col in cat_cols:\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        df.boxplot(column=target_col, by=col, vert=False,\n",
    "                   grid=False, patch_artist=True)\n",
    "        plt.title(f\"{target_col} by {col}\")\n",
    "        plt.suptitle(\"\")           # remove pandas' automatic suptitle\n",
    "        plt.xlabel(target_col)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def examine_and_filter_by_sample_size(\n",
    "    df: pd.DataFrame,\n",
    "    count_col: str = \"exit_velo\",\n",
    "    group_col: str = \"batter_id\",\n",
    "    season_col: str = \"season\",\n",
    "    percentile: float = 0.05,\n",
    "    min_count: int | None = None,\n",
    "    filter_df: bool = False,\n",
    ") -> tuple[dict[int, pd.DataFrame], pd.DataFrame | None]:\n",
    "    \"\"\"\n",
    "    For each season:\n",
    "      - compute per-batter count, mean, std of `count_col`\n",
    "      - pick cutoff: min_count if provided, else the `percentile` quantile\n",
    "      - print diagnostics\n",
    "      - plot histograms *safely* (drops NaNs first)\n",
    "    Returns:\n",
    "      - summaries: dict season → per-batter summary DataFrame\n",
    "      - filtered_df: if filter_df, the original df filtered to batters ≥ cutoff\n",
    "    \"\"\"\n",
    "    summaries: dict[int, pd.DataFrame] = {}\n",
    "    mask_keep: list[pd.Series] = []\n",
    "\n",
    "    for season, sub in df.groupby(season_col):\n",
    "        # 1) per-batter summary (count *non-NA* exit_velo)\n",
    "        summary = (\n",
    "            sub\n",
    "            .groupby(group_col)[count_col]\n",
    "            .agg(count=\"count\", mean=\"mean\", std=\"std\")\n",
    "            .sort_values(\"count\")\n",
    "        )\n",
    "        summaries[season] = summary\n",
    "\n",
    "        # 2) determine cutoff\n",
    "        cutoff = min_count if min_count is not None else int(summary[\"count\"].quantile(percentile))\n",
    "        small = summary[summary[\"count\"] < cutoff]\n",
    "        large = summary[summary[\"count\"] >= cutoff]\n",
    "\n",
    "        # 3) diagnostics\n",
    "        print(f\"\\n=== Season {season} (cutoff = {cutoff}) ===\")\n",
    "        print(f\"  small (<{cutoff} events): {len(small)} batters\")\n",
    "        print(small[[\"count\",\"mean\",\"std\"]].describe(), \"\\n\")\n",
    "        print(f\"  large (≥{cutoff} events): {len(large)} batters\")\n",
    "        print(large[[\"count\",\"mean\",\"std\"]].describe())\n",
    "\n",
    "        # 4) **safe plotting**: drop NaNs, skip if nothing to plot\n",
    "        small_means = small[\"mean\"].dropna()\n",
    "        large_means = large[\"mean\"].dropna()\n",
    "\n",
    "        if small_means.empty and large_means.empty:\n",
    "            print(f\"  ⚠️  Season {season}: no valid per-batter means to plot\")\n",
    "        else:\n",
    "            plt.figure(figsize=(8, 3))\n",
    "            if not small_means.empty:\n",
    "                plt.hist(small_means, bins=30, alpha=0.6, label=f\"n<{cutoff}\")\n",
    "            if not large_means.empty:\n",
    "                plt.hist(large_means, bins=30, alpha=0.6, label=f\"n≥{cutoff}\")\n",
    "            plt.title(f\"Season {season}: per-batter EV means\")\n",
    "            plt.xlabel(\"Mean exit_velo\")\n",
    "            plt.legend()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # 5) build mask to keep only large-sample batters\n",
    "        if filter_df:\n",
    "            keep_ids = large.index\n",
    "            mask_keep.append(\n",
    "                (df[season_col] == season) &\n",
    "                (df[group_col].isin(keep_ids))\n",
    "            )\n",
    "\n",
    "    # 6) combine masks and filter\n",
    "    filtered_df = None\n",
    "    if filter_df and mask_keep:\n",
    "        combined = pd.concat(mask_keep, axis=1).any(axis=1)\n",
    "        filtered_df = df[combined].copy()\n",
    "\n",
    "    return summaries, filtered_df\n",
    "\n",
    "\n",
    "\n",
    "def hypothesis_test(df, feature, target=\"exit_velo\", test_type=\"anova\"):\n",
    "    \"\"\"\n",
    "    Perform hypothesis tests for feature significance.\n",
    "    \"\"\"\n",
    "    if test_type == \"anova\":\n",
    "        groups = [df[df[feature] == cat][target] for cat in df[feature].unique()]\n",
    "        F, p = f_oneway(*groups)\n",
    "        print(f\"ANOVA: F={F:.3f}, p={p:.3e}\")\n",
    "        return F, p\n",
    "    elif test_type == \"ttest\":\n",
    "        group1 = df[df[feature] == 0][target]\n",
    "        group2 = df[df[feature] == 1][target]\n",
    "        t, p = ttest_ind(group1, group2)\n",
    "        print(f\"T-test: t={t:.3f}, p={p:.3e}\")\n",
    "        return t, p\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  NEW: robust outlier flagging\n",
    "# ------------------------------------------------------------------\n",
    "def flag_outliers_iqr(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    iqr_mult: float = 1.5,\n",
    ") -> pd.Series:\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – extreme EVs (>120 mph or <40 mph) can distort\n",
    "      skew / variance estimates used in hierarchical priors.\n",
    "\n",
    "    Returns a boolean Series (True = *suspect* outlier) using the\n",
    "    classic IQR rule: value < Q1 − k·IQR  or  > Q3 + k·IQR.\n",
    "    \"\"\"\n",
    "    q1, q3 = df[velo_col].quantile([0.25, 0.75])\n",
    "    iqr = q3 - q1\n",
    "    lower, upper = q1 - iqr_mult * iqr, q3 + iqr_mult * iqr\n",
    "    mask = (df[velo_col] < lower) | (df[velo_col] > upper)\n",
    "    n = int(mask.sum())\n",
    "    print(f\"> Outlier flag ({velo_col}): {n} rows outside [{lower:.1f}, {upper:.1f}]\")\n",
    "    return mask\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  NEW: EV distribution summary + QQ plot\n",
    "# ------------------------------------------------------------------\n",
    "def ev_distribution_summary(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    bins: int = 40,\n",
    "):\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – confirms right‑skew & heavy‑tail nature of EV\n",
    "      so you can choose a skew‑normal or Student‑t likelihood.\n",
    "\n",
    "    Prints skew/kurtosis, shows histogram, KDE, CDF & QQ (if scipy).\n",
    "    \"\"\"\n",
    "    data = df[velo_col].dropna()\n",
    "    print(\n",
    "        f\"Skewness = {stats.skew(data):.2f},  \"\n",
    "        f\"Kurtosis = {stats.kurtosis(data, fisher=False):.2f}\"\n",
    "    )\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(12, 3))\n",
    "    ax[0].hist(data, bins=bins, density=True, alpha=0.7)\n",
    "    data.plot(kind=\"kde\", ax=ax[0], linewidth=2)\n",
    "    ax[0].set_title(\"Histogram & KDE\")\n",
    "\n",
    "    # empirical CDF\n",
    "    ecdf_x = np.sort(data)\n",
    "    ecdf_y = np.arange(1, len(ecdf_x) + 1) / len(ecdf_x)\n",
    "    ax[1].plot(ecdf_x, ecdf_y)\n",
    "    ax[1].set_title(\"Empirical CDF\")\n",
    "\n",
    "    # QQ vs normal\n",
    "    from scipy import stats as _st\n",
    "    _st.probplot(data, dist=\"norm\", plot=ax[2])\n",
    "    ax[2].set_title(\"QQ‑plot vs Normal\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  NEW: Year/era trend diagnostic\n",
    "# ------------------------------------------------------------------\n",
    "def year_trend_ev(\n",
    "    df: pd.DataFrame,\n",
    "    season_col: str = \"season\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    ci: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – detects ball‑era shifts (e.g. 2019 “juiced”,\n",
    "      2021 “deadened”) so forecasts for 2024 use correct baseline.\n",
    "\n",
    "    Produces a table & line plot of mean/median EV per season.\n",
    "    \"\"\"\n",
    "    g = df.groupby(season_col)[velo_col]\n",
    "    stats_df = g.agg(mean=\"mean\", median=\"median\", n=\"count\")\n",
    "    print(\"\\n=== Exit‑velo by season ===\")\n",
    "    print(stats_df)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(7, 3))\n",
    "    stats_df[\"mean\"].plot(ax=ax, marker=\"o\", label=\"Mean EV\")\n",
    "    stats_df[\"median\"].plot(ax=ax, marker=\"s\", label=\"Median EV\")\n",
    "    if ci:\n",
    "        sem = g.sem()\n",
    "        ax.fill_between(\n",
    "            stats_df.index,\n",
    "            stats_df[\"mean\"] - 1.96 * sem,\n",
    "            stats_df[\"mean\"] + 1.96 * sem,\n",
    "            alpha=0.2,\n",
    "            label=\"95% CI (mean)\"\n",
    "        )\n",
    "    ax.set_ylabel(velo_col)\n",
    "    ax.set_title(\"Seasonal trend in exit velocity\")\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    return stats_df, fig\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "#  REPLACE old red_flag_level_effect  → clearer name & doc\n",
    "# ------------------------------------------------------------------\n",
    "def league_level_effect(\n",
    "    df: pd.DataFrame,\n",
    "    level_col: str = \"level_abbr\",\n",
    "    velo_col: str = \"exit_velo\",\n",
    ") -> tuple[float | None, float | None]:\n",
    "    \"\"\"\n",
    "    🔹 Why it matters – confirms MLB vs Triple‑A (etc.) differences to\n",
    "      justify hierarchical level effects in the model.\n",
    "\n",
    "    One‑way ANOVA of `exit_velo` across `level_col`.\n",
    "    Returns (F, p) or (None, None) if SciPy unavailable.\n",
    "    \"\"\"\n",
    "    if not _HAS_STATS_LIBS:\n",
    "        print(\"> SciPy unavailable – falling back to group summary\")\n",
    "        print(df.groupby(level_col)[velo_col].describe())\n",
    "        return None, None\n",
    "\n",
    "    groups = [df[df[level_col] == lv][velo_col].dropna()\n",
    "              for lv in df[level_col].unique()]\n",
    "    f_val, p_val = stats.f_oneway(*groups)\n",
    "    print(f\"> Level effect ANOVA: F={f_val:.3f}, p={p_val:.3e}\")\n",
    "    return f_val, p_val\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# debugs:\n",
    "def summarize_categorical_missingness(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each categorical column (ordinal + nominal), compute:\n",
    "      - original_null_count / pct\n",
    "      - imputed_missing_count / pct\n",
    "    Safely handles pandas.Categorical by first adding 'MISSING' to its categories.\n",
    "    \"\"\"\n",
    "    cols    = _ColumnSchema()\n",
    "    cat_cols = cols.ordinal() + cols.nominal()\n",
    "    summary = []\n",
    "    n = len(df)\n",
    "\n",
    "    for col in cat_cols:\n",
    "        ser = df[col]\n",
    "        orig_null = ser.isna().sum()\n",
    "\n",
    "        # If it's a Categorical, add 'MISSING' as a valid category\n",
    "        if is_categorical_dtype(ser):\n",
    "            ser = ser.cat.add_categories(['MISSING'])\n",
    "\n",
    "        # Count rows that would become 'MISSING'\n",
    "        imputed_missing = ser.fillna('MISSING').eq('MISSING').sum()\n",
    "\n",
    "        summary.append({\n",
    "            'column': col,\n",
    "            'original_null_count':   orig_null,\n",
    "            'original_null_pct':     orig_null / n,\n",
    "            'imputed_missing_count': imputed_missing,\n",
    "            'imputed_missing_pct':   imputed_missing / n,\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(summary)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "\n",
    "    # --- inspect nulls in the raw data ---\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values in raw data.\")\n",
    "    else:\n",
    "        print(\"=== Raw data null counts ===\")\n",
    "        for col, cnt in null_counts.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "\n",
    "    \n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:     \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    # use list‐comprehension to drop target(s) from numerical features\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "\n",
    "    # Filter out rows with nulls in the target column(s)\n",
    "    target_cols = cols.target()\n",
    "    df_filtered = df.dropna(subset=target_cols)\n",
    "\n",
    "    # Check for nulls in the filtered data\n",
    "    null_counts_filtered = df_filtered.isnull().sum()\n",
    "    null_counts_filtered = null_counts_filtered[null_counts_filtered > 0]\n",
    "    if null_counts_filtered.empty:\n",
    "        print(\"✅  No missing values in filtered data.\")\n",
    "    else:\n",
    "        print(\"=== Filtered data null counts ===\")\n",
    "        for col, cnt in null_counts_filtered.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "\n",
    "    print(\"\\n===== check on small samples =====\")\n",
    "    summaries, _ = examine_and_filter_by_sample_size(df_filtered, percentile=0.05)\n",
    "    summaries, df_filtered = examine_and_filter_by_sample_size(\n",
    "        df_filtered, percentile=0.05, min_count=15, filter_df=False\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Example usage\n",
    "    print(\"\\n===== NULLS CHECK =====\")\n",
    "    check_nulls(df_fe)\n",
    "    \n",
    "    print(\"\\n===== QUICK PULSE CHECK =====\")\n",
    "    quick_pulse_check(df_fe)\n",
    "    \n",
    "    print(\"\\n===== RED FLAGS CHECK =====\")\n",
    "    check_red_flags(df_fe)\n",
    "    \n",
    "    print(\"\\n===== AGE EFFECT ANALYSIS =====\")\n",
    "    diag_age_effect(df_fe, age_col=\"age\")\n",
    "    \n",
    "    print(\"\\n===== TIME SERIES ANALYSIS =====\")\n",
    "    diag_time_series_dw(df_fe)\n",
    "    \n",
    "    print(\"\\n===== PLOTTING =====\")\n",
    "    fig1 = plot_distributions(df_fe, by=\"hit_type\")\n",
    "    fig2 = plot_correlations(df_fe, numericals)  # Using cols schema\n",
    "    fig3 = plot_time_trends(df_fe, sample=20)\n",
    "\n",
    "\n",
    "    # — Numeric features —\n",
    "    num_summary = summarize_numeric_vs_target(df_fe)\n",
    "    plot_numeric_vs_target(df_fe)\n",
    "\n",
    "    # — Categorical features —\n",
    "    cat_summary = summarize_categorical_vs_target(df_fe)\n",
    "    plot_categorical_vs_target(df_fe)\n",
    "\n",
    "    # Example: Test if age has significant effect\n",
    "    hypothesis_test(df_fe, feature=\"age_bin\", test_type=\"anova\")\n",
    "    \n",
    "    \n",
    "    league_level_effect(df_fe)\n",
    "    year_trend_ev(df_fe)\n",
    "    flag_outliers_iqr(df_fe)\n",
    "    ev_distribution_summary(df_fe)\n",
    "    # _optional_dw_check(df_fe)   # only if you still care\n",
    "\n",
    "\n",
    "    summary_df = summarize_categorical_missingness(df_fe)\n",
    "    print(summary_df.to_markdown(index=False))\n",
    "    \n",
    "    # uniques of outcome\n",
    "    print(\"outcome uniques:=========================\")\n",
    "    print(df_fe[\"outcome\"].unique())\n",
    "    # uniques of hit_type\n",
    "    print(\"hit_type uniques:=========================\")\n",
    "    print(df_fe[\"hit_type\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/features/data_prep.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def compute_clip_bounds(\n",
    "    series: pd.Series,\n",
    "    *,\n",
    "    method: str = \"quantile\",\n",
    "    quantiles: tuple[float,float] = (0.01,0.99),\n",
    "    std_multiplier: float = 3.0,\n",
    ") -> tuple[float,float]:\n",
    "    \"\"\"\n",
    "    Compute (lower, upper) but do not apply them.\n",
    "    \"\"\"\n",
    "    s = series.dropna()\n",
    "    if method == \"quantile\":\n",
    "        return tuple(s.quantile(list(quantiles)).to_list())\n",
    "    if method == \"mean_std\":\n",
    "        mu, sigma = s.mean(), s.std()\n",
    "        return mu - std_multiplier*sigma, mu + std_multiplier*sigma\n",
    "    if method == \"iqr\":\n",
    "        q1, q3 = s.quantile([0.25,0.75])\n",
    "        iqr = q3 - q1\n",
    "        return q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "    raise ValueError(f\"Unknown method {method}\")\n",
    "\n",
    "\n",
    "def clip_extreme_ev(\n",
    "    df: pd.DataFrame,\n",
    "    velo_col: str = \"exit_velo\",\n",
    "    lower: float | None = None,\n",
    "    upper: float | None = None,\n",
    "    *,\n",
    "    method: str = \"quantile\",\n",
    "    quantiles: tuple[float, float] = (0.01, 0.99),\n",
    "    std_multiplier: float = 3.0,\n",
    "    debug: bool = False,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clip exit velocities to [lower, upper].\n",
    "\n",
    "    If lower/upper are None, compute them from the data using one of:\n",
    "      - method=\"quantile\": use df[velo_col].quantile(quantiles)\n",
    "      - method=\"mean_std\":  use mean ± std_multiplier * std\n",
    "      - method=\"iqr\":       use [Q1 - 1.5*IQR, Q3 + 1.5*IQR]\n",
    "\n",
    "    When debug=True, prints counts & percentages of values that will be clipped.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    series = df[velo_col].dropna()\n",
    "\n",
    "    # 1) infer bounds if not given\n",
    "    if lower is None or upper is None:\n",
    "        if method == \"quantile\":\n",
    "            low_q, high_q = quantiles\n",
    "            lower_, upper_ = series.quantile([low_q, high_q]).to_list()\n",
    "        elif method == \"mean_std\":\n",
    "            mu, sigma = series.mean(), series.std()\n",
    "            lower_, upper_ = mu - std_multiplier * sigma, mu + std_multiplier * sigma\n",
    "        elif method == \"iqr\":\n",
    "            q1, q3 = series.quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            lower_, upper_ = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method '{method}' for clip_extreme_ev\")\n",
    "        lower  = lower  if lower  is not None else lower_\n",
    "        upper  = upper  if upper  is not None else upper_\n",
    "\n",
    "    # 2) debug: count how many will be clipped\n",
    "    if debug:\n",
    "        total   = len(series)\n",
    "        n_low   = (series < lower).sum()\n",
    "        n_high  = (series > upper).sum()\n",
    "        print(f\"[clip_extreme_ev] lower={lower:.2f}, upper={upper:.2f}\")\n",
    "        print(f\"  → {n_low:,} / {total:,} ({n_low/total:.2%}) below lower\")\n",
    "        print(f\"  → {n_high:,} / {total:,} ({n_high/total:.2%}) above upper\")\n",
    "\n",
    "    # 3) actually clip\n",
    "    df[velo_col] = df[velo_col].clip(lower, upper)\n",
    "    return df\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def filter_bunts_and_popups(\n",
    "    df: pd.DataFrame,\n",
    "    hit_col: str = \"hit_type\",\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop bunts and pop-ups from the DataFrame.\n",
    "    If debug=True, prints how many rows were removed.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    initial = len(df)\n",
    "    mask = ~df[hit_col].str.upper().isin([\"BUNT\", \"POP_UP\"])\n",
    "    filtered = df[mask]\n",
    "    if debug:\n",
    "        removed = initial - len(filtered)\n",
    "        print(f\"[filter_bunts_and_popups] dropped {removed:,} rows \"\n",
    "              f\"({removed/initial:.2%}) bunts/pop-ups\")\n",
    "    return filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# Smoke test / CLI entry\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.data.ColumnSchema import _ColumnSchema\n",
    "    from src.features.eda import summarize_categorical_missingness\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    # from src.features.data_prep import filter_and_clip\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    \n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    # use list‐comprehension to drop target(s) from numerical features\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "    \n",
    "\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    summary_df = summarize_categorical_missingness(df_fe)\n",
    "    print(summary_df.to_markdown(index=False))\n",
    "    \n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    \n",
    "    # filter out bunts and popups\n",
    "    df_fe = filter_bunts_and_popups(df_fe)\n",
    "    # chekc on bunts and popups\n",
    "    print(df_fe[\"hit_type\"].unique())\n",
    "    print(df_fe[\"outcome\"].unique())\n",
    "\n",
    "    debug = True\n",
    "    TARGET = cols.target()\n",
    "    lower, upper = compute_clip_bounds(\n",
    "        df[TARGET],\n",
    "        method=\"quantile\",            # default: 1st/99th percentile\n",
    "        quantiles=(0.01, 0.99),\n",
    "    )\n",
    "    if debug:\n",
    "        total = len(df)\n",
    "        n_low = (df[TARGET] < lower).sum()\n",
    "        n_high= (df[TARGET] > upper).sum()\n",
    "        print(f\"[fit_preprocessor] clipping train EV to [{lower:.2f}, {upper:.2f}]\")\n",
    "        print(f\"  → {n_low:,}/{total:,} ({n_low/total:.2%}) below\")\n",
    "        print(f\"  → {n_high:,}/{total:,} ({n_high/total:.2%}) above\")\n",
    "    df_clipped = clip_extreme_ev(df, lower=lower, upper=upper)\n",
    "\n",
    "    print(\"Final rows after filter & clip:\", len(df_clipped))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/features/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/features/preprocess.py\n",
    "\"\"\"\n",
    "Preprocessing module for exit velocity pipeline.\n",
    "Supports multiple model types (linear, XGBoost, PyMC, etc.) with\n",
    "automatic ordinal-category detection from the data.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer, IterativeImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.features.data_prep import filter_bunts_and_popups, compute_clip_bounds, clip_extreme_ev\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# Numeric & nominal pipelines (unchanged)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "numeric_linear = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='median', add_indicator=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "numeric_iterative = Pipeline([\n",
    "    ('impute', IterativeImputer(random_state=0, add_indicator=True)),\n",
    "    ('scale', StandardScaler()),\n",
    "])\n",
    "nominal_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value='MISSING')),\n",
    "    ('encode', OneHotEncoder(drop='first', handle_unknown='ignore')),\n",
    "])\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# Helper function for domain cleaning to reduce duplication\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "def filter_and_clip(\n",
    "    df: pd.DataFrame,\n",
    "    lower: float = None,\n",
    "    upper: float = None,\n",
    "    quantiles: tuple[float, float] = (0.01, 0.99),\n",
    "    debug: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Clean the dataset by:\n",
    "    1. Filtering out bunts and popups\n",
    "    2. Clipping extreme exit velocity values\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Input DataFrame to clean\n",
    "    lower : float, optional\n",
    "        Lower bound for clipping, computed from data if None\n",
    "    upper : float, optional\n",
    "        Upper bound for clipping, computed from data if None\n",
    "    quantiles : tuple[float, float], optional\n",
    "        Quantiles to use if computing bounds from data\n",
    "    debug : bool, optional\n",
    "        Whether to print diagnostic information\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cleaned DataFrame (copy of input)\n",
    "    tuple[float, float]\n",
    "        The (lower, upper) bounds used for clipping\n",
    "    \"\"\"\n",
    "    cols = _ColumnSchema()\n",
    "    TARGET = cols.target()\n",
    "    \n",
    "    # 1. Filter out bunts and popups\n",
    "    df = filter_bunts_and_popups(df, debug=debug)\n",
    "    \n",
    "    # 2. Compute bounds if not provided\n",
    "    if lower is None or upper is None:\n",
    "        lower_computed, upper_computed = compute_clip_bounds(\n",
    "            df[TARGET], method=\"quantile\", quantiles=quantiles\n",
    "        )\n",
    "        lower = lower if lower is not None else lower_computed\n",
    "        upper = upper if upper is not None else upper_computed\n",
    "    \n",
    "    # 3. Clip extreme values\n",
    "    df = clip_extreme_ev(df, lower=lower, upper=upper, debug=debug)\n",
    "    \n",
    "    return df, (lower, upper)\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# Dynamic preprocess functions\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 1) fit_preprocessor (with train‐set‐only bound computation + clip)  \n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def fit_preprocessor(\n",
    "    df: pd.DataFrame,\n",
    "    model_type: str = \"linear\",\n",
    "    debug: bool = False,\n",
    "    quantiles: tuple[float, float] = (0.01, 0.99),\n",
    "    max_safe_rows: int = 200000  # Added parameter for warning threshold\n",
    ") -> tuple[np.ndarray, pd.Series, ColumnTransformer]:\n",
    "    \"\"\"\n",
    "    Fit a preprocessing pipeline to transform raw data into model-ready features.\n",
    "    \n",
    "    IMPORTANT: This function should ONLY be called on TRAINING data, not the full dataset.\n",
    "    The bounds for clipping are computed from this data and applied to future transforms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Training data to fit the preprocessor on. Should NOT include test data.\n",
    "    model_type : str, default=\"linear\"\n",
    "        Type of model to prepare for (\"linear\" or other for iterative imputation)\n",
    "    debug : bool, default=False\n",
    "        Whether to print diagnostic information\n",
    "    quantiles : tuple[float, float], default=(0.01, 0.99)\n",
    "        Quantiles to use for clipping extreme values\n",
    "    max_safe_rows : int, default=200000\n",
    "        Warning threshold for suspiciously large datasets\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_mat : np.ndarray\n",
    "        Transformed feature matrix\n",
    "    y : pd.Series\n",
    "        Target values\n",
    "    ct : ColumnTransformer\n",
    "        Fitted transformer\n",
    "        \n",
    "    Warns\n",
    "    -----\n",
    "    UserWarning\n",
    "        If the input DataFrame has more rows than max_safe_rows\n",
    "    \"\"\"\n",
    "    # Warn if input is suspiciously large (might be entire dataset)\n",
    "    if len(df) > max_safe_rows:\n",
    "        warnings.warn(\n",
    "            f\"Dataset has {len(df)} rows which exceeds threshold of {max_safe_rows}. \"\n",
    "            \"You should call fit_preprocessor ONLY on TRAINING data, not the full dataset.\",\n",
    "            UserWarning\n",
    "        )\n",
    "\n",
    "    cols = _ColumnSchema()\n",
    "    TARGET = cols.target()\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 0) domain filter  ➔ 1) clip target\n",
    "    # ------------------------------------------------------------------ #\n",
    "    df, (lower, upper) = filter_and_clip(df, quantiles=quantiles, debug=debug)\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 2) split feature sets + coerce numerics\n",
    "    # ------------------------------------------------------------------ #\n",
    "    num_feats = [c for c in cols.numerical() if c != TARGET]\n",
    "    ord_feats = cols.ordinal()\n",
    "    nom_feats = cols.nominal()\n",
    "\n",
    "    df[num_feats] = df[num_feats].apply(pd.to_numeric, errors=\"coerce\")\n",
    "    X = df[num_feats + ord_feats + nom_feats].copy()\n",
    "    y = df[TARGET]\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 3) **NEW** – make ordinals plain strings before mutation\n",
    "    # ------------------------------------------------------------------ #\n",
    "    X[ord_feats] = X[ord_feats].astype(\"string\")\n",
    "\n",
    "    # safe masking\n",
    "    X.loc[:, ord_feats] = X.loc[:, ord_feats].mask(X.loc[:, ord_feats].isna(), np.nan)\n",
    "\n",
    "    ordinal_categories = [[*X[c].dropna().unique(), \"MISSING\"] for c in ord_feats]\n",
    "    ordinal_pipe = Pipeline([\n",
    "        (\"impute\", SimpleImputer(strategy=\"constant\", fill_value=\"MISSING\")),\n",
    "        (\"encode\", OrdinalEncoder(\n",
    "            categories=ordinal_categories,\n",
    "            handle_unknown=\"use_encoded_value\",\n",
    "            unknown_value=-1,\n",
    "            dtype=\"int32\",\n",
    "        )),\n",
    "    ])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 4) numeric pipeline selection\n",
    "    # ------------------------------------------------------------------ #\n",
    "    imp_cls, imp_kwargs = (\n",
    "        (SimpleImputer, {\"strategy\": \"median\", \"add_indicator\": True})\n",
    "        if model_type == \"linear\"\n",
    "        else (IterativeImputer, {\"random_state\": 0, \"add_indicator\": True})\n",
    "    )\n",
    "\n",
    "    numeric_pipe = Pipeline([\n",
    "        (\"impute\", imp_cls(**imp_kwargs)),\n",
    "        (\"scale\",  StandardScaler()),\n",
    "    ])\n",
    "\n",
    "    # ------------------------------------------------------------------ #\n",
    "    # 5) ColumnTransformer\n",
    "    # ------------------------------------------------------------------ #\n",
    "    ct = ColumnTransformer(\n",
    "        [(\"num\", numeric_pipe, num_feats),\n",
    "         (\"ord\", ordinal_pipe, ord_feats),\n",
    "         (\"nom\", nominal_pipe, nom_feats)],\n",
    "        remainder=\"drop\",\n",
    "        verbose_feature_names_out=False,\n",
    "    )\n",
    "    ct.lower_, ct.upper_ = lower, upper\n",
    "    X_mat = ct.fit_transform(X, y)\n",
    "    return X_mat, y, ct\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "def transform_preprocessor(\n",
    "    df: pd.DataFrame,\n",
    "    transformer: ColumnTransformer,\n",
    ") -> tuple[np.ndarray, pd.Series]:\n",
    "    \"\"\"\n",
    "    Transform new data using a fitted preprocessor.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        New data to transform\n",
    "    transformer : ColumnTransformer\n",
    "        Fitted transformer from fit_preprocessor\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    X_mat : np.ndarray\n",
    "        Transformed feature matrix\n",
    "    y : pd.Series\n",
    "        Target values\n",
    "    \"\"\"\n",
    "    cols, TARGET = _ColumnSchema(), _ColumnSchema().target()\n",
    "\n",
    "    # Domain filter & clip using the bounds from the fitted transformer\n",
    "    df, _ = filter_and_clip(df, lower=transformer.lower_, upper=transformer.upper_)\n",
    "\n",
    "    # rebuild lists & coerce numerics\n",
    "    num_feats = [c for c in cols.numerical() if c != TARGET]\n",
    "    ord_feats = cols.ordinal()\n",
    "    nom_feats = cols.nominal()\n",
    "    df[num_feats] = df[num_feats].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "    X = df[num_feats + ord_feats + nom_feats].copy()\n",
    "\n",
    "    # **NEW** – ensure ordinals are strings, then fill unseen→\"MISSING\"\n",
    "    X[ord_feats] = X[ord_feats].astype(\"string\")\n",
    "    X.loc[:, ord_feats] = X.loc[:, ord_feats].where(X.loc[:, ord_feats].notna(), \"MISSING\")\n",
    "\n",
    "    y = df[TARGET]\n",
    "    X_mat = transformer.transform(X)\n",
    "    return X_mat, y\n",
    "\n",
    "\n",
    "def inverse_transform_preprocessor(\n",
    "    X_trans: np.ndarray,\n",
    "    transformer: ColumnTransformer\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Invert each block of a ColumnTransformer back to its original features,\n",
    "    based on the exact column lists we passed in.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import warnings\n",
    "\n",
    "    # 1) Flatten the lists we gave each transformer to recover original feature order\n",
    "    orig_features: list[str] = []\n",
    "    for name, _, cols in transformer.transformers_:\n",
    "        if cols == 'drop':\n",
    "            continue\n",
    "        orig_features.extend(cols)\n",
    "\n",
    "    parts = []\n",
    "    start = 0\n",
    "    n_rows = X_trans.shape[0]\n",
    "\n",
    "    # 2) For each transformer, slice & inverse-transform\n",
    "    for name, trans, cols in transformer.transformers_:\n",
    "        if cols == 'drop':\n",
    "            continue\n",
    "\n",
    "        fitted = transformer.named_transformers_[name]\n",
    "\n",
    "        # how many columns did it produce?  (we only use this for slicing)\n",
    "        dummy = np.zeros((1, len(cols)))\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", category=UserWarning)\n",
    "            try:\n",
    "                out = fitted.transform(dummy)\n",
    "            except Exception:\n",
    "                out = dummy\n",
    "        n_out = out.shape[1]\n",
    "\n",
    "        # now slice the real block\n",
    "        block = X_trans[:, start : start + n_out]\n",
    "        start += n_out\n",
    "\n",
    "        # apply inverse_transform to get back original columns\n",
    "        if trans == 'passthrough':\n",
    "            inv = block\n",
    "        elif name == 'num':\n",
    "            scaler = fitted.named_steps['scale']\n",
    "            inv_full = scaler.inverse_transform(block)\n",
    "            inv = inv_full[:, :len(cols)]\n",
    "        else:\n",
    "            # ordinal or nominal pipelines\n",
    "            if isinstance(fitted, Pipeline):\n",
    "                last = list(fitted.named_steps.values())[-1]\n",
    "                inv = last.inverse_transform(block)\n",
    "            else:\n",
    "                inv = fitted.inverse_transform(block)\n",
    "\n",
    "        parts.append(pd.DataFrame(inv, columns=cols, index=range(n_rows)))\n",
    "\n",
    "    # 3) Concatenate & reorder\n",
    "    df_orig = pd.concat(parts, axis=1)\n",
    "    return df_orig[orig_features]\n",
    "\n",
    "\n",
    "\n",
    "def prepare_for_mixed_and_hierarchical(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Cleans the rows *and* adds convenience covariates expected by the\n",
    "    hierarchical and mixed-effects models.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Feature-engineered DataFrame\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Cleaned DataFrame with domain filtering and category encoding\n",
    "    \"\"\"\n",
    "    cols = _ColumnSchema()\n",
    "    TARGET = cols.target()\n",
    "\n",
    "    # Apply domain cleaning with default quantiles\n",
    "    df, _ = filter_and_clip(df.copy())\n",
    "\n",
    "    # Category coding for later random-effects\n",
    "    df[\"batter_id\"] = df[\"batter_id\"].astype(\"category\")\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    df_fe = feature_engineer(df)\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "\n",
    "\n",
    "    train_df, test_df = train_test_split(df_fe, test_size=0.2, random_state=42)\n",
    "    # run with debug prints\n",
    "    X_train, y_train, tf = fit_preprocessor(train_df, model_type='linear', debug=True)\n",
    "    X_test,  y_test      = transform_preprocessor(test_df, tf)\n",
    "\n",
    "    print(\"Processed shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "    # Example of inverse transform: \n",
    "    print(\"==========Example of inverse transform:==========\")\n",
    "    df_back = inverse_transform_preprocessor(X_train, tf)\n",
    "    print(\"\\n✅ Inverse‐transformed head (should mirror your original X_train):\")\n",
    "    print(df_back.head())\n",
    "    print(\"Shape:\", df_back.shape, \"→ original X_train shape before transform:\", X_train.shape)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/features/feature_selection.py\n",
    "import pandas as pd\n",
    "\n",
    "# ── NEW: model and importance imports\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "import shap\n",
    "\n",
    "from pathlib import Path\n",
    "from src.data.load_data import load_and_clean_data\n",
    "from src.features.feature_engineering import feature_engineer\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "# ── NEW: shapash and shapiq imports\n",
    "from shapash import SmartExplainer\n",
    "import shapiq\n",
    "from sklearn.utils import resample\n",
    "\n",
    "def train_baseline_model(X, y):\n",
    "    \"\"\"\n",
    "    Fit a RandomForestRegressor on X, y.\n",
    "    Returns the fitted model.\n",
    "    \"\"\"\n",
    "    # You can adjust hyperparameters as needed\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def compute_permutation_importance(\n",
    "    model,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.Series,\n",
    "    n_repeats: int = 10,\n",
    "    n_jobs: int = 1,\n",
    "    max_samples: float | int = None,\n",
    "    random_state: int = 42,\n",
    "    verbose: bool = True\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute permutation importances with controlled resource usage.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : estimator\n",
    "        Fitted model implementing .predict and .score.\n",
    "    X : pd.DataFrame\n",
    "        Features.\n",
    "    y : pd.Series or array\n",
    "        Target.\n",
    "    n_repeats : int\n",
    "        Number of shuffles per feature.\n",
    "    n_jobs : int\n",
    "        Number of parallel jobs (avoid -1 on Windows).\n",
    "    max_samples : float or int, optional\n",
    "        If float in (0,1], fraction of rows to sample.\n",
    "        If int, absolute number of rows to sample.\n",
    "    random_state : int\n",
    "        Seed for reproducibility.\n",
    "    verbose : bool\n",
    "        Print debug info if True.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Columns: feature, importance_mean, importance_std.\n",
    "        Sorted descending by importance_mean.\n",
    "    \"\"\"\n",
    "    # Debug info\n",
    "    if verbose:\n",
    "        print(f\"⏳ Computing permutation importances on {X.shape[0]} rows × {X.shape[1]} features\")\n",
    "        print(f\"   n_repeats={n_repeats}, n_jobs={n_jobs}, max_samples={max_samples}\")\n",
    "\n",
    "    # Subsample if requested\n",
    "    X_sel, y_sel = X, y\n",
    "    if max_samples is not None:\n",
    "        if isinstance(max_samples, float):\n",
    "            nsamp = int(len(X) * max_samples)\n",
    "        else:\n",
    "            nsamp = int(max_samples)\n",
    "        if verbose:\n",
    "            print(f\"   Subsampling to {nsamp} rows for speed\")\n",
    "        X_sel, y_sel = resample(X, y, replace=False, n_samples=nsamp, random_state=random_state)\n",
    "\n",
    "    try:\n",
    "        result = permutation_importance(\n",
    "            model,\n",
    "            X_sel, y_sel,\n",
    "            n_repeats=n_repeats,\n",
    "            random_state=random_state,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "    except OSError as e:\n",
    "        # Graceful fallback to single job\n",
    "        if verbose:\n",
    "            print(f\"⚠️  OSError ({e}). Retrying with n_jobs=1\")\n",
    "        result = permutation_importance(\n",
    "            model,\n",
    "            X_sel, y_sel,\n",
    "            n_repeats=n_repeats,\n",
    "            random_state=random_state,\n",
    "            n_jobs=1,\n",
    "        )\n",
    "\n",
    "    # Build and sort DataFrame\n",
    "    importance_df = (\n",
    "        pd.DataFrame({\n",
    "            \"feature\": X.columns,\n",
    "            \"importance_mean\": result.importances_mean,\n",
    "            \"importance_std\": result.importances_std,\n",
    "        })\n",
    "        .sort_values(\"importance_mean\", ascending=False)\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    if verbose:\n",
    "        print(\"✅ Permutation importances computed.\")\n",
    "    return importance_df\n",
    "\n",
    "\n",
    "def compute_shap_importance(model, X, nsamples=100):\n",
    "    \"\"\"\n",
    "    Compute mean absolute SHAP values per feature.\n",
    "    Returns a DataFrame sorted by importance.\n",
    "    \"\"\"\n",
    "    # DeepExplainer or TreeExplainer for tree-based models\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    # sample for speed\n",
    "    X_sample = X.sample(n=min(nsamples, len(X)), random_state=42)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    # For regression, shap_values is a 2D array\n",
    "    mean_abs_shap = pd.DataFrame({\n",
    "        \"feature\": X_sample.columns,\n",
    "        \"shap_importance\": np.abs(shap_values).mean(axis=0),\n",
    "    })\n",
    "    mean_abs_shap = mean_abs_shap.sort_values(\"shap_importance\", ascending=False).reset_index(drop=True)\n",
    "    return mean_abs_shap\n",
    "\n",
    "\n",
    "\n",
    "def filter_permutation_features(\n",
    "    importance_df: pd.DataFrame,\n",
    "    threshold: float\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Return features whose permutation importance_mean exceeds threshold.\n",
    "    \"\"\"\n",
    "    kept = importance_df.loc[\n",
    "        importance_df[\"importance_mean\"] > threshold, \"feature\"\n",
    "    ]\n",
    "    return kept.tolist()\n",
    "\n",
    "\n",
    "def filter_shap_features(\n",
    "    importance_df: pd.DataFrame,\n",
    "    threshold: float\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Return features whose shap_importance exceeds threshold.\n",
    "    \"\"\"\n",
    "    kept = importance_df.loc[\n",
    "        importance_df[\"shap_importance\"] > threshold, \"feature\"\n",
    "    ]\n",
    "    return kept.tolist()\n",
    "\n",
    "\n",
    "def select_final_features(\n",
    "    perm_feats: list[str],\n",
    "    shap_feats: list[str],\n",
    "    mode: str = \"intersection\"\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Combine permutation and SHAP feature lists.\n",
    "    mode=\"intersection\" for features in both lists,\n",
    "    mode=\"union\" for features in either list.\n",
    "    \"\"\"\n",
    "    set_perm = set(perm_feats)\n",
    "    set_shap = set(shap_feats)\n",
    "    if mode == \"union\":\n",
    "        final = set_perm | set_shap\n",
    "    else:\n",
    "        final = set_perm & set_shap\n",
    "    # return sorted for reproducibility\n",
    "    return sorted(final)\n",
    "\n",
    "\n",
    "\n",
    "def load_final_features(\n",
    "    file_path: str = \"data/models/features/final_features.txt\"\n",
    ") -> list[str]:\n",
    "    \"\"\"\n",
    "    Read the newline-delimited feature names file and return as a list.\n",
    "    \"\"\"\n",
    "    with open(file_path, \"r\") as fp:\n",
    "        return [line.strip() for line in fp if line.strip()]\n",
    "\n",
    "\n",
    "def filter_to_final_features(\n",
    "    df: pd.DataFrame,\n",
    "    file_path: str = \"data/models/features/final_features.txt\"\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Given a feature-engineered DataFrame, load the final feature list,\n",
    "    then return df[ ID_cols + final_features + [target] ].\n",
    "    \"\"\"\n",
    "    # load the feature names\n",
    "    final_feats = load_final_features(file_path)\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    keep = cols.id() + final_feats + [cols.target()]\n",
    "    missing = set(keep) - set(df.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Cannot filter: missing columns {missing}\")\n",
    "    return df[keep].copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- existing loading & schema logic ---\n",
    "    raw_path = Path(\"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\")\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values in raw data.\")\n",
    "    else:\n",
    "        print(\"=== Raw data null counts ===\")\n",
    "        for col, cnt in null_counts.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "    df_fe = feature_engineer(df)\n",
    "    print(\"Raw →\", df.shape, \"//  Feature-engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "\n",
    "    cols = _ColumnSchema()\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:     \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "\n",
    "    # ── STEP 1: fully preprocess the engineered DataFrame ──\n",
    "    from src.features.preprocess import fit_preprocessor, inverse_transform_preprocessor\n",
    "\n",
    "    # fit_preprocessor returns (X_matrix, y, fitted_transformer)\n",
    "    X_np, y, preproc = fit_preprocessor(df_fe, model_type='linear', debug=False)\n",
    "\n",
    "    # Use the same index that y carries (only non-bunt, non-NA rows)\n",
    "    idx = y.index\n",
    "    \n",
    "    # turn that into a DataFrame with the same column names:\n",
    "    feat_names = preproc.get_feature_names_out()\n",
    "    X = pd.DataFrame(X_np, columns=feat_names, index=idx)\n",
    "    print(f\"✅ Preprocessed feature matrix: {X.shape[0]} rows × {X.shape[1]} cols\")\n",
    "\n",
    "    # (optional) confirm inverse transform lines up:\n",
    "    df_back = inverse_transform_preprocessor(X_np, preproc)\n",
    "    df_back.index = idx\n",
    "    print(\"✅ inverse_transform round-trip (head):\")\n",
    "    print(df_back.head())\n",
    "\n",
    "    # ── STEP 2: train & compute importances on *that* X ──\n",
    "    print(\"\\nTraining baseline model…\")\n",
    "    model = train_baseline_model(X, y)\n",
    "\n",
    "    print(\"\\n🔍 Permutation Importances:\")\n",
    "    perm_imp = compute_permutation_importance(\n",
    "        model, X, y,\n",
    "        n_repeats=10,\n",
    "        n_jobs=2,            # test small parallelism\n",
    "        max_samples=0.5,     # test subsampling\n",
    "        verbose=True\n",
    "    )\n",
    "    print(perm_imp)\n",
    "\n",
    "\n",
    "    print(\"\\n🔍 SHAP Importances:\")\n",
    "    shap_imp = compute_shap_importance(model, X)\n",
    "    print(shap_imp)\n",
    "\n",
    "    # ── STEP 3: threshold & select your final features ──\n",
    "    perm_thresh = 0.01\n",
    "    shap_thresh = 0.01\n",
    "    perm_feats = filter_permutation_features(perm_imp, perm_thresh)\n",
    "    shap_feats = filter_shap_features(shap_imp, shap_thresh)\n",
    "    final_feats = select_final_features(perm_feats, shap_feats, mode=\"intersection\")\n",
    "    print(f\"\\nFinal preprocessed feature list ({len(final_feats)}):\")\n",
    "    print(final_feats)\n",
    "\n",
    "    # ── STEP 4: build & save a dataset with just those features + target + IDs ──\n",
    "    df_final = pd.concat([\n",
    "        df_fe[cols.id()],\n",
    "        df_fe[[cols.target()]],\n",
    "        X[final_feats]\n",
    "    ], axis=1)\n",
    "    print(\"Final dataset shape:\", df_final.shape)\n",
    "\n",
    "    Path(\"data/models/features/final_features.txt\").write_text(\"\\n\".join(final_feats))\n",
    "    print(\"✔️ Saved feature list to final_features.txt\")\n",
    "\n",
    "\n",
    "    # Demo: filter the full df_fe back to just those features\n",
    "    df_filtered = filter_to_final_features(df_fe)\n",
    "    print(\"Filtered to final features shape:\", df_filtered.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model choices\n",
    "\n",
    "see modelling_choices.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/models/linear.py\n",
    "\n",
    "\"\"\"\n",
    "Fast linear baselines (OLS and Ridge).\n",
    "\n",
    "Usage\n",
    "-----\n",
    ">>> from src.models.linear import fit_ridge\n",
    ">>> fitted, rmse = fit_ridge(train_df, val_df)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "def _split_xy(df: pd.DataFrame):\n",
    "    X = df.drop(columns=[\"exit_velo\"])\n",
    "    y = df[\"exit_velo\"]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def fit_ridge(X_tr: pd.DataFrame,\n",
    "              y_tr: pd.DataFrame,\n",
    "              X_te: pd.DataFrame,\n",
    "              y_te: pd.DataFrame,\n",
    "              alpha: float = 1.0):\n",
    "    \"\"\"\n",
    "    Returns (sklearn Pipeline, RMSE on test set).\n",
    "    \"\"\"\n",
    "\n",
    "    model = Pipeline(\n",
    "        [(\"reg\" , Ridge(alpha=alpha, random_state=0))]\n",
    "    ).fit(X_tr, y_tr)\n",
    "\n",
    "    pred = model.predict(X_te)\n",
    "    rmse = np.sqrt(np.mean((pred - y_te) ** 2))\n",
    "    return model, rmse\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.data.ColumnSchema import _ColumnSchema\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from src.features.preprocess import summarize_categorical_missingness\n",
    "    from src.features.preprocess import fit_preprocessor, transform_preprocessor, inverse_transform_preprocessor\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "\n",
    "    # --- inspect nulls in the raw data ---\n",
    "    null_counts = df.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values in raw data.\")\n",
    "    else:\n",
    "        print(\"=== Raw data null counts ===\")\n",
    "        for col, cnt in null_counts.items():\n",
    "            print(f\" • {col!r}: {cnt} missing\")\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "    # use list‐comprehension to drop target(s) from numerical features\n",
    "    numericals_without_y = [c for c in numericals if c not in cols.target()]\n",
    "\n",
    "    summary_df = summarize_categorical_missingness(df_fe)\n",
    "    print(summary_df.to_markdown(index=False))\n",
    "\n",
    "\n",
    "    # check nulls\n",
    "    print(\"🛠️  Nulls in X before fit_transform:\")\n",
    "    null_counts = df_fe.isnull().sum()\n",
    "    null_counts = null_counts[null_counts > 0]\n",
    "    if null_counts.empty:\n",
    "        print(\"✅  No missing values after feature engineering.\")\n",
    "    else:\n",
    "        print(\"=== Null counts post-engineering ===\")\n",
    "        print(null_counts)\n",
    "\n",
    "    train_df, test_df = train_test_split(df_fe, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # only on training data for linear/XGB\n",
    "    train_df = clip_extreme_ev(train_df)\n",
    "    #valid_df = clip_extreme_ev(valid_df)\n",
    "    \n",
    "    # run with debug prints\n",
    "    X_train, y_train, tf = fit_preprocessor(train_df, model_type='linear', debug=True)\n",
    "    X_test,  y_test      = transform_preprocessor(test_df, tf)\n",
    "\n",
    "        \n",
    "    print(\"Processed shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "    # Example of inverse transform: \n",
    "    print(\"==========Example of inverse transform:==========\")\n",
    "    df_back = inverse_transform_preprocessor(X_train, tf)\n",
    "    print(\"\\n✅ Inverse‐transformed head (should mirror your original X_train):\")\n",
    "    print(df_back.head())\n",
    "    print(\"Shape:\", df_back.shape, \"→ original X_train shape before transform:\", X_train.shape)\n",
    "    \n",
    "\n",
    "    # === NEW: Train & evaluate Ridge regression ===\n",
    "    model_ridge, rmse_ridge = fit_ridge(X_train, y_train, X_test,  y_test)\n",
    "    print(f\"Ridge regression RMSE: {rmse_ridge:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows with NaN in exit_velo\n",
      "season               0\n",
      "level_abbr           0\n",
      "batter_id            0\n",
      "pitcher_id           0\n",
      "exit_velo        97979\n",
      "launch_angle      9130\n",
      "spray_angle       6227\n",
      "hangtime         75968\n",
      "hit_type           201\n",
      "batter_hand          0\n",
      "pitcher_hand         0\n",
      "batter_height        0\n",
      "pitch_group          0\n",
      "outcome              0\n",
      "age                  0\n",
      "dtype: int64\n",
      "after rows dropped with NaN in exit_velo\n",
      "season               0\n",
      "level_abbr           0\n",
      "batter_id            0\n",
      "pitcher_id           0\n",
      "exit_velo            0\n",
      "launch_angle         0\n",
      "spray_angle          0\n",
      "hangtime         12429\n",
      "hit_type             0\n",
      "batter_hand          0\n",
      "pitcher_hand         0\n",
      "batter_height        0\n",
      "pitch_group          0\n",
      "outcome              0\n",
      "age                  0\n",
      "dtype: int64\n",
      "⚠️  Nulls after target‑filter:\n",
      "  • hangtime         12,429 (1.00%)\n",
      "   season level_abbr  batter_id  pitcher_id  exit_velo  launch_angle  \\\n",
      "0    2023        mlb        235        1335    95.7352       47.2362   \n",
      "1    2023        mlb       3182        1335    95.9380        4.7291   \n",
      "2    2023        mlb       3856        1988    89.1404      -16.2251   \n",
      "3    2023        mlb       2017        1988    88.7278       -6.8385   \n",
      "4    2023        mlb       1594        1988    89.2888        0.5079   \n",
      "\n",
      "   spray_angle  hangtime     hit_type batter_hand pitcher_hand  batter_height  \\\n",
      "0      -6.4422    6.4960     fly_ball           R            R             72   \n",
      "1      -4.8052    0.7806  ground_ball           L            R             75   \n",
      "2      15.2382    0.0311  ground_ball           S            R             72   \n",
      "3     -11.5988    0.1215  ground_ball           R            R             69   \n",
      "4     -22.1899    0.3802  ground_ball           R            R             73   \n",
      "\n",
      "  pitch_group outcome   age  \n",
      "0          FB     out  32.8  \n",
      "1          OS     out  29.2  \n",
      "2          OS     out  29.7  \n",
      "3          BB     out  23.4  \n",
      "4          FB     out  35.3  \n",
      "Index(['season', 'level_abbr', 'batter_id', 'pitcher_id', 'exit_velo',\n",
      "       'launch_angle', 'spray_angle', 'hangtime', 'hit_type', 'batter_hand',\n",
      "       'pitcher_hand', 'batter_height', 'pitch_group', 'outcome', 'age'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Marlins_Data_Science_Project/src/features/feature_engineering.py:83: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"player_ev_mean50\"].fillna(gm, inplace=True)\n",
      "/workspaces/Marlins_Data_Science_Project/src/features/feature_engineering.py:84: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"player_ev_std50\"].fillna(gs, inplace=True)\n",
      "/workspaces/Marlins_Data_Science_Project/src/features/feature_engineering.py:88: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"pitcher_ev_mean50\"].fillna(gm, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw → (1246545, 15) //  Feature‑engineered → (1246545, 29)\n",
      "   season level_abbr  batter_id  pitcher_id  exit_velo  launch_angle  \\\n",
      "0    2023        MLB        235        1335    95.7352       47.2362   \n",
      "1    2023        MLB       3182        1335    95.9380        4.7291   \n",
      "2    2023        MLB       3856        1988    89.1404      -16.2251   \n",
      "3    2023        MLB       2017        1988    88.7278       -6.8385   \n",
      "4    2023        MLB       1594        1988    89.2888        0.5079   \n",
      "\n",
      "   spray_angle  hangtime     hit_type batter_hand  ...           spray_bin  \\\n",
      "0      -6.4422    6.4960     FLY_BALL           R  ...     (-12.378, 9.48]   \n",
      "1      -4.8052    0.7806  GROUND_BALL           L  ...     (-12.378, 9.48]   \n",
      "2      15.2382    0.0311  GROUND_BALL           S  ...     (9.48, 179.962]   \n",
      "3     -11.5988    0.1215  GROUND_BALL           R  ...     (-12.378, 9.48]   \n",
      "4     -22.1899    0.3802  GROUND_BALL           R  ...  (-179.87, -12.378]   \n",
      "\n",
      "   same_hand hand_match pitch_hand_match  player_ev_mean50  player_ev_std50  \\\n",
      "0       True     R_VS_R        FB_R_VS_R         88.014816        14.479972   \n",
      "1      False     L_VS_R        OS_L_VS_R         88.014816        14.479972   \n",
      "2      False     S_VS_R        OS_S_VS_R         88.014816        14.479972   \n",
      "3       True     R_VS_R        BB_R_VS_R         88.014816        14.479972   \n",
      "4       True     R_VS_R        FB_R_VS_R         88.014816        14.479972   \n",
      "\n",
      "  pitcher_ev_mean50  age_centered season_centered level_idx  \n",
      "0         88.014816           6.4             2.0         2  \n",
      "1         88.014816           2.8             2.0         2  \n",
      "2         88.014816           3.3             2.0         2  \n",
      "3         88.014816          -3.0             2.0         2  \n",
      "4         88.014816           8.9             2.0         2  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "ID columns:          ['season', 'batter_id', 'pitcher_id']\n",
      "Ordinal columns:     ['level_abbr', 'age_bin', 'la_bin', 'spray_bin']\n",
      "Nominal columns:     ['hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand']\n",
      "All categorical:     ['level_abbr', 'age_bin', 'la_bin', 'spray_bin', 'hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand']\n",
      "Numerical columns:   ['launch_angle', 'spray_angle', 'hangtime', 'height_diff', 'age_sq', 'age_centered', 'season_centered', 'level_idx', 'player_ev_mean50', 'player_ev_std50', 'pitcher_ev_mean50']\n",
      "Model features:      ['launch_angle', 'spray_angle', 'hangtime', 'height_diff', 'age_sq', 'age_centered', 'season_centered', 'level_idx', 'player_ev_mean50', 'player_ev_std50', 'pitcher_ev_mean50', 'hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand', 'level_abbr', 'age_bin', 'la_bin', 'spray_bin']\n",
      "Target columns:   exit_velo\n",
      "All raw columns:     ['season', 'batter_id', 'pitcher_id', 'level_abbr', 'age_bin', 'la_bin', 'spray_bin', 'hit_type', 'pitch_group', 'batter_hand', 'pitcher_hand', 'hand_match', 'pitch_hand_match', 'same_hand', 'launch_angle', 'spray_angle', 'hangtime', 'height_diff', 'age_sq', 'age_centered', 'season_centered', 'level_idx', 'player_ev_mean50', 'player_ev_std50', 'pitcher_ev_mean50']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Marlins_Data_Science_Project/src/features/preprocess.py:138: UserWarning: Dataset has 997236 rows which exceeds threshold of 200000. You should call fit_preprocessor ONLY on TRAINING data, not the full dataset.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[filter_bunts_and_popups] dropped 70,284 rows (7.05%) bunts/pop-ups\n",
      "[clip_extreme_ev] lower=49.01, upper=110.25\n",
      "  → 9,270 / 926,952 (1.00%) below lower\n",
      "  → 9,268 / 926,952 (1.00%) above upper\n",
      "Processed shapes: (926952, 46) (231778, 46)\n",
      "==========Example of inverse transform:==========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 15:46:06,435] A new study created in memory with name: no-name-009d1576-23a4-486d-8bce-d01de1ebd97f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Inverse‐transformed head (should mirror your original X_train):\n",
      "   launch_angle  spray_angle  hangtime  height_diff  age_sq  age_centered  \\\n",
      "0       20.8142       1.3858    3.8519      0.81666  686.44          -0.2   \n",
      "1      -75.5873      24.1692    0.0250      1.81666  817.96           2.2   \n",
      "2       -4.0382       5.8295    0.1551      3.81666  576.00          -2.4   \n",
      "3      -24.0564      42.8805    0.0543     -0.18334  823.69           2.3   \n",
      "4       33.4007      11.4086    5.6830     -3.18334  681.21          -0.3   \n",
      "\n",
      "   season_centered  level_idx  player_ev_mean50  player_ev_std50  ...  \\\n",
      "0             -2.0        0.0         87.179676        13.061763  ...   \n",
      "1              2.0        0.0         88.659742        11.356806  ...   \n",
      "2             -2.0        0.0         89.364194        15.835891  ...   \n",
      "3              1.0        2.0         90.890562        16.252956  ...   \n",
      "4              0.0        2.0         88.196544        15.333166  ...   \n",
      "\n",
      "          age_bin             la_bin        spray_bin     hit_type  \\\n",
      "0    (24.5, 26.4]    (20.24, 34.842]  (-12.378, 9.48]     FLY_BALL   \n",
      "1    (26.4, 29.0]  (-89.685, -8.162]  (9.48, 179.962]  GROUND_BALL   \n",
      "2  (17.099, 24.5]    (-8.162, 7.452]  (-12.378, 9.48]  GROUND_BALL   \n",
      "3    (26.4, 29.0]  (-89.685, -8.162]  (9.48, 179.962]  GROUND_BALL   \n",
      "4    (24.5, 26.4]    (20.24, 34.842]  (9.48, 179.962]     FLY_BALL   \n",
      "\n",
      "  pitch_group batter_hand pitcher_hand hand_match pitch_hand_match same_hand  \n",
      "0          FB           R            R     R_VS_R        FB_R_VS_R      True  \n",
      "1          BB           R            R     R_VS_R        BB_R_VS_R      True  \n",
      "2          BB           R            R     R_VS_R        BB_R_VS_R      True  \n",
      "3          OS           S            R     S_VS_R        OS_S_VS_R     False  \n",
      "4          OS           S            R     S_VS_R        OS_S_VS_R     False  \n",
      "\n",
      "[5 rows x 22 columns]\n",
      "Shape: (926952, 22) → original X_train shape before transform: (997236, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:46:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:46:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:46:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 15:46:35,819] Trial 0 finished with value: 8.731132989261937 and parameters: {'n_estimators': 158, 'learning_rate': 0.034217879703847, 'max_depth': 6, 'subsample': 0.881693573285602, 'colsample_bytree': 0.6866674778019892}. Best is trial 0 with value: 8.731132989261937.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:46:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:46:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:46:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 15:48:27,759] Trial 1 finished with value: 7.767404638889651 and parameters: {'n_estimators': 517, 'learning_rate': 0.07220900645889793, 'max_depth': 10, 'subsample': 0.9040272112417169, 'colsample_bytree': 0.7260897810981992}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:48:31] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:48:31] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:48:31] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 15:49:26,503] Trial 2 finished with value: 8.627541884278111 and parameters: {'n_estimators': 551, 'learning_rate': 0.03516894099159102, 'max_depth': 3, 'subsample': 0.909792629964992, 'colsample_bytree': 0.9926927181878533}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:49:29] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:49:29] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:49:29] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 15:50:40,229] Trial 3 finished with value: 8.087621005612403 and parameters: {'n_estimators': 629, 'learning_rate': 0.05149167855171073, 'max_depth': 4, 'subsample': 0.9537311081992013, 'colsample_bytree': 0.8192430967776525}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:50:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:50:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:50:43] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 15:52:18,173] Trial 4 finished with value: 7.970661360778965 and parameters: {'n_estimators': 943, 'learning_rate': 0.28858134110421013, 'max_depth': 3, 'subsample': 0.5473872085782148, 'colsample_bytree': 0.53152549243923}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:52:21] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:52:21] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:52:21] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 15:53:07,427] Trial 5 finished with value: 8.006546761938928 and parameters: {'n_estimators': 175, 'learning_rate': 0.03884994394835903, 'max_depth': 10, 'subsample': 0.9452365411763131, 'colsample_bytree': 0.7480519108380903}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:53:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:53:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:53:10] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 15:53:53,934] Trial 6 finished with value: 12.184522098524274 and parameters: {'n_estimators': 290, 'learning_rate': 0.001407698194862246, 'max_depth': 5, 'subsample': 0.8271529393205364, 'colsample_bytree': 0.5570095759937375}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:53:56] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:53:56] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:53:56] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 15:54:40,117] Trial 7 finished with value: 11.292862183099986 and parameters: {'n_estimators': 415, 'learning_rate': 0.004228345521296381, 'max_depth': 3, 'subsample': 0.9323061493169145, 'colsample_bytree': 0.6492414668489427}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:54:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:54:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:54:42] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 15:56:05,321] Trial 8 finished with value: 7.7786157866560295 and parameters: {'n_estimators': 417, 'learning_rate': 0.07690239718902428, 'max_depth': 9, 'subsample': 0.6922430895532076, 'colsample_bytree': 0.8689832899326314}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:56:08] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:56:09] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:56:09] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 15:57:52,137] Trial 9 finished with value: 8.330157875809386 and parameters: {'n_estimators': 646, 'learning_rate': 0.01246700565136351, 'max_depth': 6, 'subsample': 0.5739191679446387, 'colsample_bytree': 0.9155326396646282}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:57:55] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:57:55] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [15:57:55] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 16:00:28,686] Trial 10 finished with value: 8.167626695052123 and parameters: {'n_estimators': 872, 'learning_rate': 0.28174816915363204, 'max_depth': 8, 'subsample': 0.740128573718293, 'colsample_bytree': 0.6319847433180174}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:00:31] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:00:31] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:00:31] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 16:02:01,274] Trial 11 finished with value: 7.847631966023395 and parameters: {'n_estimators': 405, 'learning_rate': 0.11061017960245784, 'max_depth': 10, 'subsample': 0.6737853002489056, 'colsample_bytree': 0.8369609683675999}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:02:03] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:02:03] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:02:04] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 16:04:05,105] Trial 12 finished with value: 7.798745093232096 and parameters: {'n_estimators': 714, 'learning_rate': 0.09893252488047904, 'max_depth': 8, 'subsample': 0.6623331238360548, 'colsample_bytree': 0.7860921923963251}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:04:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:04:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:04:07] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 16:05:36,410] Trial 13 finished with value: 8.200297289452992 and parameters: {'n_estimators': 425, 'learning_rate': 0.013222017751385446, 'max_depth': 8, 'subsample': 0.7933267673652381, 'colsample_bytree': 0.8941817313837515}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:05:40] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:05:40] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:05:40] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 16:08:31,687] Trial 14 finished with value: 7.822740186143423 and parameters: {'n_estimators': 783, 'learning_rate': 0.09613847635821915, 'max_depth': 9, 'subsample': 0.7056981941655018, 'colsample_bytree': 0.727989123177487}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:08:35] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:08:35] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:08:35] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 16:10:43,498] Trial 15 finished with value: 8.464947790206846 and parameters: {'n_estimators': 500, 'learning_rate': 0.006330018683237447, 'max_depth': 9, 'subsample': 0.6132543787983856, 'colsample_bytree': 0.887866626599479}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:10:46] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:10:46] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:10:46] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 16:11:36,363] Trial 16 finished with value: 7.786377136851468 and parameters: {'n_estimators': 281, 'learning_rate': 0.12463901451068655, 'max_depth': 9, 'subsample': 0.9986569485046243, 'colsample_bytree': 0.9972572173972645}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:11:38] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:11:38] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:11:39] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "[I 2025-05-06 16:12:28,762] Trial 17 finished with value: 8.416476113357936 and parameters: {'n_estimators': 271, 'learning_rate': 0.021170332351325635, 'max_depth': 7, 'subsample': 0.8158987012981695, 'colsample_bytree': 0.842939244238079}. Best is trial 1 with value: 7.767404638889651.\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:12:31] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:12:31] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n",
      "/opt/conda/envs/marlins-ds-gpu/lib/python3.10/site-packages/xgboost/training.py:183: UserWarning: [16:12:31] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1745056857893/work/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "# %%writefile src/models/gbm.py\n",
    "\n",
    "\"\"\"\n",
    "Gradient‑boosting baseline (XGBoost regressor).\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "import optuna\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from xgboost.core import XGBoostError\n",
    "\n",
    "# ————— Detect GPU support using modern API —————\n",
    "try:\n",
    "    XGBRegressor(tree_method=\"hist\", device=\"cuda\")\n",
    "    GPU_SUPPORT = True\n",
    "except XGBoostError:\n",
    "    GPU_SUPPORT = False\n",
    "\n",
    "def _split_xy(df: pd.DataFrame):\n",
    "    X = df.drop(columns=[\"exit_velo\"])\n",
    "    y = df[\"exit_velo\"]\n",
    "    return X, y\n",
    "\n",
    "def tune_gbm(X, y, n_trials: int = 50):\n",
    "    \"\"\"\n",
    "    Run an Optuna study to minimize CV RMSE of an XGBRegressor.\n",
    "    Uses device=cuda if available, else CPU.\n",
    "    \"\"\"\n",
    "    def objective(trial):\n",
    "        params = {\n",
    "            \"n_estimators\": trial.suggest_int(\"n_estimators\", 100, 1000),\n",
    "            \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.3, log=True),\n",
    "            \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
    "            \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
    "            \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
    "            \"random_state\": 0,\n",
    "            \"n_jobs\": -1,\n",
    "        }\n",
    "        params.update({\"tree_method\": \"hist\"})\n",
    "        model = XGBRegressor(**params)\n",
    "        scores = cross_val_score(\n",
    "            model, X, y,\n",
    "            scoring=\"neg_root_mean_squared_error\",\n",
    "            cv=3, n_jobs=-1\n",
    "        )\n",
    "        return -scores.mean()\n",
    "\n",
    "    study = optuna.create_study(direction=\"minimize\")\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "    return study.best_trial.params\n",
    "\n",
    "def fit_gbm(X_tr, y_tr, X_te, y_te, **gbm_kw):\n",
    "    \"\"\"\n",
    "    Train XGBRegressor with optional hyperparams, early stopping,\n",
    "    and automatic GPU/CPU selection.\n",
    "    Returns (model, RMSE).\n",
    "    \"\"\"\n",
    "    gbm_default = dict(\n",
    "        n_estimators=600,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=6,\n",
    "        subsample=0.7,\n",
    "        colsample_bytree=0.6,\n",
    "        random_state=0,\n",
    "        n_jobs=-1,\n",
    "        early_stopping_rounds=50,\n",
    "    )\n",
    "    gbm_default.update({\"tree_method\": \"hist\"})\n",
    "    gbm_default.update(gbm_kw)\n",
    "\n",
    "    model = XGBRegressor(**gbm_default)\n",
    "    model.fit(\n",
    "        X_tr, y_tr,\n",
    "        eval_set=[(X_te, y_te)],\n",
    "        early_stopping_rounds=gbm_default[\"early_stopping_rounds\"],\n",
    "        verbose=False\n",
    "    )\n",
    "    preds = model.predict(X_te)\n",
    "    rmse = mean_squared_error(y_te, preds, squared=False)\n",
    "    return model, rmse\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    from pathlib import Path\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from features.archive.preprocess import (fit_preprocessor\n",
    "                                        ,transform_preprocessor\n",
    "                                        ,inverse_transform_preprocessor)\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    print(df.head())\n",
    "    print(df.columns)\n",
    "    df_fe = feature_engineer(df)\n",
    "    print(\"Raw →\", df.shape, \"//  Feature‑engineered →\", df_fe.shape)\n",
    "    print(df_fe.head())\n",
    "    # singleton instance people can import as `cols`\n",
    "    cols = _ColumnSchema()\n",
    "\n",
    "    __all__ = [\"cols\"]\n",
    "    print(\"ID columns:         \", cols.id())\n",
    "    print(\"Ordinal columns:    \", cols.ordinal())\n",
    "    print(\"Nominal columns:    \", cols.nominal())\n",
    "    print(\"All categorical:    \", cols.categorical())\n",
    "    print(\"Numerical columns:  \", cols.numerical())\n",
    "    print(\"Model features:     \", cols.model_features())\n",
    "    print(\"Target columns:  \", cols.target())\n",
    "    print(\"All raw columns:    \", cols.all_raw())\n",
    "    numericals = cols.numerical()\n",
    "\n",
    "\n",
    "    train_df, test_df = train_test_split(df_fe, test_size=0.2, random_state=42)\n",
    "    # run with debug prints\n",
    "    # Record original feature count before preprocessing\n",
    "    X_orig = train_df.drop(columns=[\"exit_velo\"])\n",
    "    orig_shape = X_orig.shape\n",
    "\n",
    "    # Run preprocessing\n",
    "    X_train, y_train, tf = fit_preprocessor(train_df, model_type='linear', debug=True)\n",
    "    X_test,  y_test      = transform_preprocessor(test_df, tf)\n",
    "\n",
    "    print(\"Processed shapes:\", X_train.shape, X_test.shape)\n",
    "\n",
    "    # Example of inverse transform: \n",
    "    print(\"==========Example of inverse transform:==========\")\n",
    "    df_back = inverse_transform_preprocessor(X_train, tf)\n",
    "    print(\"\\n✅ Inverse‐transformed head (should mirror your original X_train):\")\n",
    "    print(df_back.head())\n",
    "    print(f\"Shape: {df_back.shape} → original X_train shape before transform: {orig_shape}\")\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    # === Hyperparameter tuning ===\n",
    "    best_params = tune_gbm(X_train, y_train, n_trials=50)\n",
    "    print(\"Tuned params:\", best_params)\n",
    "\n",
    "    # === Train & evaluate ===\n",
    "    gbm_model, rmse = fit_gbm(\n",
    "        X_train, y_train, X_test, y_test, **best_params\n",
    "    )\n",
    "    print(f\"Tuned XGBoost RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/models/mixed.py\n",
    "\n",
    "\"\"\"\n",
    "Frequentist mixed‑effects model using statsmodels MixedLM.\n",
    "\n",
    "Formula implemented:\n",
    "    exit_velo ~ 1 + level_ord + age_centered\n",
    "              + (1 | batter_id)\n",
    "\n",
    "We rely on columns already produced by preprocess():\n",
    "    • level_idx  (0,1,2)   – ordinal\n",
    "    • age_centered\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "\n",
    "def fit_mixed(train: pd.DataFrame,\n",
    "              test: pd.DataFrame):\n",
    "    \"\"\"Return (fitted model, RMSE on test).\"\"\"\n",
    "    # statsmodels wants a *single* DataFrame with all cols\n",
    "    # so we concatenates and keep row positions for slicing\n",
    "    combined = pd.concat([train, test], axis=0)\n",
    "    # ensure categorical dtype\n",
    "    combined[\"level_ord\"] = combined[\"level_idx\"].astype(int)\n",
    "\n",
    "    mdl = smf.mixedlm(\n",
    "        formula=\"exit_velo ~ 1 + level_ord + age_centered\",\n",
    "        data=combined.iloc[: len(train)],\n",
    "        groups=combined.iloc[: len(train)][\"batter_id\"]\n",
    "    ).fit(reml=False)\n",
    "\n",
    "    # predict on test set\n",
    "    pred = mdl.predict(exog=combined.iloc[len(train):])\n",
    "    true = test[\"exit_velo\"].values\n",
    "    rmse = np.sqrt(np.mean((pred - true) ** 2))\n",
    "    return mdl, rmse\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.features.preprocess import prepare_for_mixed_and_hierarchical\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    # Prepare and split\n",
    "    df_model = prepare_for_mixed_and_hierarchical(df_fe)\n",
    "    train_df, test_df = train_test_split(df_model, test_size=0.2, random_state=42)\n",
    " \n",
    "    # Fit mixed-effects\n",
    "    mixed_model, rmse_mixed = fit_mixed(train_df, test_df)\n",
    "    print(f\"Mixed-effects model RMSE: {rmse_mixed:.4f}\")\n",
    "\n",
    "    # In the smoke test section: P-Value Checks for Mixed-Effects Models\n",
    "    print(\"Mixed-effects model summary:\\n\", mixed_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/utils/bayesian_explainability.py\n",
    "import arviz as az\n",
    "import shap, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------- Posterior summaries -----------------\n",
    "def plot_parameter_forest(idata, var_names=None, hdi_prob=0.95):\n",
    "    \"\"\"Caterpillar/forest plot of posterior estimates.\"\"\"\n",
    "    return az.plot_forest(\n",
    "        idata,\n",
    "        var_names=var_names,\n",
    "        combined=True,\n",
    "        hdi_prob=hdi_prob,\n",
    "        kind=\"forest\",\n",
    "        figsize=(6, len(var_names or idata.posterior.data_vars) * 0.4),\n",
    "    )\n",
    "\n",
    "def posterior_table(idata, round_to=2):\n",
    "    \"\"\"\n",
    "    Return a nicely rounded HDI/mean table with significance.\n",
    "    \"\"\"\n",
    "    summary = az.summary(idata, hdi_prob=0.95).round(round_to)\n",
    "    summary[\"significant\"] = (summary[\"hdi_2.5%\"] > 0) | (summary[\"hdi_97.5%\"] < 0)\n",
    "    return summary\n",
    "\n",
    "# ---------------- Posterior‑predictive checks ---------\n",
    "def plot_ppc(idata, kind=\"overlay\"):\n",
    "    \"\"\"Visual PPC (over‑laid densities by default).\"\"\"\n",
    "    return az.plot_ppc(idata, kind=kind, alpha=0.1)\n",
    "\n",
    "# ---------------- SHAP-based feature importances ------\n",
    "def shap_explain(predict_fn, background_df, sample_df):\n",
    "    \"\"\"\n",
    "    Model‑agnostic Kernel SHAP on the *posterior mean predictor*.\n",
    "\n",
    "    predict_fn(df) must return a 1‑D numpy array of predictions.\n",
    "    \"\"\"\n",
    "    explainer = shap.KernelExplainer(predict_fn, background_df)\n",
    "    shap_values = explainer.shap_values(sample_df, nsamples=200)\n",
    "    shap.summary_plot(shap_values, sample_df, show=False)\n",
    "    plt.tight_layout()\n",
    "    return shap_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/models/hierarchical_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/hierarchical_utils.py\n",
    "\n",
    "import arviz as az\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "def save_model(idata, file_path: str, overwrite: bool = True):\n",
    "    \"\"\"Save ArviZ InferenceData to NetCDF.\"\"\"\n",
    "    idata.to_netcdf(file_path, engine=\"h5netcdf\", overwrite_existing=overwrite)\n",
    "    print(f\"✔︎ saved model → {file_path}\")\n",
    "\n",
    "def load_model(file_path: str):\n",
    "    \"\"\"Load ArviZ InferenceData from NetCDF.\"\"\"\n",
    "    idata = az.from_netcdf(file_path, engine=\"h5netcdf\")\n",
    "    print(f\"✔︎ loaded model ← {file_path}\")\n",
    "    return idata\n",
    "\n",
    "def save_preprocessor(transformer, file_path: str):\n",
    "    \"\"\"Save the scikit-learn transformer to a joblib file.\"\"\"\n",
    "    joblib.dump(transformer, file_path)\n",
    "    print(f\"✔︎ saved preprocessor → {file_path}\")\n",
    "\n",
    "def load_preprocessor(file_path: str):\n",
    "    \"\"\"Load the scikit-learn transformer from a joblib file.\"\"\"\n",
    "    transformer = joblib.load(file_path)\n",
    "    print(f\"✔︎ loaded preprocessor ← {file_path}\")\n",
    "    return transformer\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # === Editable settings ===\n",
    "    # Path to the saved model (NetCDF format)\n",
    "    MODEL_PATH = \"data/models/saved_models/model.nc\"\n",
    "    # Path to the preprocessor\n",
    "    PREPROC_PATH = \"data/models/saved_models/preprocessor.joblib\"\n",
    "    # Input data for prediction (raw CSV with exit velocity data)\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    # Output predictions file (CSV) or set to None to print to console\n",
    "    OUTPUT_PREDS_2024 = \"data/predictions/predictions_2024.csv\"  # <-- EDITABLE: set output CSV path or None\n",
    "    \n",
    "    model = load_model(MODEL_PATH)\n",
    "    save_model(model, MODEL_PATH)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/utils/posterior.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/utils/posterior.py\n",
    "# src/utils/posterior.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import arviz as az\n",
    "\n",
    "# ── REPLACEMENT: paste over the whole function ─────────────────────────\n",
    "import json, pathlib, numpy as np, pandas as pd, arviz as az\n",
    "\n",
    "JSON_GLOBAL = pathlib.Path(\"data/models/saved_models/global_effects.json\")\n",
    "\n",
    "def posterior_to_frame(idata: az.InferenceData) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Returns a batter‑level summary **AND** writes global effects to JSON.\n",
    "\n",
    "    File written  ➜  data/models/saved_models/global_effects.json\n",
    "    \"\"\"\n",
    "    post = idata.posterior\n",
    "\n",
    "    # ------- per‑batter u summaries -----------------------------------\n",
    "    u   = post[\"u\"]                                         # (chain,draw,batter)\n",
    "    stats = {\n",
    "        \"u_mean\"  : u.mean((\"chain\",\"draw\")).values,\n",
    "        \"u_sd\"    : u.std ((\"chain\",\"draw\")).values,\n",
    "        \"u_q2.5\"  : np.percentile(u.values,  2.5, axis=(0,1)),\n",
    "        \"u_q50\"   : np.percentile(u.values, 50.0, axis=(0,1)),\n",
    "        \"u_q97.5\" : np.percentile(u.values,97.5, axis=(0,1)),\n",
    "    }\n",
    "    df = pd.DataFrame({\"batter_idx\": np.arange(u.shape[-1]), **stats})\n",
    "\n",
    "    # ------- global effects ------------------------------------------\n",
    "    mu_mean = post[\"mu\"].mean().item()\n",
    "\n",
    "    # β_age  ➜ last entry of beta vector (age_centered was added last)\n",
    "    beta  = post[\"beta\"]\n",
    "    feat_dim = [d for d in beta.dims if d not in (\"chain\",\"draw\")][0]\n",
    "    beta_age = beta.isel({feat_dim: -1}).mean().item()\n",
    "\n",
    "    beta_level = post[\"beta_level\"].mean((\"chain\",\"draw\")).values.tolist()\n",
    "    sigma_b    = post[\"sigma_b\"].mean().item()\n",
    "    sigma_e    = post[\"sigma_e\"].mean().item()\n",
    "\n",
    "    global_eff = dict(\n",
    "        mu_mean=mu_mean,\n",
    "        beta_age=beta_age,\n",
    "        beta_level=beta_level,\n",
    "        sigma_b=sigma_b,\n",
    "        sigma_e=sigma_e,\n",
    "        median_age=idata.attrs.get(\"median_age\", 26.0),\n",
    "        beta=post[\"beta\"].mean((\"chain\",\"draw\")).values.tolist(),  # Save all beta coefficients\n",
    "        feature_names=idata.attrs.get(\"feature_names\", [])  # Also save feature names if available\n",
    "    )\n",
    "\n",
    "    # ➜  write side‑car JSON (overwrite every run)\n",
    "    JSON_GLOBAL.write_text(json.dumps(global_eff, indent=2))\n",
    "    print(f\"✔︎ wrote global effects → {JSON_GLOBAL}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def align_batter_codes(df_roster: pd.DataFrame,\n",
    "                       train_cats: pd.Index) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Convert integer batter_ids in *roster* into the categorical codes\n",
    "    **identical** to what the model saw during training.\n",
    "\n",
    "    Any unseen batter gets code = -1 (handled later).\n",
    "    \"\"\"\n",
    "    cat = pd.Categorical(df_roster[\"batter_id\"], categories=train_cats)\n",
    "    return pd.Series(cat.codes, index=df_roster.index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/models/hierarchical.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/hierarchical.py\n",
    "\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "from src.data.ColumnSchema import _ColumnSchema\n",
    "from src.features.preprocess import transform_preprocessor\n",
    "# ── Attempt to import JAX ──────────────────────────────────────\n",
    "USE_JAX = True\n",
    "try:\n",
    "    import jax\n",
    "    # Debug: confirm what module is loaded\n",
    "    print(f\"🔍 JAX module: {jax!r}\")\n",
    "    print(f\"🔍 JAX path:   {getattr(jax, '__file__', 'builtin')}\")\n",
    "    # Ensure version attribute exists (guards circular-import)\n",
    "    if not hasattr(jax, \"__version__\"):\n",
    "        raise ImportError(\"jax.__version__ missing—possible circular import\")\n",
    "    print(f\"✅ JAX version: {jax.__version__}\")\n",
    "    # Enable 64-bit floats on GPU/CPU\n",
    "    jax.config.update(\"jax_enable_x64\", True)\n",
    "except Exception as e:\n",
    "    USE_JAX = False\n",
    "    print(f\"⚠️  Warning: could not import JAX ({e}). Falling back to CPU sampling.\")\n",
    "\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Configure JAX for GPU use and X64 precision\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "print(\"JAX version:\", jax.__version__)\n",
    "print(\"JAX devices:\", jax.devices())\n",
    "print(\"GPU count:\", jax.device_count(\"gpu\"))\n",
    "print(\"Default backend:\", jax.default_backend())\n",
    "\n",
    "import logging, pymc.sampling.jax as pmjax\n",
    "\n",
    "\n",
    "# ── NEW: fit_bayesian_hierarchical with timing & ETAs ────────────────\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from tqdm.auto import tqdm          # auto‑selects rich bar in Jupyter / CLI\n",
    "\n",
    "@contextmanager\n",
    "def _timed_section(label: str):\n",
    "    \"\"\"Context manager that prints elapsed time for a code block.\"\"\"\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    dt = time.time() - t0\n",
    "    print(f\"[{label}] finished in {dt:,.1f} s\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def fit_bayesian_hierarchical(\n",
    "    df_raw,\n",
    "    transformer,\n",
    "    batter_idx: np.ndarray,\n",
    "    level_idx: np.ndarray,\n",
    "    *,\n",
    "    feature_list: list[str] | None = None,\n",
    "    mu_mean: float  = 88.0,\n",
    "    mu_sd:   float  = 30.0,\n",
    "    sigma_prior: float = 10.0,\n",
    "    draws: int      = 200,\n",
    "    tune:  int      = 200,\n",
    "    target_accept: float = 0.9,\n",
    "    verbose: bool   = True,\n",
    "    sampler: str    = \"jax\",\n",
    "):\n",
    "    cols = _ColumnSchema()\n",
    "    if feature_list is None:\n",
    "        feature_list = cols.model_features()\n",
    "\n",
    "    # 1) design matrix\n",
    "    X_all, y_ser = transform_preprocessor(df_raw, transformer)\n",
    "    names = transformer.get_feature_names_out()\n",
    "    X     = X_all[:, np.isin(names, feature_list)]\n",
    "    y     = y_ser.values\n",
    "\n",
    "    # ── CAST TO PYTHON INTS ───────────────────────────────────────────────\n",
    "    n_bat = int(batter_idx.max()) + 1\n",
    "    n_lvl = int(level_idx.max()) + 1\n",
    "    n_feat= int(X.shape[1])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"🔍 n_bat={n_bat} ({type(n_bat)}), \"\n",
    "              f\"n_lvl={n_lvl} ({type(n_lvl)}), \"\n",
    "              f\"n_feat={n_feat} ({type(n_feat)})\")\n",
    "\n",
    "    with pm.Model() as model:\n",
    "        # ── priors ─────────────────────────────────────────\n",
    "        mu         = pm.Normal(\"mu\", mu_mean, mu_sd)\n",
    "        beta_level = pm.Normal(\"beta_level\", 0, 5, shape=n_lvl)\n",
    "        beta       = pm.Normal(\"beta\",       0, 5, shape=n_feat)\n",
    "        sigma_b    = pm.HalfNormal(\"sigma_b\", sigma_prior)\n",
    "        u_raw      = pm.Normal(\"u_raw\", 0, 1, shape=n_bat)\n",
    "        u          = pm.Deterministic(\"u\", u_raw * sigma_b)\n",
    "\n",
    "        # ── likelihood ────────────────────────────────────\n",
    "        theta    = mu + beta_level[level_idx] + pm.math.dot(X, beta) + u[batter_idx]\n",
    "        sigma_e  = pm.HalfNormal(\"sigma_e\", sigma_prior)\n",
    "        pm.Normal(\"y_obs\", theta, sigma_e, observed=y)\n",
    "\n",
    "        # ── sampling ──────────────────────────────────────\n",
    "        idata = pm.sample(\n",
    "            draws=draws,\n",
    "            tune=tune,\n",
    "            chains=4,\n",
    "            target_accept=target_accept,\n",
    "            nuts_sampler=\"numpyro\" if (sampler==\"jax\" and USE_JAX) else \"nuts\",\n",
    "            progressbar=True,\n",
    "        )\n",
    "        idata.extend(pm.sample_posterior_predictive(idata, var_names=[\"y_obs\"]))\n",
    "\n",
    "    # Store feature names on idata for downstream use\n",
    "    used_feats = [names[i] for i in range(len(names)) if np.isin(names[i], feature_list)]\n",
    "    idata.attrs[\"feature_names\"] = used_feats\n",
    "\n",
    "    if verbose:\n",
    "        print(\"⚡ First 5 posterior-pred EV samples:\",\n",
    "              idata.posterior_predictive[\"y_obs\"].stack(s=(\"chain\",\"draw\")).values[:5])\n",
    "\n",
    "    return idata\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    # %%writefile src/train_hierarchical.py\n",
    "    from pathlib import Path\n",
    "    import pandas as pd, numpy as np, arviz as az\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.features.preprocess import (fit_preprocessor,\n",
    "                                        prepare_for_mixed_and_hierarchical)\n",
    "    # from src.models.hierarchical import fit_bayesian_hierarchical\n",
    "    from src.models.hierarchical_utils import save_model, save_preprocessor\n",
    "    from src.utils.posterior import posterior_to_frame\n",
    "\n",
    "    RAW   = Path(\"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\")\n",
    "    OUT_NC = Path(\"data/models/saved_models/exitvelo_hmc.nc\")\n",
    "    OUT_POST = Path(\"data/models/saved_models/posterior_summary.parquet\")\n",
    "    OUT_PREPROC = Path(\"data/models/saved_models/preprocessor.joblib\")\n",
    "\n",
    "    # 1 · prep\n",
    "    df = load_and_clean_data(RAW)\n",
    "    df_fe = feature_engineer(df)\n",
    "    df_model = prepare_for_mixed_and_hierarchical(df_fe)\n",
    "\n",
    "    _, _, tf = fit_preprocessor(df_model, model_type=\"linear\", debug=False)\n",
    "\n",
    "    b_idx = df_model[\"batter_id\"].cat.codes.values\n",
    "    l_idx = df_model[\"level_idx\"].values\n",
    "    draws_and_tune = 100\n",
    "    target_accept=0.9\n",
    "    # 2 · fit\n",
    "    idata = fit_bayesian_hierarchical(df_model, tf, b_idx, l_idx,\n",
    "                                    sampler=\"jax\", draws=draws_and_tune, tune=draws_and_tune, target_accept=target_accept)\n",
    "\n",
    "    idata.attrs[\"median_age\"] = df_model[\"age\"].median()   # ← NEW\n",
    "\n",
    "    # 3 · persist\n",
    "    save_model(idata, OUT_NC)\n",
    "    save_preprocessor(tf, OUT_PREPROC)\n",
    "    posterior_to_frame(idata).to_parquet(OUT_POST)\n",
    "    print(\"✅ training complete – artefacts written:\")\n",
    "    print(\"   •\", OUT_NC)\n",
    "    print(\"   •\", OUT_POST)\n",
    "    print(\"   •\", OUT_PREPROC)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/models/hierarchical_predict.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/models/hierarchical_predict.py\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "import arviz as az\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from src.features.feature_engineering import feature_engineer\n",
    "from src.features.preprocess import prepare_for_mixed_and_hierarchical\n",
    "from src.utils.posterior import align_batter_codes\n",
    "\n",
    "\n",
    "def predict_from_idata(\n",
    "    df_raw: pd.DataFrame,\n",
    "    idata: az.InferenceData\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate population-level predictions from a fitted hierarchical model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_raw : pd.DataFrame\n",
    "        Raw roster DataFrame containing at least ['season','batter_id','age','level'].\n",
    "    idata : arviz.InferenceData\n",
    "        Fitted model inference data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Input df_raw with added columns:\n",
    "        - level_idx, age_centered\n",
    "        - pred_mean (μ + β_level + β_age·age_centered)\n",
    "    \"\"\"\n",
    "    # 1) feature engineering & preprocessing\n",
    "    df_feat = feature_engineer(df_raw)\n",
    "    df_model = prepare_for_mixed_and_hierarchical(df_feat)\n",
    "\n",
    "    # 2) extract posterior-means of the fixed effects\n",
    "    pm = idata.posterior.mean(dim=(\"chain\", \"draw\"))\n",
    "    mu = pm[\"mu\"].values\n",
    "    beta_level = pm[\"beta_level\"].values\n",
    "    beta_age = pm[\"beta_age\"].values\n",
    "\n",
    "    # 3) compute point predictions\n",
    "    df_model[\"pred_mean\"] = (\n",
    "        mu[df_model.index]  # mu can be vector per row if modeled that way\n",
    "        + beta_level[df_model[\"level_idx\"].values]\n",
    "        + beta_age * df_model[\"age_centered\"].values\n",
    "    )\n",
    "\n",
    "    return df_model\n",
    "\n",
    "\n",
    "def predict_from_summaries(\n",
    "    roster_csv: Path,\n",
    "    posterior_parquet: Path,\n",
    "    global_effects_json: Path,\n",
    "    output_csv: Path,\n",
    "    preprocessor_path: Path = Path(\"data/models/saved_models/preprocessor.joblib\")\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Load your saved summaries + raw roster, merge in random effects,\n",
    "    compute full point & interval predictions, and write to CSV.\n",
    "\n",
    "    Returns the full merged DataFrame (with contrib_* and pred_lo95/hi95).\n",
    "    \"\"\"\n",
    "    # 1) load data and preprocessor\n",
    "    from src.models.hierarchical_utils import load_preprocessor\n",
    "    \n",
    "    df_post = pd.read_parquet(posterior_parquet)   # tidy posterior summary\n",
    "    df_roster = pd.read_csv(roster_csv)            # season, batter_id, age\n",
    "    \n",
    "    try:\n",
    "        transformer = load_preprocessor(preprocessor_path)\n",
    "        have_transformer = True\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Could not load preprocessor: {e}\")\n",
    "        print(\"Continuing without feature effects (predictions may be biased)\")\n",
    "        have_transformer = False\n",
    "\n",
    "    # 2) global (static) effects\n",
    "    glob = json.loads(global_effects_json.read_text())\n",
    "    post_mu    = glob[\"mu_mean\"]\n",
    "    beta_age   = glob[\"beta_age\"]\n",
    "    beta_level = glob[\"beta_level\"][2]      # MLB index\n",
    "    med_age    = glob[\"median_age\"]\n",
    "    \n",
    "    # Get feature coefficients if available\n",
    "    beta_features = glob.get(\"beta\", [])\n",
    "    feature_names = glob.get(\"feature_names\", [])\n",
    "\n",
    "    # 3) merge random effects\n",
    "    df_roster[\"batter_idx\"] = align_batter_codes(df_roster, df_post[\"batter_idx\"])\n",
    "    df = df_roster.merge(df_post, on=\"batter_idx\", how=\"left\")\n",
    "    global_sigma_b = df_post[\"u_sd\"].mean()\n",
    "    df[\"u_mean\"] = df[\"u_mean\"].fillna(0.0)\n",
    "    df[\"u_sd\"]   = df[\"u_sd\"].fillna(global_sigma_b)\n",
    "\n",
    "    # 4) Feature engineering and preprocessing (if transformer is available)\n",
    "    feature_effects = 0.0\n",
    "    if have_transformer and beta_features and feature_names:\n",
    "        from src.features.feature_engineering import feature_engineer\n",
    "        from src.features.preprocess import transform_preprocessor, prepare_for_mixed_and_hierarchical\n",
    "        \n",
    "        try:\n",
    "            # Apply same preprocessing pipeline as during training\n",
    "            df_feat = feature_engineer(df_roster)\n",
    "            df_model = prepare_for_mixed_and_hierarchical(df_feat)\n",
    "            \n",
    "            # Transform to get design matrix\n",
    "            X_all, _ = transform_preprocessor(df_model, transformer)\n",
    "            \n",
    "            # Get feature values from transformer\n",
    "            all_names = transformer.get_feature_names_out()\n",
    "            feature_indices = [i for i, name in enumerate(all_names) if name in feature_names]\n",
    "            \n",
    "            if feature_indices:\n",
    "                X = X_all[:, feature_indices]\n",
    "                feature_effects = np.dot(X, beta_features)\n",
    "                print(f\"✅ Applied effects from {len(feature_indices)} features\")\n",
    "            else:\n",
    "                print(\"⚠️ Could not match feature names\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Error applying feature preprocessing: {e}\")\n",
    "            print(\"Continuing with base effects only\")\n",
    "\n",
    "    # 5) compute predictions + contributions + 95% intervals\n",
    "    df[\"age_centered\"] = df[\"age\"] - med_age\n",
    "    df[\"pred_mean\"] = (\n",
    "        post_mu\n",
    "        + beta_level\n",
    "        + beta_age * df[\"age_centered\"]\n",
    "        + df[\"u_mean\"]\n",
    "        + feature_effects  # Add the feature effects\n",
    "    )\n",
    "    df[\"contrib_age\"]   = beta_age * df[\"age_centered\"]\n",
    "    df[\"contrib_level\"] = beta_level\n",
    "    df[\"contrib_u\"]     = df[\"u_mean\"]\n",
    "    df[\"contrib_features\"] = feature_effects\n",
    "    z95 = 1.96\n",
    "    df[\"pred_lo95\"] = df[\"pred_mean\"] - z95 * df[\"u_sd\"]\n",
    "    df[\"pred_hi95\"] = df[\"pred_mean\"] + z95 * df[\"u_sd\"]\n",
    "\n",
    "    # 6) export\n",
    "    df[[\n",
    "        \"season\", \"batter_id\", \"pred_mean\", \"pred_lo95\", \"pred_hi95\", \n",
    "        \"contrib_age\", \"contrib_level\", \"contrib_u\", \"contrib_features\"\n",
    "    ]].to_csv(output_csv, index=False)\n",
    "    print(f\"📄 Predictions written → {output_csv}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example CLI usage for your 2024 predictions\n",
    "    BASE          = Path(\"data/models/saved_models\")\n",
    "    P_SUMMARY     = BASE / \"posterior_summary.parquet\"\n",
    "    P_GLOBAL      = BASE / \"global_effects.json\"\n",
    "    P_PREPROC     = BASE / \"preprocessor.joblib\"\n",
    "    POSTERIOR = Path(\"data/models/saved_models/posterior_summary.parquet\")\n",
    "    ROSTER_INPUT    = Path(\"data/Research Data Project/Research Data Project/exit_velo_validate_data.csv\")\n",
    "    OUTPUT_CSV    = Path(\"data/predictions/exitvelo_predictions_2024.csv\")\n",
    "\n",
    "\n",
    "    predict_df = predict_from_summaries(\n",
    "        roster_csv=ROSTER_INPUT,\n",
    "        posterior_parquet=P_SUMMARY,\n",
    "        global_effects_json=P_GLOBAL,\n",
    "        output_csv=OUTPUT_CSV,\n",
    "        preprocessor_path=P_PREPROC\n",
    "    )\n",
    "\n",
    "\n",
    "    print(predict_df.head()[[\n",
    "        \"batter_id\", \"pred_mean\", \n",
    "        \"contrib_level\", \"contrib_age\", \"contrib_u\", \"contrib_features\"\n",
    "    ]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/utils/validation.py\n",
    "\"\"\"\n",
    "Generic K‑fold validator.\n",
    "\n",
    "• Works for sklearn Pipelines *or* PyMC idata.\n",
    "• Decides how to extract predictions based on\n",
    "  the object returned by `fit_func`.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "import arviz as az\n",
    "\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from typing import Callable, List, Union\n",
    "import statsmodels as sm\n",
    "\n",
    "def _split_xy(df: pd.DataFrame):\n",
    "    X = df.drop(columns=[\"exit_velo\"])\n",
    "    y = df[\"exit_velo\"]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "def _rmse(a, b):\n",
    "    return np.sqrt(np.mean((a - b) ** 2))\n",
    "\n",
    "\n",
    "def run_kfold_cv(\n",
    "    fit_func: Callable[[pd.DataFrame, pd.DataFrame], tuple],\n",
    "    df: pd.DataFrame,\n",
    "    k: int = 5,\n",
    "    random_state: int = 0,\n",
    "    **fit_kw\n",
    ") -> List[float]:\n",
    "    \"\"\"\n",
    "    fit_func(train_df, test_df, **fit_kw) -> (model_or_idata, rmse)\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    rmses: List[float] = []\n",
    "\n",
    "    for train_idx, test_idx in kf.split(df):\n",
    "        train, test = df.iloc[train_idx], df.iloc[test_idx]\n",
    "        _, rmse = fit_func(train, test, **fit_kw)\n",
    "        rmses.append(rmse)\n",
    "\n",
    "    return rmses\n",
    "\n",
    "\n",
    "# helper to score a *single* train/test split for idata\n",
    "def rmse_pymc(idata: az.InferenceData, test_df: pd.DataFrame) -> float:\n",
    "    \"\"\"Posterior mean vs truth.\"\"\"\n",
    "    pred = (\n",
    "        idata.posterior_predictive[\"y_obs\"]\n",
    "        .mean((\"chain\", \"draw\"))\n",
    "        .values\n",
    "    )\n",
    "    return _rmse(pred, test_df[\"exit_velo\"].values)\n",
    "\n",
    "def run_kfold_cv(fit_func, df, k=5, random_state=0, **fit_kwargs):\n",
    "    \"\"\"\n",
    "    Apply `fit_func(train_df, **fit_kwargs)` then evaluate on held-out.\n",
    "    Returns list of held-out log_likelihoods or RMSEs.\n",
    "    \"\"\"\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=random_state)\n",
    "    scores = []\n",
    "    for train_idx, test_idx in kf.split(df):\n",
    "        train, test = df.iloc[train_idx], df.iloc[test_idx]\n",
    "        idata = fit_func(train, **fit_kwargs)\n",
    "\n",
    "        # posterior predictive on test\n",
    "        ppc = az.from_pymc(posterior_predictive=idata, model=None)\n",
    "        pred_mean = ppc.posterior_predictive[\"y_obs\"].mean((\"chain\",\"draw\")).values\n",
    "        true = test[\"exit_velo\"].values\n",
    "        rmse = np.sqrt(((pred_mean - true)**2).mean())\n",
    "        scores.append(rmse)\n",
    "    return scores\n",
    "\n",
    "def posterior_predictive_check(idata, df, batter_idx):\n",
    "    \"\"\"\n",
    "    Plot observed vs. simulated exit-velo histograms.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    obs = df[\"exit_velo\"].values\n",
    "    sim = idata.posterior_predictive[\"y_obs\"].stack(samples=(\"chain\",\"draw\")).values.flatten()\n",
    "\n",
    "    fig, ax = plt.subplots(1,2,figsize=(8,3))\n",
    "    ax[0].hist(obs, bins=30); ax[0].set_title(\"Observed\")\n",
    "    ax[1].hist(sim, bins=30); ax[1].set_title(\"Simulated\")\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def prediction_interval(model, X, alpha=0.05, method='linear'):\n",
    "    \"\"\"\n",
    "    Compute prediction intervals for a model.\n",
    "    \"\"\"\n",
    "    if method == 'linear':\n",
    "        # For OLS and Ridge\n",
    "        X_const = sm.add_constant(X)\n",
    "        preds = model.get_prediction(X_const)\n",
    "        pred_int = preds.conf_int(alpha=alpha)\n",
    "        return preds.predicted_mean, pred_int[:, 0], pred_int[:, 1]\n",
    "    elif method == 'bayesian':\n",
    "        # For Bayesian models\n",
    "        hdi = az.hdi(model, hdi=1 - alpha)\n",
    "        return (\n",
    "            hdi.posterior_predictive.y_obs.sel(hdi=f\"{alpha/2*100}%\"),\n",
    "            hdi.posterior_predictive.y_obs.sel(hdi=f\"{(1-alpha/2)*100}%\")\n",
    "        )\n",
    "    elif method == 'gbm':\n",
    "        # For XGBoost quantile regression\n",
    "        lower = model.predict(X, pred_contribs=False, iteration_range=(0, model.best_iteration))\n",
    "        upper = model.predict(X, pred_contribs=False, iteration_range=(0, model.best_iteration))\n",
    "        return lower, upper  # Replace with actual quantile regression\n",
    "    else:\n",
    "        raise ValueError(\"Method not supported\")\n",
    "\n",
    "# Example for bootstrapping GBM\n",
    "def bootstrap_prediction_interval(model, X, n_bootstraps=1000, alpha=0.05):\n",
    "    preds = np.zeros((n_bootstraps, X.shape[0]))\n",
    "    for i in range(n_bootstraps):\n",
    "        indices = np.random.choice(X.shape[0], X.shape[0], replace=True)\n",
    "        preds[i] = model.predict(X[indices])\n",
    "    lower = np.percentile(preds, 100 * alpha / 2, axis=0)\n",
    "    upper = np.percentile(preds, 100 * (1 - alpha / 2), axis=0)\n",
    "    return lower, upper\n",
    "\n",
    "\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "# 6. Smoke test (only run when module executed directly)\n",
    "# ───────────────────────────────────────────────────────────────────────\n",
    "if __name__ == \"__main__\":\n",
    "    from src.data.load_data import load_and_clean_data\n",
    "    from src.features.feature_engineering import feature_engineer\n",
    "    from src.features.preprocess import prepare_for_mixed_and_hierarchical\n",
    "\n",
    "    raw_path = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "    predict_path = \"data/Research Data Project/Research Data Project/exit_velo_validate_data.csv.csv\"\n",
    "    df = load_and_clean_data(raw_path)\n",
    "    df_fe = feature_engineer(df)\n",
    "\n",
    "    # Prepare the DataFrame\n",
    "    df_model = prepare_for_mixed_and_hierarchical(df_fe)\n",
    "\n",
    "    # Extract arrays for PyMC\n",
    "    batter_idx   = df_model[\"batter_id\"].cat.codes.values\n",
    "    level_idx    = df_model[\"level_idx\"].values\n",
    "    age_centered = df_model[\"age_centered\"].values\n",
    "\n",
    "    # Fit the Bayesian hierarchical model\n",
    "    idata = fit_bayesian_hierarchical(\n",
    "        df_model, batter_idx, level_idx, age_centered,\n",
    "        mu_prior=90, sigma_prior=5,\n",
    "        sampler=\"jax\",   #  <-- GPU NUTS\n",
    "        draws=1000, tune=1000\n",
    "    )\n",
    "\n",
    "    print(idata)\n",
    "    \n",
    "    posterior_predictive_check(idata, df_model, df_model.batter_id.cat.codes.values)\n",
    "    \n",
    "    # For Bayesian model:\n",
    "    lower, upper = prediction_interval(idata, test_df, method='bayesian')\n",
    "    print(f\"Bayesian 95% Prediction Interval: {lower.mean():.2f}–{upper.mean():.2f} mph\")\n",
    "\n",
    "    # For Ridge model:\n",
    "    pred, lower, upper = prediction_interval(model_ridge, X_test, method='linear')\n",
    "    print(f\"Ridge 95% Prediction Interval: {lower[0]:.2f}–{upper[0]:.2f} mph\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile src/train.py\n",
    "\"\"\"\n",
    "Train / compare four families on a 70‑30 split.\n",
    "\n",
    "Run:\n",
    "    python -m src.train\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from src.data.load_data import load_and_clean_data\n",
    "from src.features.preprocess import preprocess\n",
    "\n",
    "from src.models.linear import fit_ridge\n",
    "from src.models.gbm   import fit_gbm\n",
    "from src.models.mixed import fit_mixed\n",
    "from src.models.hierarchical import fit_bayesian_hierarchical\n",
    "\n",
    "RAW_PATH = \"data/Research Data Project/Research Data Project/exit_velo_project_data.csv\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    df_raw   = load_and_clean_data(RAW_PATH)\n",
    "    df_clean = preprocess(df_raw)\n",
    "\n",
    "    train_df, test_df = train_test_split(\n",
    "        df_clean, test_size=0.30, random_state=42, stratify=df_clean[\"level_abbr\"]\n",
    "    )\n",
    "\n",
    "    # ––– A  Ridge  –––––––––––––––––––––––––––––––\n",
    "    _, rmse_ridge = fit_ridge(train_df, test_df)\n",
    "    print(f\"Ridge RMSE ……  {rmse_ridge:5.2f} mph\")\n",
    "\n",
    "    # ––– B  Gradient‑Boost  ––––––––––––––––––––––\n",
    "    _, rmse_gbm = fit_gbm(train_df, test_df)\n",
    "    print(f\"XGBoost RMSE … {rmse_gbm:5.2f} mph\")\n",
    "\n",
    "    # ––– C  Mixed‑Effects  –––––––––––––––––––––––\n",
    "    _, rmse_mixed = fit_mixed(train_df, test_df)\n",
    "    print(f\"Mixed‑LM RMSE  {rmse_mixed:5.2f} mph\")\n",
    "\n",
    "    # ––– D  Bayesian Hierarchical (quick sample) –\n",
    "    idata = fit_bayesian_hierarchical(\n",
    "        train_df,\n",
    "        batter_idx=train_df.batter_id.astype(\"category\").cat.codes.values,\n",
    "        level_idx=train_df.level_idx.values,\n",
    "        age_centered=train_df.age_centered.values,\n",
    "        mu_prior=90,\n",
    "        sigma_prior=5,\n",
    "        draws=500, tune=500   # short run for demo\n",
    "    )\n",
    "    from src.utils.validation import rmse_pymc\n",
    "    rmse_bayes = rmse_pymc(idata, test_df)\n",
    "    print(f\"PyMC RMSE ……  {rmse_bayes:5.2f} mph\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile src/models/model_shap_reports.py\n",
    "# ── NEW: shapash and shapiq imports\n",
    "from shapash import SmartExplainer\n",
    "import shapiq\n",
    "\n",
    "def generate_shapash_report(\n",
    "    model,\n",
    "    X,\n",
    "    y,\n",
    "    features_dict: dict | None = None,\n",
    "    preprocessing: object | None = None,\n",
    "    report_path: str = \"shapash_report.html\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Instantiate Shapash SmartExplainer, compile with data, and\n",
    "    generate both a live app and a standalone HTML report.\n",
    "\n",
    "    Parameters:\n",
    "    - model: trained ML model (supports .predict)\n",
    "    - X: pd.DataFrame, input features\n",
    "    - y: pd.Series or array, true target values\n",
    "    - features_dict: optional mapping {col: label} for display\n",
    "    - preprocessing: optional transformer with inverse_transform\n",
    "    - report_path: file path to save HTML report\n",
    "    \"\"\"\n",
    "    # 1️⃣ Create the explainer\n",
    "    xpl = SmartExplainer(\n",
    "        model=model,\n",
    "        features_dict=features_dict or {c: c for c in X.columns},\n",
    "        preprocessing=preprocessing\n",
    "    )\n",
    "    # 2️⃣ Compile dataset for Shapash\n",
    "    y_pred = model.predict(X)\n",
    "    xpl.compile(\n",
    "        x=X,\n",
    "        y_pred=y_pred,\n",
    "        y_target=y,\n",
    "        additional_data=None\n",
    "    )\n",
    "    # 3️⃣ Launch interactive app (optional; returns a Flask app)\n",
    "    # app = xpl.run_app()\n",
    "    # 4️⃣ Generate standalone HTML report\n",
    "    xpl.generate_report(\n",
    "        output_file=report_path,\n",
    "        title_story=\"Model Explainability Report\",\n",
    "        title_description=\"Auto-generated by Shapash\",\n",
    "        x_train=None, y_train=None, y_test=X, metrics=[]\n",
    "    )\n",
    "    # Return the explainer for further interaction\n",
    "    return xpl\n",
    "\n",
    "def compute_shapiq_interactions(\n",
    "    model,\n",
    "    X,\n",
    "    sample_size: int = 100,\n",
    "    max_order: int = 2\n",
    "):\n",
    "    \"\"\"\n",
    "    Use shapiq to compute Shapley Interaction values up to `max_order`\n",
    "    for up to `sample_size` observations.\n",
    "\n",
    "    Parameters:\n",
    "    - model: trained ML model\n",
    "    - X: pd.DataFrame of features\n",
    "    - sample_size: how many rows to explain\n",
    "    - max_order: maximum interaction order (e.g., 2 for pairwise)\n",
    "    \n",
    "    Returns:\n",
    "    - interaction_values: shapiq InteractionValues object\n",
    "    \"\"\"\n",
    "    # 1️⃣ Sample data for performance\n",
    "    X_sample = X.sample(n=min(len(X), sample_size), random_state=42).to_numpy()\n",
    "    # 2️⃣ Instantiate the explainer\n",
    "    explainer = shapiq.TabularExplainer(\n",
    "        model=model,\n",
    "        data=X_sample,\n",
    "        index=\"k-SII\",   # or \"SV\" for standard Shapley values\n",
    "        max_order=max_order\n",
    "    )\n",
    "    # 3️⃣ Explain the first sample\n",
    "    interaction_values = explainer.explain(X_sample[0], budget=256)\n",
    "    return interaction_values\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marlins-ds-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
